{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Standard Libraries\n",
    "# =========================\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import bz2\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import multiprocessing\n",
    "import warnings\n",
    "from glob import glob\n",
    "\n",
    "# Suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn = warn\n",
    "\n",
    "# =========================\n",
    "# IPython Extensions\n",
    "# =========================\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# =========================\n",
    "# Data Handling\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# RDKit\n",
    "# =========================\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, PandasTools, Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem.Descriptors import MolLogP\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
    "from rdkit.DataStructs import ExplicitBitVect\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit.Chem.AtomPairs import Pairs\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "# =========================\n",
    "# Standardiser\n",
    "# =========================\n",
    "from standardiser import break_bonds, neutralise, rules, unsalt\n",
    "from standardiser.utils import StandardiseException, sanity_check\n",
    "\n",
    "# =========================\n",
    "# Machine Learning / Scikit-learn\n",
    "# =========================\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, confusion_matrix, precision_score,\n",
    "    recall_score, f1_score, cohen_kappa_score, make_scorer\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    ShuffleSplit, RepeatedStratifiedKFold, StratifiedShuffleSplit,\n",
    "    GridSearchCV, StratifiedKFold\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "\n",
    "# ==========================\n",
    "# 데이터셋 불러오기\n",
    "# ==========================\n",
    "train_df = pd.read_excel(r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\FDA MDD (manual split)\\Dataset\\Train_set_FDAMMD_with_fingerprints_sorted_with_RDKit_and_CDK_features.xlsx\")\n",
    "\n",
    "# ==========================\n",
    "# 1️⃣ 데이터셋 크기와 클래스 분포 확인\n",
    "# ==========================\n",
    "print(\"=== Dataset Info ===\")\n",
    "# 데이터 행(row) 수와 특징(feature) 수 출력 (SMILES와 Outcome 제외)\n",
    "print(f\"Total rows: {train_df.shape[0]}, Total features (excluding SMILES/target): {train_df.shape[1]-2}\")\n",
    "\n",
    "# 클래스 분포 출력\n",
    "print(\"\\nClass distribution:\")\n",
    "print(train_df['Outcome'].value_counts())\n",
    "\n",
    "# 클래스 비율 출력\n",
    "print(\"\\nClass ratio:\")\n",
    "print(train_df['Outcome'].value_counts(normalize=True))\n",
    "\n",
    "# ==========================\n",
    "# 2️⃣ SMILES 중복 확인\n",
    "# ==========================\n",
    "if 'SMILES' in train_df.columns:\n",
    "    dup_count = train_df['SMILES'].duplicated().sum()\n",
    "    print(f\"\\nNumber of duplicated SMILES: {dup_count}\")\n",
    "else:\n",
    "    print(\"\\nSMILES column not found for duplicate check.\")  # SMILES 컬럼이 없을 경우 메시지 출력\n",
    "\n",
    "# ==========================\n",
    "# 3️⃣ 결측치(NaN) 및 특징 타입 확인\n",
    "# ==========================\n",
    "# SMILES와 Outcome 컬럼 제외 후 숫자형(numeric) 특징 선택\n",
    "numeric_features = train_df.drop(columns=['SMILES','Outcome'], errors='ignore').select_dtypes(include=['int64','float64'])\n",
    "\n",
    "print(f\"\\nNumber of numeric features: {numeric_features.shape[1]}\")  # 숫자형 feature 수\n",
    "print(f\"Any missing values: {numeric_features.isna().sum().sum()}\")   # 결측치 확인\n",
    "\n",
    "print(\"\\nFeature types:\")\n",
    "print(train_df.dtypes.value_counts())  # 각 컬럼 타입 개수 확인\n",
    "\n",
    "# ==========================\n",
    "# 4️⃣ 특징과 타겟 간 상관관계 확인\n",
    "# ==========================\n",
    "# 숫자형 특징과 Outcome 간 상관계수 계산\n",
    "corr = numeric_features.corrwith(train_df['Outcome'])\n",
    "# 절댓값 기준 상위 10개 특징 출력\n",
    "top_corr = corr.abs().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 features most correlated with target:\")\n",
    "print(top_corr)\n",
    "\n",
    "# ==========================\n",
    "# 5️⃣ Murcko Scaffold 고유성 확인 (옵션)\n",
    "# ==========================\n",
    "if 'SMILES' in train_df.columns:\n",
    "    try:\n",
    "        # 각 SMILES에 대해 Murcko Scaffold 추출\n",
    "        scaffolds = train_df['SMILES'].apply(lambda s: MurckoScaffold.MurckoScaffoldSmiles(smiles=s))\n",
    "        # 고유한 Scaffold 수 출력\n",
    "        print(f\"\\nNumber of unique Murcko scaffolds: {scaffolds.nunique()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing scaffolds: {e}\")  # 에러 발생 시 메시지 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 라이브러리 임포트\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, accuracy_score, roc_auc_score,\n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ================================\n",
    "# Bemis–Murcko scaffold-based 10-fold\n",
    "# ================================\n",
    "def get_bemis_murcko_scaffold(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold) if scaffold is not None else None\n",
    "\n",
    "def scaffold_kfold_indices(smiles_list, n_splits=10, random_state=42):\n",
    "    scaffolds = [get_bemis_murcko_scaffold(smi) for smi in smiles_list]\n",
    "    scaffold_to_indices = {}\n",
    "    for idx, scaf in enumerate(scaffolds):\n",
    "        scaffold_to_indices.setdefault(scaf, []).append(idx)\n",
    "\n",
    "    unique_scaffolds = list(scaffold_to_indices.keys())\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(unique_scaffolds)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    folds = []\n",
    "    scaffold_array = np.array(unique_scaffolds)\n",
    "    for scaf_train_idx, scaf_val_idx in kf.split(scaffold_array):\n",
    "        train_idx = []\n",
    "        val_idx = []\n",
    "        for i in scaf_train_idx:\n",
    "            train_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        for i in scaf_val_idx:\n",
    "            val_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        folds.append((np.array(train_idx, dtype=int),\n",
    "                      np.array(val_idx, dtype=int)))\n",
    "    return folds\n",
    "\n",
    "def compute_bacc_from_preds(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bacc = 0.5 * (sens + spec)\n",
    "    return bacc, sens, spec\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2.0\n",
    "\n",
    "# ================================\n",
    "# 데이터셋 불러오기\n",
    "# ================================\n",
    "data_file = train_path  # 데이터셋 파일 경로\n",
    "df = pd.read_excel(data_file)\n",
    "\n",
    "# SMILES (untuk scaffold)\n",
    "smiles_all = df['SMILES'].astype(str).values\n",
    "\n",
    "# ================================\n",
    "# 특징(feature)과 타겟(target) 설정\n",
    "# ================================\n",
    "drop_cols = ['SMILES', 'Morgan_Descriptors', 'MACCS_Descriptors', 'APF_Descriptors', 'Outcome']\n",
    "X_values = df.drop(columns=drop_cols).values  # RDKit+CDK physchem features\n",
    "y_values = df['Outcome'].astype(int).values   # 타겟 변수\n",
    "\n",
    "print(\"Dataset loaded:\", X_values.shape, \"features,\", len(y_values), \"samples\")\n",
    "\n",
    "# ================================\n",
    "# 하이퍼파라미터 그리드 설정\n",
    "# ================================\n",
    "paramgrid = {\n",
    "    \"max_features\": [\n",
    "        X_values.shape[1],\n",
    "        X_values.shape[1] // 2,\n",
    "        X_values.shape[1] // 4,\n",
    "        X_values.shape[1] // 12,\n",
    "        X_values.shape[1] // 10,\n",
    "        X_values.shape[1] // 7,\n",
    "        X_values.shape[1] // 5,\n",
    "        X_values.shape[1] // 3\n",
    "    ],\n",
    "    \"n_estimators\": [10, 100, 300, 500],\n",
    "}\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# ================================\n",
    "# 10-fold scaffold-CV\n",
    "# ================================\n",
    "folds = scaffold_kfold_indices(smiles_all, n_splits=10, random_state=42)\n",
    "\n",
    "accuracies, auc_scores, precisions, recalls = [], [], [], []\n",
    "f1_scores, specificities, sensitivity_scores = [], [], []\n",
    "ppvs, npvs, ccrs, bacc_scores = [], [], [], []\n",
    "\n",
    "for train_idx, test_idx in tqdm(folds, total=len(folds), desc=\"Scaffold-CV folds (RDKit+CDK RF)\"):\n",
    "    X_train, X_test = X_values[train_idx], X_values[test_idx]\n",
    "    y_train, y_test = y_values[train_idx], y_values[test_idx]\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(class_weight='balanced'),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    bacc, sens, spec = compute_bacc_from_preds(y_test, y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    tn = cm[0, 0]\n",
    "    fn = cm[1, 0]\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    ccr = calculate_ccr(sens, spec)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(sens)\n",
    "    f1_scores.append(f1)\n",
    "    specificities.append(spec)\n",
    "    sensitivity_scores.append(sens)\n",
    "    ppvs.append(ppv)\n",
    "    npvs.append(npv)\n",
    "    ccrs.append(ccr)\n",
    "    bacc_scores.append(bacc)\n",
    "\n",
    "    print(f\"Fold: AUC={auc:.4f}, BACC={bacc:.4f}, ACC={acc:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# CV 결과 요약 (fold 평균) + Sm\n",
    "# ================================\n",
    "mean_acc = np.nanmean(accuracies)\n",
    "mean_auc = np.nanmean(auc_scores)\n",
    "mean_bacc = np.nanmean(bacc_scores)\n",
    "Sm = 0.5 * (mean_auc + mean_bacc)\n",
    "\n",
    "metrics_summary = {\n",
    "    \"Accuracy\": mean_acc,\n",
    "    \"AUC\": mean_auc,\n",
    "    \"BACC\": mean_bacc,\n",
    "    \"Precision\": np.nanmean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.nanmean(sensitivity_scores),\n",
    "    \"F1\": np.nanmean(f1_scores),\n",
    "    \"Specificity\": np.nanmean(specificities),\n",
    "    \"PPV\": np.nanmean(ppvs),\n",
    "    \"NPV\": np.nanmean(npvs),\n",
    "    \"CCR\": np.nanmean(ccrs),\n",
    "    \"Sm\": Sm\n",
    "}\n",
    "\n",
    "print(\"\\n===== Scaffold-CV Results (RDKit+CDK RF) =====\")\n",
    "for k, v in metrics_summary.items():\n",
    "    print(f\"CV {k}: {v:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# 전체 데이터셋으로 최종 모델 학습\n",
    "# ================================\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced'),\n",
    "    param_grid=paramgrid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "grid_final.fit(X_values, y_values)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# ================================\n",
    "# 모델 및 평가 지표 저장 (+ Sm)\n",
    "# ================================\n",
    "output_dir = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(output_dir, \"Dermal_rf_rdkitcdk.pkl\")\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(f\"\\nFinal model saved: {model_path}\")\n",
    "\n",
    "metrics_path = os.path.join(output_dir, \"Dermal_rf_rdkitcdk_metrics_with_Sm.xlsx\")\n",
    "pd.DataFrame([metrics_summary]).to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics (incl. Sm) report saved: {metrics_path}\")\n",
    "\n",
    "# Sm untuk consensus QSAR\n",
    "sm_csv_path = os.path.join(output_dir, \"Sm_Physchem_RF.csv\")\n",
    "pd.DataFrame([{\"Descriptor\": \"Physchem\", \"Algorithm\": \"RF\", \"Sm\": Sm}]).to_csv(sm_csv_path, index=False)\n",
    "print(f\"Sm for Physchem RF saved at: {sm_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Import libraries\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, accuracy_score, roc_auc_score,\n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ================================\n",
    "# Scaffold-based 10-fold utils\n",
    "# ================================\n",
    "def get_bemis_murcko_scaffold(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold) if scaffold is not None else None\n",
    "\n",
    "def scaffold_kfold_indices(smiles_list, n_splits=10, random_state=42):\n",
    "    scaffolds = [get_bemis_murcko_scaffold(smi) for smi in smiles_list]\n",
    "    scaffold_to_indices = {}\n",
    "    for idx, scaf in enumerate(scaffolds):\n",
    "        scaffold_to_indices.setdefault(scaf, []).append(idx)\n",
    "\n",
    "    unique_scaffolds = list(scaffold_to_indices.keys())\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(unique_scaffolds)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    folds = []\n",
    "    scaffold_array = np.array(unique_scaffolds)\n",
    "    for scaf_train_idx, scaf_val_idx in kf.split(scaffold_array):\n",
    "        train_idx = []\n",
    "        val_idx = []\n",
    "        for i in scaf_train_idx:\n",
    "            train_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        for i in scaf_val_idx:\n",
    "            val_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        folds.append((np.array(train_idx, dtype=int),\n",
    "                      np.array(val_idx, dtype=int)))\n",
    "    return folds\n",
    "\n",
    "def compute_bacc_from_preds(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bacc = 0.5 * (sens + spec)\n",
    "    return bacc, sens, spec\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2.0\n",
    "\n",
    "# ================================\n",
    "# Load dataset\n",
    "# ================================\n",
    "data_file = train_path\n",
    "df = pd.read_excel(data_file)\n",
    "\n",
    "smiles_all = df['SMILES'].astype(str).values\n",
    "\n",
    "# ================================\n",
    "# Features & Target (physchem only)\n",
    "# ================================\n",
    "drop_cols = ['SMILES', 'Morgan_Descriptors', 'MACCS_Descriptors', 'APF_Descriptors', 'Outcome']\n",
    "X_values = df.drop(columns=drop_cols).values\n",
    "y_values = df['Outcome'].astype(int).values\n",
    "\n",
    "print(\"Dataset loaded:\", X_values.shape, \"features,\", len(y_values), \"samples\")\n",
    "\n",
    "# ================================\n",
    "# Hyperparameter grid for XGBoost\n",
    "# ================================\n",
    "paramgrid = {\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# ================================\n",
    "# 10-fold scaffold-CV\n",
    "# ================================\n",
    "folds = scaffold_kfold_indices(smiles_all, n_splits=10, random_state=42)\n",
    "\n",
    "accuracies, auc_scores, precisions, recalls = [], [], [], []\n",
    "f1_scores, specificities, sensitivity_scores = [], [], []\n",
    "ppvs, npvs, ccrs, bacc_scores = [], [], [], []\n",
    "\n",
    "for train_idx, test_idx in tqdm(folds, total=len(folds), desc=\"Scaffold-CV folds (RDKit+CDK XGB)\"):\n",
    "    X_train, X_test = X_values[train_idx], X_values[test_idx]\n",
    "    y_train, y_test = y_values[train_idx], y_values[test_idx]\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        estimator=XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        ),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    bacc, sens, spec = compute_bacc_from_preds(y_test, y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    tn = cm[0, 0]\n",
    "    fn = cm[1, 0]\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    ccr = calculate_ccr(sens, spec)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(sens)\n",
    "    f1_scores.append(f1)\n",
    "    specificities.append(spec)\n",
    "    sensitivity_scores.append(sens)\n",
    "    ppvs.append(ppv)\n",
    "    npvs.append(npv)\n",
    "    ccrs.append(ccr)\n",
    "    bacc_scores.append(bacc)\n",
    "\n",
    "    print(f\"Fold: AUC={auc:.4f}, BACC={bacc:.4f}, ACC={acc:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# CV results + Sm\n",
    "# ================================\n",
    "mean_acc = np.nanmean(accuracies)\n",
    "mean_auc = np.nanmean(auc_scores)\n",
    "mean_bacc = np.nanmean(bacc_scores)\n",
    "Sm = 0.5 * (mean_auc + mean_bacc)\n",
    "\n",
    "metrics_summary = {\n",
    "    \"Accuracy\": mean_acc,\n",
    "    \"AUC\": mean_auc,\n",
    "    \"BACC\": mean_bacc,\n",
    "    \"Precision\": np.nanmean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.nanmean(sensitivity_scores),\n",
    "    \"F1\": np.nanmean(f1_scores),\n",
    "    \"Specificity\": np.nanmean(specificities),\n",
    "    \"PPV\": np.nanmean(ppvs),\n",
    "    \"NPV\": np.nanmean(npvs),\n",
    "    \"CCR\": np.nanmean(ccrs),\n",
    "    \"Sm\": Sm\n",
    "}\n",
    "\n",
    "print(\"\\n===== Scaffold-CV Results (RDKit+CDK XGB) =====\")\n",
    "for k, v in metrics_summary.items():\n",
    "    print(f\"CV {k}: {v:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# Train final model on full dataset\n",
    "# ================================\n",
    "print(\"\\nTraining final XGBoost model on full dataset with GridSearchCV...\")\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    param_grid=paramgrid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_final.fit(X_values, y_values)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# ================================\n",
    "# Save model & metrics\n",
    "# ================================\n",
    "output_dir = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(output_dir, \"Dermal_xgb_rdkitcdk.pkl\")\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(f\"\\nFinal XGBoost model saved: {model_path}\")\n",
    "\n",
    "metrics_path = os.path.join(output_dir, \"Dermal_xgb_rdkitcdk_metrics_with_Sm.xlsx\")\n",
    "pd.DataFrame([metrics_summary]).to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics (incl. Sm) report saved: {metrics_path}\")\n",
    "\n",
    "# Sm untuk consensus QSAR\n",
    "sm_csv_path = os.path.join(output_dir, \"Sm_Physchem_XGB.csv\")\n",
    "pd.DataFrame([{\"Descriptor\": \"Physchem\", \"Algorithm\": \"XGB\", \"Sm\": Sm}]).to_csv(sm_csv_path, index=False)\n",
    "print(f\"Sm for Physchem XGB saved at: {sm_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Import libraries\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, accuracy_score, roc_auc_score,\n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ================================\n",
    "# Scaffold-based 10-fold utils\n",
    "# ================================\n",
    "def get_bemis_murcko_scaffold(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold) if scaffold is not None else None\n",
    "\n",
    "def scaffold_kfold_indices(smiles_list, n_splits=10, random_state=42):\n",
    "    scaffolds = [get_bemis_murcko_scaffold(smi) for smi in smiles_list]\n",
    "    scaffold_to_indices = {}\n",
    "    for idx, scaf in enumerate(scaffolds):\n",
    "        scaffold_to_indices.setdefault(scaf, []).append(idx)\n",
    "\n",
    "    unique_scaffolds = list(scaffold_to_indices.keys())\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(unique_scaffolds)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    folds = []\n",
    "    scaffold_array = np.array(unique_scaffolds)\n",
    "    for scaf_train_idx, scaf_val_idx in kf.split(scaffold_array):\n",
    "        train_idx = []\n",
    "        val_idx = []\n",
    "        for i in scaf_train_idx:\n",
    "            train_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        for i in scaf_val_idx:\n",
    "            val_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        folds.append((np.array(train_idx, dtype=int),\n",
    "                      np.array(val_idx, dtype=int)))\n",
    "    return folds\n",
    "\n",
    "def compute_bacc_from_preds(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bacc = 0.5 * (sens + spec)\n",
    "    return bacc, sens, spec\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2.0\n",
    "\n",
    "# ================================\n",
    "# Load dataset\n",
    "# ================================\n",
    "file_path = train_path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "smiles_all = df['SMILES'].astype(str).values\n",
    "\n",
    "# Drop kolom non-fitur (pakai physchem saja)\n",
    "drop_cols = ['SMILES', 'Morgan_Descriptors', 'MACCS_Descriptors', 'APF_Descriptors', 'Outcome']\n",
    "X_values = df.drop(columns=drop_cols).values\n",
    "y_values = df['Outcome'].astype(int).values\n",
    "\n",
    "print(\"Dataset loaded:\", X_values.shape, \"features,\", len(y_values), \"samples\")\n",
    "\n",
    "# ================================\n",
    "# Hyperparameter grid\n",
    "# ================================\n",
    "param_grid = {\n",
    "    \"svc__C\": [0.1, 1, 10, 100],\n",
    "    \"svc__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    \"svc__gamma\": ['scale', 'auto']\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svc\", SVC(probability=True, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# ================================\n",
    "# 10-fold scaffold-CV\n",
    "# ================================\n",
    "folds = scaffold_kfold_indices(smiles_all, n_splits=10, random_state=42)\n",
    "\n",
    "accuracies, auc_scores, precisions, recalls = [], [], [], []\n",
    "f1_scores, specificities, sensitivity_scores = [], [], []\n",
    "ppvs, npvs, ccrs, bacc_scores = [], [], [], []\n",
    "\n",
    "for train_idx, test_idx in tqdm(folds, total=len(folds), desc=\"Scaffold-CV folds (Physchem SVM)\"):\n",
    "    X_train, X_test = X_values[train_idx], X_values[test_idx]\n",
    "    y_train, y_test = y_values[train_idx], y_values[test_idx]\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_test, y_proba)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    bacc, sens, spec = compute_bacc_from_preds(y_test, y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    tn = cm[0, 0]\n",
    "    fn = cm[1, 0]\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    ccr = calculate_ccr(sens, spec)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(sens)\n",
    "    f1_scores.append(f1)\n",
    "    specificities.append(spec)\n",
    "    sensitivity_scores.append(sens)\n",
    "    ppvs.append(ppv)\n",
    "    npvs.append(npv)\n",
    "    ccrs.append(ccr)\n",
    "    bacc_scores.append(bacc)\n",
    "\n",
    "    print(f\"Fold: AUC={auc:.4f}, BACC={bacc:.4f}, ACC={acc:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# CV results + Sm\n",
    "# ================================\n",
    "mean_acc = np.nanmean(accuracies)\n",
    "mean_auc = np.nanmean(auc_scores)\n",
    "mean_bacc = np.nanmean(bacc_scores)\n",
    "Sm = 0.5 * (mean_auc + mean_bacc)\n",
    "\n",
    "metrics_summary = {\n",
    "    \"Accuracy\": mean_acc,\n",
    "    \"AUC\": mean_auc,\n",
    "    \"BACC\": mean_bacc,\n",
    "    \"Precision\": np.nanmean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.nanmean(sensitivity_scores),\n",
    "    \"F1\": np.nanmean(f1_scores),\n",
    "    \"Specificity\": np.nanmean(specificities),\n",
    "    \"PPV\": np.nanmean(ppvs),\n",
    "    \"NPV\": np.nanmean(npvs),\n",
    "    \"CCR\": np.nanmean(ccrs),\n",
    "    \"Sm\": Sm\n",
    "}\n",
    "\n",
    "print(\"\\n===== Scaffold-CV Results (Physchem SVM) =====\")\n",
    "for k, v in metrics_summary.items():\n",
    "    print(f\"CV {k}: {v:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# Train final model di seluruh dataset\n",
    "# ================================\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "grid_final.fit(X_values, y_values)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# ================================\n",
    "# Save model & metrics & Sm\n",
    "# ================================\n",
    "output_dir = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(output_dir, \"Dermal_svm_rdkitcdk.pkl\")\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(f\"\\nFinal SVM model saved: {model_path}\")\n",
    "\n",
    "metrics_path = os.path.join(output_dir, \"Dermal_svm_rdkitcdk_metrics_with_Sm.xlsx\")\n",
    "pd.DataFrame([metrics_summary]).to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics (incl. Sm) report saved: {metrics_path}\")\n",
    "\n",
    "sm_csv_path = os.path.join(output_dir, \"Sm_Physchem_SVM.csv\")\n",
    "pd.DataFrame([{\"Descriptor\": \"Physchem\", \"Algorithm\": \"SVM\", \"Sm\": Sm}]).to_csv(sm_csv_path, index=False)\n",
    "print(f\"Sm for Physchem SVM saved at: {sm_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
