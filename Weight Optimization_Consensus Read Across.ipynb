{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44dac9da-38f8-4757-9000-1242713981cb",
   "metadata": {},
   "source": [
    "# PRE PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b08e221-0591-458b-94d8-9a0e142ae125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ===============================================================\n",
    "# Fungsi umum\n",
    "# ===============================================================\n",
    "\n",
    "def load_excel(file_path):\n",
    "    \"\"\"Load Excel file menjadi DataFrame.\"\"\"\n",
    "    df = pd.read_excel(file_path)\n",
    "    return df\n",
    "\n",
    "def convert_smiles_to_mol(df, smiles_col='SMILES'):\n",
    "    \"\"\"Konversi SMILES ke Mol.\"\"\"\n",
    "    if smiles_col not in df.columns:\n",
    "        raise ValueError(f\"❌ Column '{smiles_col}' not found!\")\n",
    "    df['Mol'] = df[smiles_col].apply(lambda x: Chem.MolFromSmiles(str(x)) if pd.notnull(x) else None)\n",
    "    return df\n",
    "\n",
    "def drop_duplicates_and_na(df, subset_cols=['SMILES']):\n",
    "    \"\"\"Buang duplikat dan NA.\"\"\"\n",
    "    df = df.drop_duplicates(subset=subset_cols).dropna()\n",
    "    return df\n",
    "\n",
    "def parse_fingerprint_column(column):\n",
    "    \"\"\"Parsing string list menjadi array numerik.\"\"\"\n",
    "    parsed = []\n",
    "    for item in column:\n",
    "        if isinstance(item, str):\n",
    "            try:\n",
    "                parsed.append(ast.literal_eval(item))\n",
    "            except Exception:\n",
    "                parsed.append([])\n",
    "        else:\n",
    "            parsed.append(item)\n",
    "    return np.array(parsed, dtype=float)\n",
    "\n",
    "def prepare_X_y(df, fingerprint_cols, outcome_col='Outcome'):\n",
    "    \"\"\"Prepare X dan y dari DataFrame.\"\"\"\n",
    "    # Outcome\n",
    "    df[outcome_col] = df[outcome_col].astype(int)\n",
    "    y = df[outcome_col].map({0: 0, 1: 1}).to_numpy(dtype=np.int32)\n",
    "    \n",
    "    # Fingerprints\n",
    "    X_parts = [parse_fingerprint_column(df[col]) for col in fingerprint_cols]\n",
    "    \n",
    "    # Konsistensi jumlah sampel\n",
    "    n_samples = {arr.shape[0] for arr in X_parts}\n",
    "    if len(n_samples) != 1:\n",
    "        raise ValueError(\"❌ Number of samples inconsistent across fingerprints!\")\n",
    "    \n",
    "    # Concatenate semua fingerprint\n",
    "    X = np.concatenate(X_parts, axis=1)\n",
    "    return X, y\n",
    "\n",
    "def check_numerical(data, name=\"Data\"):\n",
    "    \"\"\"Validasi seluruh elemen numerik.\"\"\"\n",
    "    if not np.issubdtype(data.dtype, np.number):\n",
    "        raise ValueError(f\"❌ {name} contains non-numerical values!\")\n",
    "\n",
    "def plot_outcome_hist(y, bins=None, title=\"Outcome Histogram\"):\n",
    "    \"\"\"Plot histogram outcome.\"\"\"\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(y, bins=bins, edgecolor='k')\n",
    "    plt.xticks(np.arange(0, np.max(y)+1))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Outcome\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# ===============================================================\n",
    "# Paths & fingerprint columns\n",
    "# ===============================================================\n",
    "fingerprint_columns = [\n",
    "    'Morgan_Descriptors',\n",
    "    'MACCS_Descriptors',\n",
    "    'APF_Descriptors',\n",
    "    'RDK_Descriptors'\n",
    "]\n",
    "\n",
    "train_file = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\Old Endpoints\\Acute Dermal Toxicity (manual split)\\Read Across\\Train_set_Dermal_balanced_with_fingerprints.xlsx\"\n",
    "test_file  = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\Old Endpoints\\Acute Dermal Toxicity (manual split)\\Read Across\\Test_set_Dermal_balanced_with_fingerprints.xlsx\"\n",
    "\n",
    "# ===============================================================\n",
    "# Load & clean data\n",
    "# ===============================================================\n",
    "train_df = load_excel(train_file)\n",
    "train_df = convert_smiles_to_mol(train_df)\n",
    "train_df = drop_duplicates_and_na(train_df)\n",
    "train_df = train_df.sort_values(['Outcome'], ascending=True)\n",
    "\n",
    "test_df  = load_excel(test_file)\n",
    "test_df = convert_smiles_to_mol(test_df)\n",
    "test_df = drop_duplicates_and_na(test_df)\n",
    "test_df = test_df.sort_values(['Outcome'], ascending=True)\n",
    "\n",
    "# ===============================================================\n",
    "# Prepare X and y\n",
    "# ===============================================================\n",
    "x_train, y_train = prepare_X_y(train_df, fingerprint_columns)\n",
    "x_test,  y_test  = prepare_X_y(test_df, fingerprint_columns)\n",
    "\n",
    "# Validasi numerik\n",
    "for arr, name in zip([x_train, x_test], ['Train X', 'Test X']):\n",
    "    check_numerical(arr, name)\n",
    "\n",
    "# ===============================================================\n",
    "# Info outcome\n",
    "# ===============================================================\n",
    "def print_class_info(y, label=\"Dataset\"):\n",
    "    outcomes = np.unique(y)\n",
    "    print(f\"{label} Classes                          : \", outcomes)\n",
    "    print(f\"{label} Number of compounds in each class: \", [len(y[y==cls]) for cls in outcomes])\n",
    "    print(f\"{label} Total number of compounds        : \", len(y))\n",
    "    info = {str(cls): i for i, cls in enumerate(outcomes)}\n",
    "    print(f\"{label} Class info mapping               : \", info)\n",
    "    plot_outcome_hist(y, bins=np.arange(-0.5, np.max(y)+1, 1), title=f\"{label} Outcome Histogram\")\n",
    "    return info\n",
    "\n",
    "train_info = print_class_info(y_train, \"Train\")\n",
    "test_info  = print_class_info(y_test, \"Test\")\n",
    "\n",
    "# ===============================================================\n",
    "# Print shapes\n",
    "# ===============================================================\n",
    "print(\"✅ Train shape:\", x_train.shape, y_train.shape)\n",
    "print(\"✅ Test shape :\", x_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6ae715-4572-4192-a0f9-e7cac33ff5e7",
   "metadata": {},
   "source": [
    "# CV WEIGHT OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19180342-8f11-431d-ac79-86daef69e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.DataStructs.cDataStructs import ExplicitBitVect\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import ast\n",
    "import os\n",
    "\n",
    "# ===============================================================\n",
    "# Konversi fingerprint (pakai format DataFrame yang sudah ada)\n",
    "# ===============================================================\n",
    "def convert_list_to_bitvect(fp_list):\n",
    "    if isinstance(fp_list, str):\n",
    "        fp_list = ast.literal_eval(fp_list)\n",
    "    n_bits = len(fp_list)\n",
    "    bv = ExplicitBitVect(n_bits)\n",
    "    for i, bit in enumerate(fp_list):\n",
    "        if int(bit):\n",
    "            bv.SetBit(i)\n",
    "    return bv\n",
    "\n",
    "def prepare_fingerprints(df, fingerprint_cols):\n",
    "    fps_dict = {}\n",
    "    for col in fingerprint_cols:\n",
    "        fps_dict[col] = [convert_list_to_bitvect(x) for x in df[col]]\n",
    "    return fps_dict\n",
    "\n",
    "# ===============================================================\n",
    "# Helper metrik & BACC\n",
    "# ===============================================================\n",
    "def compute_bacc_from_probs(y_true, y_prob, thresh=0.5):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = (np.asarray(y_prob) >= thresh).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    return 0.5 * (sens + spec)\n",
    "\n",
    "# ===============================================================\n",
    "# Read-Across: similarity-weighted kNN (Eq. 4 manuskrip)\n",
    "# ===============================================================\n",
    "def compute_tanimoto_similarity(target_fp, reference_fps):\n",
    "    return np.array([\n",
    "        DataStructs.TanimotoSimilarity(target_fp, ref_fp)\n",
    "        for ref_fp in reference_fps\n",
    "    ])\n",
    "\n",
    "def predict_properties_weighted(\n",
    "    test_fps,\n",
    "    train_fps,\n",
    "    y_train,\n",
    "    k=5,\n",
    "    tanimoto_cutoff=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    P_f(q) = Σ w_j y_j / Σ w_j, w_j = Tanimoto(q,j)\n",
    "    \"\"\"\n",
    "    y_train = np.asarray(y_train, dtype=float)\n",
    "    n_test = len(test_fps)\n",
    "    probs = np.zeros(n_test, dtype=float)\n",
    "\n",
    "    for i, test_fp in enumerate(test_fps):\n",
    "        sims = compute_tanimoto_similarity(test_fp, train_fps)\n",
    "\n",
    "        # filter analog dengan cut-off (jika ingin)\n",
    "        if tanimoto_cutoff > 0.0:\n",
    "            valid_idx = np.where(sims >= tanimoto_cutoff)[0]\n",
    "        else:\n",
    "            valid_idx = np.arange(len(sims))\n",
    "\n",
    "        if len(valid_idx) == 0:\n",
    "            probs[i] = y_train.mean()\n",
    "            continue\n",
    "\n",
    "        sims_valid = sims[valid_idx]\n",
    "\n",
    "        # ambil k tetangga paling mirip dari yang valid\n",
    "        if len(sims_valid) > k:\n",
    "            nn_local_idx = np.argsort(sims_valid)[-k:]\n",
    "            nn_idx = valid_idx[nn_local_idx]\n",
    "        else:\n",
    "            nn_idx = valid_idx\n",
    "\n",
    "        nn_sims = sims[nn_idx]\n",
    "        nn_labels = y_train[nn_idx]\n",
    "\n",
    "        weights = nn_sims.copy()\n",
    "        if weights.sum() == 0:\n",
    "            probs[i] = nn_labels.mean()\n",
    "        else:\n",
    "            probs[i] = np.sum(weights * nn_labels) / np.sum(weights)\n",
    "\n",
    "    return probs\n",
    "\n",
    "# ===============================================================\n",
    "# 10-fold scaffold CV untuk Read-Across per fingerprint\n",
    "# ===============================================================\n",
    "def get_bemis_murcko_scaffold(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold) if scaffold is not None else None\n",
    "\n",
    "def scaffold_kfold_indices(smiles_list, n_splits=10, random_state=42):\n",
    "    \"\"\"\n",
    "    Menghasilkan indeks train/val berbasis scaffold (approx),\n",
    "    dengan KFold pada level scaffolds.\n",
    "    \"\"\"\n",
    "    scaffolds = [get_bemis_murcko_scaffold(smi) for smi in smiles_list]\n",
    "    scaffold_to_indices = {}\n",
    "    for idx, scaf in enumerate(scaffolds):\n",
    "        scaffold_to_indices.setdefault(scaf, []).append(idx)\n",
    "\n",
    "    unique_scaffolds = list(scaffold_to_indices.keys())\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(unique_scaffolds)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    folds = []\n",
    "    scaffold_array = np.array(unique_scaffolds)\n",
    "    for scaf_train_idx, scaf_val_idx in kf.split(scaffold_array):\n",
    "        train_idx = []\n",
    "        val_idx = []\n",
    "        for i in scaf_train_idx:\n",
    "            train_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        for i in scaf_val_idx:\n",
    "            val_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        folds.append((np.array(train_idx, dtype=int),\n",
    "                      np.array(val_idx, dtype=int)))\n",
    "    return folds\n",
    "\n",
    "def compute_Sf_via_scaffold_cv(\n",
    "    df,\n",
    "    fingerprint_cols,\n",
    "    outcome_col=\"Outcome\",\n",
    "    n_splits=10,\n",
    "    k_neighbors=5,\n",
    "    tanimoto_cutoff=0.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Menghitung AUC, BACC, dan Sf per fingerprint pakai 10-fold scaffold-CV\n",
    "    di training set (sesuai definisi Sf di manuskrip).\n",
    "    \"\"\"\n",
    "    # outcome\n",
    "    y = df[outcome_col].astype(int).values\n",
    "\n",
    "    # siapkan SMILES untuk scaffold grouping\n",
    "    smiles = df[\"SMILES\"].astype(str).values\n",
    "\n",
    "    # precompute fingerprints RDKit objects per kolom\n",
    "    fps_all = prepare_fingerprints(df, fingerprint_cols)\n",
    "\n",
    "    folds = scaffold_kfold_indices(smiles, n_splits=n_splits, random_state=42)\n",
    "\n",
    "    Sf_results = {}\n",
    "\n",
    "    for fp_col in fingerprint_cols:\n",
    "        print(f\"\\n=== CV Read-Across untuk fingerprint: {fp_col} ===\")\n",
    "        auc_scores = []\n",
    "        bacc_scores = []\n",
    "\n",
    "        fps = np.array(fps_all[fp_col], dtype=object)\n",
    "\n",
    "        for fold_id, (idx_train, idx_val) in enumerate(folds, start=1):\n",
    "            y_tr = y[idx_train]\n",
    "            y_val = y[idx_val]\n",
    "\n",
    "            fps_tr = fps[idx_train]\n",
    "            fps_val = fps[idx_val]\n",
    "\n",
    "            # prediksi RA weighted kNN di fold ini\n",
    "            probs_val = predict_properties_weighted(\n",
    "                test_fps=fps_val,\n",
    "                train_fps=fps_tr,\n",
    "                y_train=y_tr,\n",
    "                k=k_neighbors,\n",
    "                tanimoto_cutoff=tanimoto_cutoff\n",
    "            )\n",
    "\n",
    "            # metrik\n",
    "            # handle kasus degenerate (semua label sama + probabilitas sama)\n",
    "            try:\n",
    "                auc = roc_auc_score(y_val, probs_val)\n",
    "            except ValueError:\n",
    "                auc = np.nan\n",
    "            bacc = compute_bacc_from_probs(y_val, probs_val, thresh=0.5)\n",
    "\n",
    "            auc_scores.append(auc)\n",
    "            bacc_scores.append(bacc)\n",
    "\n",
    "            print(f\"Fold {fold_id}: AUC={auc:.4f}, BACC={bacc:.4f}\")\n",
    "\n",
    "        # buang NaN AUC (jika ada) sebelum rata-rata\n",
    "        auc_scores = np.array(auc_scores, dtype=float)\n",
    "        bacc_scores = np.array(bacc_scores, dtype=float)\n",
    "\n",
    "        valid = ~np.isnan(auc_scores)\n",
    "        if valid.sum() == 0:\n",
    "            mean_auc = np.nan\n",
    "        else:\n",
    "            mean_auc = auc_scores[valid].mean()\n",
    "\n",
    "        mean_bacc = bacc_scores.mean()\n",
    "\n",
    "        Sf = 0.5 * (mean_auc + mean_bacc) if not np.isnan(mean_auc) else mean_bacc\n",
    "\n",
    "        Sf_results[fp_col] = {\n",
    "            \"Mean_AUC_CV\": mean_auc,\n",
    "            \"Mean_BACC_CV\": mean_bacc,\n",
    "            \"Sf\": Sf\n",
    "        }\n",
    "\n",
    "        print(f\"===> {fp_col}: mean AUC={mean_auc:.4f}, mean BACC={mean_bacc:.4f}, Sf={Sf:.4f}\")\n",
    "\n",
    "    return Sf_results\n",
    "\n",
    "# ===============================================================\n",
    "# PANGGIL FUNGSI UNTUK TRAIN SET ANDA\n",
    "# (train_df sudah disiapkan dari preprocessing yang Anda tulis)\n",
    "# ===============================================================\n",
    "\n",
    "fingerprint_columns = [\n",
    "    'Morgan_Descriptors',\n",
    "    'MACCS_Descriptors',\n",
    "    'APF_Descriptors',\n",
    "    'RDK_Descriptors'\n",
    "]\n",
    "\n",
    "# parameter RA\n",
    "k_neighbors = 5\n",
    "tanimoto_cutoff = 0.0    # set ke 0.3/0.4/0.5 kalau mau cut-off similarity eksplisit\n",
    "\n",
    "Sf_results = compute_Sf_via_scaffold_cv(\n",
    "    df=train_df,\n",
    "    fingerprint_cols=fingerprint_columns,\n",
    "    outcome_col=\"Outcome\",\n",
    "    n_splits=10,\n",
    "    k_neighbors=k_neighbors,\n",
    "    tanimoto_cutoff=tanimoto_cutoff\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# Simpan Sf ke CSV di path yang sama seperti sebelumnya\n",
    "# ===============================================================\n",
    "output_dir = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Acute Dermal Toxicity\\Read Across\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "sf_csv_path = os.path.join(output_dir, \"Sf_per_fingerprint.csv\")\n",
    "\n",
    "rows = []\n",
    "for fp, vals in Sf_results.items():\n",
    "    rows.append({\n",
    "        \"Fingerprint\": fp,\n",
    "        \"Mean_AUC_CV\": vals[\"Mean_AUC_CV\"],\n",
    "        \"Mean_BACC_CV\": vals[\"Mean_BACC_CV\"],\n",
    "        \"Sf\": vals[\"Sf\"]\n",
    "    })\n",
    "\n",
    "sf_df = pd.DataFrame(rows)\n",
    "sf_df.to_csv(sf_csv_path, index=False)\n",
    "\n",
    "print(\"\\n=== Sf per fingerprint disimpan ke ===\")\n",
    "print(sf_csv_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my-work-env]",
   "language": "python",
   "name": "conda-env-my-work-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
