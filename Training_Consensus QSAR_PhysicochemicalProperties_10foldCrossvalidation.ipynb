{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Standard Libraries\n",
    "# =========================\n",
    "import sys\n",
    "import os\n",
    "import csv\n",
    "import bz2\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "import multiprocessing\n",
    "import warnings\n",
    "from glob import glob\n",
    "\n",
    "# Suppress warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn = warn\n",
    "\n",
    "# =========================\n",
    "# IPython Extensions\n",
    "# =========================\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# =========================\n",
    "# Data Handling\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# RDKit\n",
    "# =========================\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, PandasTools, Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem.Descriptors import MolLogP\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
    "from rdkit.DataStructs import ExplicitBitVect\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from rdkit.Chem.AtomPairs import Pairs\n",
    "from rdkit.Chem import rdMolDescriptors\n",
    "\n",
    "# =========================\n",
    "# Standardiser\n",
    "# =========================\n",
    "from standardiser import break_bonds, neutralise, rules, unsalt\n",
    "from standardiser.utils import StandardiseException, sanity_check\n",
    "\n",
    "# =========================\n",
    "# Machine Learning / Scikit-learn\n",
    "# =========================\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, confusion_matrix, precision_score,\n",
    "    recall_score, f1_score, cohen_kappa_score, make_scorer\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    ShuffleSplit, RepeatedStratifiedKFold, StratifiedShuffleSplit,\n",
    "    GridSearchCV, StratifiedKFold\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "\n",
    "# ==========================\n",
    "# Load dataset\n",
    "# ==========================\n",
    "train_df = pd.read_excel(r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\FDA MDD (manual split)\\Dataset\\Train_set_FDAMMD_with_fingerprints_sorted_with_RDKit_and_CDK_features.xlsx\")\n",
    "\n",
    "# ==========================\n",
    "# 1️⃣ Ukuran dataset dan distribusi kelas\n",
    "# ==========================\n",
    "print(\"=== Dataset Info ===\")\n",
    "print(f\"Total rows: {train_df.shape[0]}, Total features (excluding SMILES/target): {train_df.shape[1]-2}\")\n",
    "print(\"\\nClass distribution:\")\n",
    "print(train_df['Outcome'].value_counts())\n",
    "print(\"\\nClass ratio:\")\n",
    "print(train_df['Outcome'].value_counts(normalize=True))\n",
    "\n",
    "# ==========================\n",
    "# 2️⃣ Cek duplikasi SMILES\n",
    "# ==========================\n",
    "if 'SMILES' in train_df.columns:\n",
    "    dup_count = train_df['SMILES'].duplicated().sum()\n",
    "    print(f\"\\nNumber of duplicated SMILES: {dup_count}\")\n",
    "else:\n",
    "    print(\"\\nSMILES column not found for duplicate check.\")\n",
    "\n",
    "# ==========================\n",
    "# 3️⃣ Cek NaN dan tipe fitur\n",
    "# ==========================\n",
    "numeric_features = train_df.drop(columns=['SMILES','Outcome'], errors='ignore').select_dtypes(include=['int64','float64'])\n",
    "print(f\"\\nNumber of numeric features: {numeric_features.shape[1]}\")\n",
    "print(f\"Any missing values: {numeric_features.isna().sum().sum()}\")\n",
    "\n",
    "print(\"\\nFeature types:\")\n",
    "print(train_df.dtypes.value_counts())\n",
    "\n",
    "# ==========================\n",
    "# 4️⃣ Korelasi fitur dengan target\n",
    "# ==========================\n",
    "corr = numeric_features.corrwith(train_df['Outcome'])\n",
    "top_corr = corr.abs().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 features most correlated with target:\")\n",
    "print(top_corr)\n",
    "\n",
    "# ==========================\n",
    "# 5️⃣ Scaffold uniqueness (opsional)\n",
    "# ==========================\n",
    "if 'SMILES' in train_df.columns:\n",
    "    try:\n",
    "        scaffolds = train_df['SMILES'].apply(lambda s: MurckoScaffold.MurckoScaffoldSmiles(smiles=s))\n",
    "        print(f\"\\nNumber of unique Murcko scaffolds: {scaffolds.nunique()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing scaffolds: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "\n",
    "# ==========================\n",
    "# 데이터셋 불러오기\n",
    "# ==========================\n",
    "train_df = pd.read_excel(r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\FDA MDD (manual split)\\Dataset\\Train_set_FDAMMD_with_fingerprints_sorted_with_RDKit_and_CDK_features.xlsx\")\n",
    "\n",
    "# ==========================\n",
    "# 1️⃣ 데이터셋 크기와 클래스 분포 확인\n",
    "# ==========================\n",
    "print(\"=== Dataset Info ===\")\n",
    "# 데이터 행(row) 수와 특징(feature) 수 출력 (SMILES와 Outcome 제외)\n",
    "print(f\"Total rows: {train_df.shape[0]}, Total features (excluding SMILES/target): {train_df.shape[1]-2}\")\n",
    "\n",
    "# 클래스 분포 출력\n",
    "print(\"\\nClass distribution:\")\n",
    "print(train_df['Outcome'].value_counts())\n",
    "\n",
    "# 클래스 비율 출력\n",
    "print(\"\\nClass ratio:\")\n",
    "print(train_df['Outcome'].value_counts(normalize=True))\n",
    "\n",
    "# ==========================\n",
    "# 2️⃣ SMILES 중복 확인\n",
    "# ==========================\n",
    "if 'SMILES' in train_df.columns:\n",
    "    dup_count = train_df['SMILES'].duplicated().sum()\n",
    "    print(f\"\\nNumber of duplicated SMILES: {dup_count}\")\n",
    "else:\n",
    "    print(\"\\nSMILES column not found for duplicate check.\")  # SMILES 컬럼이 없을 경우 메시지 출력\n",
    "\n",
    "# ==========================\n",
    "# 3️⃣ 결측치(NaN) 및 특징 타입 확인\n",
    "# ==========================\n",
    "# SMILES와 Outcome 컬럼 제외 후 숫자형(numeric) 특징 선택\n",
    "numeric_features = train_df.drop(columns=['SMILES','Outcome'], errors='ignore').select_dtypes(include=['int64','float64'])\n",
    "\n",
    "print(f\"\\nNumber of numeric features: {numeric_features.shape[1]}\")  # 숫자형 feature 수\n",
    "print(f\"Any missing values: {numeric_features.isna().sum().sum()}\")   # 결측치 확인\n",
    "\n",
    "print(\"\\nFeature types:\")\n",
    "print(train_df.dtypes.value_counts())  # 각 컬럼 타입 개수 확인\n",
    "\n",
    "# ==========================\n",
    "# 4️⃣ 특징과 타겟 간 상관관계 확인\n",
    "# ==========================\n",
    "# 숫자형 특징과 Outcome 간 상관계수 계산\n",
    "corr = numeric_features.corrwith(train_df['Outcome'])\n",
    "# 절댓값 기준 상위 10개 특징 출력\n",
    "top_corr = corr.abs().sort_values(ascending=False).head(10)\n",
    "print(\"\\nTop 10 features most correlated with target:\")\n",
    "print(top_corr)\n",
    "\n",
    "# ==========================\n",
    "# 5️⃣ Murcko Scaffold 고유성 확인 (옵션)\n",
    "# ==========================\n",
    "if 'SMILES' in train_df.columns:\n",
    "    try:\n",
    "        # 각 SMILES에 대해 Murcko Scaffold 추출\n",
    "        scaffolds = train_df['SMILES'].apply(lambda s: MurckoScaffold.MurckoScaffoldSmiles(smiles=s))\n",
    "        # 고유한 Scaffold 수 출력\n",
    "        print(f\"\\nNumber of unique Murcko scaffolds: {scaffolds.nunique()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing scaffolds: {e}\")  # 에러 발생 시 메시지 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# 라이브러리 임포트\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, accuracy_score, roc_auc_score, \n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ================================\n",
    "# 데이터셋 불러오기\n",
    "# ================================\n",
    "data_file = train_path  # 데이터셋 파일 경로\n",
    "df = pd.read_excel(data_file)\n",
    "\n",
    "# ================================\n",
    "# 특징(feature)과 타겟(target) 설정\n",
    "# ================================\n",
    "drop_cols = ['SMILES', 'Morgan_Descriptors', 'MACCS_Descriptors', 'APF_Descriptors', 'Outcome']\n",
    "X_values = df.drop(columns=drop_cols).values  # 입력 특징\n",
    "y_values = df['Outcome'].astype(int).values   # 타겟 변수\n",
    "\n",
    "print(\"Dataset loaded:\", X_values.shape, \"features,\", len(y_values), \"samples\")  # 데이터 크기 확인\n",
    "\n",
    "# ================================\n",
    "# 하이퍼파라미터 그리드 설정\n",
    "# ================================\n",
    "paramgrid = {\n",
    "    \"max_features\": [\n",
    "        X_values.shape[1],\n",
    "        X_values.shape[1] // 2,\n",
    "        X_values.shape[1] // 4,\n",
    "        X_values.shape[1] // 12,\n",
    "        X_values.shape[1] // 10,\n",
    "        X_values.shape[1] // 7,\n",
    "        X_values.shape[1] // 5,\n",
    "        X_values.shape[1] // 3\n",
    "    ],\n",
    "    \"n_estimators\": [10, 100, 300, 500],\n",
    "}\n",
    "\n",
    "# 사용자 정의 스코어 (quadratic weighted kappa)\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)  # 10-fold CV\n",
    "\n",
    "# ================================\n",
    "# 평가 지표 저장용 컨테이너\n",
    "# ================================\n",
    "accuracies, auc_scores, precisions, recalls, f1_scores = [], [], [], [], []\n",
    "specificities, sensitivity_scores, ppvs, npvs, ccrs = [], [], [], [], []\n",
    "confusion_matrices = []\n",
    "\n",
    "# CCR 계산 함수 정의\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2\n",
    "\n",
    "# ================================\n",
    "# 교차검증(CV) 루프\n",
    "# ================================\n",
    "for train_idx, test_idx in tqdm(cv.split(X_values, y_values), total=cv.get_n_splits(), desc=\"CV folds\"):\n",
    "    # 학습/검증 데이터 분리\n",
    "    X_train, X_test = X_values[train_idx], X_values[test_idx]\n",
    "    y_train, y_test = y_values[train_idx], y_values[test_idx]\n",
    "    \n",
    "    # GridSearchCV를 이용한 RF 모델 학습\n",
    "    grid = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(class_weight='balanced'),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_  # 최적 모델 선택\n",
    "    \n",
    "    # 예측 수행\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # 평가 지표 계산\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_proba))\n",
    "    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    \n",
    "    ppvs.append(tp / (tp + fp) if (tp + fp) > 0 else 0)  # 양성 예측도(PPV)\n",
    "    npvs.append(tn / (tn + fn) if (tn + fn) > 0 else 0)  # 음성 예측도(NPV)\n",
    "    ccrs.append(calculate_ccr(sensitivity, specificity))  # CCR 계산\n",
    "\n",
    "# ================================\n",
    "# CV 결과 요약 (fold 평균)\n",
    "# ================================\n",
    "metrics_summary = {\n",
    "    \"Accuracy\": np.mean(accuracies),\n",
    "    \"AUC\": np.mean(auc_scores),\n",
    "    \"Precision\": np.mean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.mean(sensitivity_scores),\n",
    "    \"F1\": np.mean(f1_scores),\n",
    "    \"Specificity\": np.mean(specificities),\n",
    "    \"PPV\": np.mean(ppvs),\n",
    "    \"NPV\": np.mean(npvs),\n",
    "    \"CCR\": np.mean(ccrs)\n",
    "}\n",
    "\n",
    "print(\"\\n===== CV Results =====\")\n",
    "for k, v in metrics_summary.items():\n",
    "    print(f\"CV {k}: {v:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# 전체 데이터셋으로 최종 모델 학습\n",
    "# ================================\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced'),\n",
    "    param_grid=paramgrid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "grid_final.fit(X_values, y_values)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# ================================\n",
    "# 모델 및 평가 지표 저장\n",
    "# ================================\n",
    "output_dir = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 모델 저장\n",
    "model_path = os.path.join(output_dir, \"Dermal_rf_rdkitcdk.pkl\")\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(f\"\\nFinal model saved: {model_path}\")\n",
    "\n",
    "# 평가 지표 저장\n",
    "metrics_path = os.path.join(output_dir, \"Dermal_rf_rdkitcdk_metrics.xlsx\")\n",
    "pd.DataFrame([metrics_summary]).to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics report saved: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Import libraries\n",
    "# ================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, accuracy_score, roc_auc_score, \n",
    "    confusion_matrix, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# ================================\n",
    "# Load dataset\n",
    "# ================================\n",
    "data_file = train_path\n",
    "df = pd.read_excel(data_file)\n",
    "\n",
    "# ================================\n",
    "# Features & Target\n",
    "# ================================\n",
    "drop_cols = ['SMILES', 'Morgan_Descriptors', 'MACCS_Descriptors', 'APF_Descriptors', 'Outcome']\n",
    "X_values = df.drop(columns=drop_cols).values\n",
    "y_values = df['Outcome'].astype(int).values\n",
    "\n",
    "print(\"Dataset loaded:\", X_values.shape, \"features,\", len(y_values), \"samples\")\n",
    "\n",
    "# ================================\n",
    "# Hyperparameter grid for XGBoost\n",
    "# ================================\n",
    "paramgrid = {\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# ================================\n",
    "# Evaluation containers\n",
    "# ================================\n",
    "accuracies, auc_scores, precisions, recalls, f1_scores = [], [], [], [], []\n",
    "specificities, sensitivity_scores, ppvs, npvs, ccrs = [], [], [], [], []\n",
    "confusion_matrices = []\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2\n",
    "\n",
    "# ================================\n",
    "# Cross-validation loop\n",
    "# ================================\n",
    "for train_idx, test_idx in tqdm(cv.split(X_values, y_values), total=cv.get_n_splits(), desc=\"CV folds\"):\n",
    "    X_train, X_test = X_values[train_idx], X_values[test_idx]\n",
    "    y_train, y_test = y_values[train_idx], y_values[test_idx]\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        estimator=XGBClassifier(\n",
    "            objective='binary:logistic', \n",
    "            use_label_encoder=False, \n",
    "            eval_metric='logloss'\n",
    "        ),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        verbose=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_proba))\n",
    "    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    \n",
    "    ppvs.append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "    npvs.append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "    ccrs.append(calculate_ccr(sensitivity, specificity))\n",
    "\n",
    "# ================================\n",
    "# Report metrics (mean across folds)\n",
    "# ================================\n",
    "metrics_summary = {\n",
    "    \"Accuracy\": np.mean(accuracies),\n",
    "    \"AUC\": np.mean(auc_scores),\n",
    "    \"Precision\": np.mean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.mean(sensitivity_scores),\n",
    "    \"F1\": np.mean(f1_scores),\n",
    "    \"Specificity\": np.mean(specificities),\n",
    "    \"PPV\": np.mean(ppvs),\n",
    "    \"NPV\": np.mean(npvs),\n",
    "    \"CCR\": np.mean(ccrs)\n",
    "}\n",
    "\n",
    "print(\"\\n===== CV Results =====\")\n",
    "for k, v in metrics_summary.items():\n",
    "    print(f\"CV {k}: {v:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# Train final model on full dataset\n",
    "# ================================\n",
    "print(\"\\nTraining final XGBoost model on full dataset with GridSearchCV...\")\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=XGBClassifier(\n",
    "        objective='binary:logistic', \n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    param_grid=paramgrid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_final.fit(X_values, y_values)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# ================================\n",
    "# Save model & metrics\n",
    "# ================================\n",
    "output_dir = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(output_dir, \"Dermal_xgb_rdkitcdk.pkl\")\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(f\"\\nFinal XGBoost model saved: {model_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = os.path.join(output_dir, \"Dermal_xgb_rdkitcdk_metrics.xlsx\")\n",
    "pd.DataFrame([metrics_summary]).to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics report saved: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Data\n",
    "# ================================\n",
    "X_values = x_rdkitcdk.values\n",
    "y_values = y\n",
    "\n",
    "# ================================\n",
    "# Hyperparameter grid SVM (lebih kecil dan cepat)\n",
    "# ================================\n",
    "paramgrid = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"kernel\": ['linear', 'rbf'],\n",
    "    \"gamma\": ['scale']\n",
    "}\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# ================================\n",
    "# RandomizedSearchCV\n",
    "# ================================\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=SVC(probability=True, class_weight='balanced'),\n",
    "    param_distributions=paramgrid,\n",
    "    n_iter=5,          # cukup 5 kombinasi random\n",
    "    scoring=kappa_scorer,\n",
    "    cv=3,              # internal CV kecil\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "rand_search.fit(X_values, y_values)\n",
    "best_svm = rand_search.best_estimator_\n",
    "print(f\"Best params: {rand_search.best_params_}\")\n",
    "\n",
    "# ================================\n",
    "# Fit final model di seluruh dataset\n",
    "# ================================\n",
    "best_svm.fit(X_values, y_values)\n",
    "\n",
    "# ================================\n",
    "# Save model\n",
    "# ================================\n",
    "save_path = r'C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Carcinogencity (manual split)\\Model\\Carcino_rdkitcdk.pkl'\n",
    "joblib.dump(best_svm, save_path, compress=9)\n",
    "print(f\"Final SVM model saved: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Import libraries\n",
    "# ================================\n",
    "# ================================\n",
    "# Load dataset\n",
    "# ================================\n",
    "file_path = train_path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Drop kolom non-fitur\n",
    "drop_cols = ['SMILES', 'Morgan_Descriptors', 'MACCS_Descriptors', 'APF_Descriptors', 'Outcome']\n",
    "X_values = df.drop(columns=drop_cols).values\n",
    "y_values = df['Outcome'].astype(int).values\n",
    "\n",
    "print(\"Dataset loaded:\", X_values.shape, \"features,\", len(y_values), \"samples\")\n",
    "\n",
    "# ================================\n",
    "# Hyperparameter grid\n",
    "# ================================\n",
    "param_grid = {\n",
    "    \"svc__C\": [0.1, 1, 10, 100],\n",
    "    \"svc__kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    \"svc__gamma\": ['scale', 'auto']\n",
    "}\n",
    "\n",
    "# Pipeline: imputasi -> scaling -> SVM\n",
    "pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svc\", SVC(probability=True, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# Custom scorer pakai Cohen’s Kappa\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "cv_outer = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# ================================\n",
    "# Cross-validation loop\n",
    "# ================================\n",
    "accuracies, auc_scores, precisions, recalls, f1_scores = [], [], [], [], []\n",
    "specificities, sensitivity_scores, ppvs, npvs, ccrs = [], [], [], [], []\n",
    "confusion_matrices = []\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2\n",
    "\n",
    "for train_idx, test_idx in tqdm(cv_outer.split(X_values, y_values), total=cv_outer.get_n_splits(), desc=\"CV folds\"):\n",
    "    X_train, X_test = X_values[train_idx], X_values[test_idx]\n",
    "    y_train, y_test = y_values[train_idx], y_values[test_idx]\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_proba))\n",
    "    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "\n",
    "    ppvs.append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "    npvs.append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "    ccrs.append(calculate_ccr(sensitivity, specificity))\n",
    "\n",
    "# ================================\n",
    "# Report metrics (mean across folds)\n",
    "# ================================\n",
    "metrics_summary = {\n",
    "    \"Accuracy\": np.mean(accuracies),\n",
    "    \"AUC\": np.mean(auc_scores),\n",
    "    \"Precision\": np.mean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.mean(sensitivity_scores),\n",
    "    \"F1\": np.mean(f1_scores),\n",
    "    \"Specificity\": np.mean(specificities),\n",
    "    \"PPV\": np.mean(ppvs),\n",
    "    \"NPV\": np.mean(npvs),\n",
    "    \"CCR\": np.mean(ccrs)\n",
    "}\n",
    "\n",
    "print(\"\\n===== CV Results =====\")\n",
    "for k, v in metrics_summary.items():\n",
    "    print(f\"CV {k}: {v:.4f}\")\n",
    "\n",
    "# ================================\n",
    "# Train final model di seluruh dataset\n",
    "# ================================\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")\n",
    "grid_final.fit(X_values, y_values)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# ================================\n",
    "# Save model & metrics\n",
    "# ================================\n",
    "output_dir = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(output_dir, \"Dermal_svm_rdkitcdk.pkl\")\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(f\"\\nFinal SVM model saved: {model_path}\")\n",
    "\n",
    "# Save metrics\n",
    "metrics_path = os.path.join(output_dir, \"Dermal_svm_rdkitcdk_metrics.xlsx\")\n",
    "pd.DataFrame([metrics_summary]).to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics report saved: {metrics_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
