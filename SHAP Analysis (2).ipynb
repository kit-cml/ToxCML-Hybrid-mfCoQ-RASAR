{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46b5258-fa7a-483c-a052-d162ff76dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "\n",
    "# =========================\n",
    "# Konfigurasi path\n",
    "# =========================\n",
    "\n",
    "TRAIN_PATH = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\AMES Mutagenicity\\Train set.xlsx\"\n",
    "\n",
    "MODEL_MACCS  = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\AMES Mutagenicity\\QSAR\\AMES_rf_macckeys.pkl\"\n",
    "MODEL_MORGAN = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\AMES Mutagenicity\\QSAR\\AMES_rf_morgan.pkl\"\n",
    "MODEL_APF    = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\AMES Mutagenicity\\QSAR\\AMES_xgb_apf.pkl\"\n",
    "MODEL_PHYS   = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\AMES Mutagenicity\\QSAR\\AMES_rf_rdkitcdk.pkl\"\n",
    "\n",
    "BASE_OUT = r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\"\n",
    "ENDPOINT_NAME = \"AMES_Mutagenicity\"\n",
    "OUT_DIR = os.path.join(BASE_OUT, ENDPOINT_NAME)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Opsional: sembunyikan warning mismatch sklearn version\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
    "\n",
    "# =========================\n",
    "# Helper functions\n",
    "# =========================\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load model dengan joblib.\"\"\"\n",
    "    return joblib.load(model_path)\n",
    "\n",
    "def get_model_type(model):\n",
    "    \"\"\"Identifikasi tipe model (tree vs lainnya).\"\"\"\n",
    "    name = model.__class__.__name__.lower()\n",
    "    if \"xgb\" in name or \"forest\" in name or \"tree\" in name or \"boost\" in name:\n",
    "        return \"tree\"\n",
    "    elif \"svc\" in name or \"svm\" in name:\n",
    "        return \"svm\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "def subsample_X(X, n=1000, random_state=42):\n",
    "    \"\"\"Subsampling supaya SHAP jauh lebih cepat.\"\"\"\n",
    "    if len(X) > n:\n",
    "        return X.sample(n, random_state=random_state)\n",
    "    return X\n",
    "\n",
    "def compute_shap_values(model, X, model_type, n_background=500, class_index=1):\n",
    "    \"\"\"\n",
    "    Hitung SHAP untuk satu kelas (class_index, mis. 1 = toxic).\n",
    "    Return shap_values dengan shape (n_samples, n_features). [web:1][web:31]\n",
    "    \"\"\"\n",
    "    if model_type == \"tree\":\n",
    "        if len(X) > n_background:\n",
    "            background = X.sample(n_background, random_state=42)\n",
    "        else:\n",
    "            background = X\n",
    "\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            data=background,\n",
    "            feature_perturbation=\"interventional\",\n",
    "            model_output=\"probability\"\n",
    "        )\n",
    "        sv = explainer.shap_values(X, check_additivity=False)\n",
    "    else:\n",
    "        if len(X) > n_background:\n",
    "            background = X.sample(n_background, random_state=42)\n",
    "        else:\n",
    "            background = X\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "        sv = explainer.shap_values(X)\n",
    "\n",
    "    # Normalisasi bentuk: ambil SHAP untuk kelas target\n",
    "    if isinstance(sv, list):\n",
    "        sv = np.array(sv[class_index])  # (n_samples, n_features)\n",
    "    else:\n",
    "        sv = np.array(sv)\n",
    "        # kasus (n_samples, n_features, n_classes)\n",
    "        if sv.ndim == 3:\n",
    "            sv = sv[:, :, class_index]\n",
    "\n",
    "    # kalau masih >2D, flatten fitur\n",
    "    if sv.ndim > 2:\n",
    "        sv = sv.reshape(sv.shape[0], -1)\n",
    "\n",
    "    return explainer, sv  # sv: (n_samples, n_features)\n",
    "\n",
    "def save_shap_summary(shap_values, X, out_path_png, max_display=20):\n",
    "    \"\"\"Summary plot SHAP (global importance) untuk satu kelas.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sv = np.array(shap_values)  # (n_samples, n_features)\n",
    "    shap.summary_plot(\n",
    "        sv,\n",
    "        X,\n",
    "        feature_names=X.columns.tolist(),\n",
    "        show=False,\n",
    "        max_display=max_display\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def expand_list_column(df, col_name, prefix):\n",
    "    \"\"\"\n",
    "    df[col_name] berisi string/list seperti '[0, 1, 0, ...]'.\n",
    "    Return: DataFrame dengan kolom prefix_0, prefix_1, ...\n",
    "    \"\"\"\n",
    "    series = df[col_name].apply(lambda x: ast.literal_eval(str(x)))\n",
    "    n_bits = len(series.iloc[0])\n",
    "    arr = np.vstack(series.values)\n",
    "    cols = [f\"{prefix}_{i}\" for i in range(n_bits)]\n",
    "    return pd.DataFrame(arr, columns=cols, index=df.index)\n",
    "\n",
    "def get_top_shap_features(shap_values, X, top_n=20):\n",
    "    \"\"\"\n",
    "    Ambil top-N fitur berdasarkan mean(|SHAP|) untuk satu kelas.\n",
    "    shap_values diharapkan (n_samples, n_features). [web:63]\n",
    "    \"\"\"\n",
    "    sv = np.array(shap_values)\n",
    "    if sv.ndim > 2:\n",
    "        sv = sv.reshape(sv.shape[0], -1)\n",
    "\n",
    "    mean_abs = np.mean(np.abs(sv), axis=0)   # (n_features,)\n",
    "\n",
    "    # cek konsistensi dengan X\n",
    "    n_feat_X = X.shape[1]\n",
    "    if mean_abs.shape[0] != n_feat_X:\n",
    "        n_common = min(mean_abs.shape[0], n_feat_X)\n",
    "        print(f\"[WARNING] n_features SHAP ({mean_abs.shape[0]}) != n_features X ({n_feat_X}), \"\n",
    "              f\"pakai {n_common} pertama.\")\n",
    "        mean_abs = mean_abs[:n_common]\n",
    "        cols = np.array(X.columns)[:n_common]\n",
    "    else:\n",
    "        cols = np.array(X.columns)\n",
    "\n",
    "    idx = np.argsort(mean_abs)[-top_n:][::-1]\n",
    "    idx = np.asarray(idx).ravel().astype(int)\n",
    "\n",
    "    features = cols[idx]\n",
    "    values = mean_abs[idx]\n",
    "\n",
    "    data = {\n",
    "        \"rank\": np.arange(1, len(idx) + 1),\n",
    "        \"feature\": features,\n",
    "        \"mean_abs_shap\": values,\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def save_shap_dependence(shap_values, X, feature_name, out_path_png):\n",
    "    \"\"\"Dependence plot untuk satu fitur, satu kelas.\"\"\"\n",
    "    sv = np.array(shap_values)  # (n_samples, n_features)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    shap.dependence_plot(\n",
    "        feature_name,\n",
    "        sv,\n",
    "        X,\n",
    "        show=False\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# 1. Load TRAIN set\n",
    "# =========================\n",
    "\n",
    "df_all = pd.read_excel(TRAIN_PATH)\n",
    "print(\"AMES train shape:\", df_all.shape)\n",
    "print(\"Columns:\", df_all.columns.tolist())\n",
    "\n",
    "# =========================\n",
    "# 2. Expand fingerprint columns (MACCS, Morgan, APF)\n",
    "# =========================\n",
    "\n",
    "FP_COL_MORGAN = \"Morgan_Descriptors\"\n",
    "FP_COL_MACCS  = \"MACCS_Descriptors\"\n",
    "FP_COL_APF    = \"APF_Descriptors\"\n",
    "\n",
    "maccs_bits  = expand_list_column(df_all, FP_COL_MACCS,  \"MACCS\")\n",
    "morgan_bits = expand_list_column(df_all, FP_COL_MORGAN, \"Morgan\")\n",
    "apf_bits    = expand_list_column(df_all, FP_COL_APF,    \"APF\")\n",
    "\n",
    "df_all = pd.concat([df_all, maccs_bits, morgan_bits, apf_bits], axis=1)\n",
    "\n",
    "# Hapus kolom fingerprint mentah\n",
    "df_all = df_all.drop(columns=[FP_COL_MACCS, FP_COL_MORGAN, FP_COL_APF])\n",
    "\n",
    "# =========================\n",
    "# 3. Definisi blok fitur per descriptor\n",
    "# =========================\n",
    "\n",
    "NON_FEATURE_COLS = [\n",
    "    \"SMILES\",\n",
    "    \"Outcome\",\n",
    "    \"RDK_Descriptors\",\n",
    "]\n",
    "\n",
    "maccs_cols  = [c for c in df_all.columns if c.startswith(\"MACCS_\")]\n",
    "morgan_cols = [c for c in df_all.columns if c.startswith(\"Morgan_\")]\n",
    "apf_cols    = [c for c in df_all.columns if c.startswith(\"APF_\")]\n",
    "\n",
    "physchem_cols = [\n",
    "    c for c in df_all.columns\n",
    "    if c not in NON_FEATURE_COLS\n",
    "    and c not in maccs_cols\n",
    "    and c not in morgan_cols\n",
    "    and c not in apf_cols\n",
    "]\n",
    "\n",
    "print(\"Num MACCS features:\", len(maccs_cols))\n",
    "print(\"Num Morgan features:\", len(morgan_cols))\n",
    "print(\"Num APF features:\", len(apf_cols))\n",
    "print(\"Num physchem features:\", len(physchem_cols))\n",
    "\n",
    "# =========================\n",
    "# 4. Siapkan X per descriptor + paksa numeric\n",
    "# =========================\n",
    "\n",
    "X_maccs  = df_all[maccs_cols].copy()\n",
    "X_morgan = df_all[morgan_cols].copy()\n",
    "X_apf    = df_all[apf_cols].copy()\n",
    "X_phys   = df_all[physchem_cols].copy()\n",
    "\n",
    "for name, X in [(\"MACCS\", X_maccs), (\"Morgan\", X_morgan), (\"APF\", X_apf), (\"Physchem\", X_phys)]:\n",
    "    X_num = X.apply(pd.to_numeric, errors=\"raise\")\n",
    "    if name == \"MACCS\":\n",
    "        X_maccs = X_num\n",
    "    elif name == \"Morgan\":\n",
    "        X_morgan = X_num\n",
    "    elif name == \"APF\":\n",
    "        X_apf = X_num\n",
    "    else:\n",
    "        X_phys = X_num\n",
    "\n",
    "# =========================\n",
    "# 4b. Subsample untuk SHAP (percepat)\n",
    "# =========================\n",
    "\n",
    "N_SHAP_SAMPLE = 1000  # bisa turunkan ke 500 kalau masih berat\n",
    "\n",
    "X_maccs_sub  = subsample_X(X_maccs,  N_SHAP_SAMPLE)\n",
    "X_morgan_sub = subsample_X(X_morgan, N_SHAP_SAMPLE)\n",
    "X_apf_sub    = subsample_X(X_apf,    N_SHAP_SAMPLE)\n",
    "X_phys_sub   = subsample_X(X_phys,   N_SHAP_SAMPLE)\n",
    "\n",
    "# =========================\n",
    "# 5. SHAP untuk tiap model/descriptor\n",
    "# =========================\n",
    "\n",
    "TOP_DEP = 5  # berapa banyak fitur top yang dibuat dependence plot\n",
    "\n",
    "# --- MACCS ---\n",
    "print(\"AMES (train) - MACCS (RF) SHAP\")\n",
    "model_maccs = load_model(MODEL_MACCS)\n",
    "type_maccs = get_model_type(model_maccs)\n",
    "_, shap_maccs = compute_shap_values(model_maccs, X_maccs_sub, type_maccs)\n",
    "\n",
    "out_maccs = os.path.join(OUT_DIR, \"AMES_train_MACCS_SHAP_summary.png\")\n",
    "save_shap_summary(shap_maccs, X_maccs_sub, out_maccs, max_display=20)\n",
    "\n",
    "df_top_maccs = get_top_shap_features(shap_maccs, X_maccs_sub, top_n=20)\n",
    "csv_maccs = os.path.join(OUT_DIR, \"AMES_train_MACCS_SHAP_top20.csv\")\n",
    "df_top_maccs.to_csv(csv_maccs, index=False)\n",
    "\n",
    "for feat in df_top_maccs[\"feature\"].iloc[:TOP_DEP]:\n",
    "    dep_maccs = os.path.join(OUT_DIR, f\"AMES_train_MACCS_SHAP_dependence_{feat}.png\")\n",
    "    save_shap_dependence(shap_maccs, X_maccs_sub, feat, dep_maccs)\n",
    "\n",
    "# --- Morgan ---\n",
    "print(\"AMES (train) - Morgan (RF) SHAP\")\n",
    "model_morgan = load_model(MODEL_MORGAN)\n",
    "type_morgan = get_model_type(model_morgan)\n",
    "_, shap_morgan = compute_shap_values(model_morgan, X_morgan_sub, type_morgan)\n",
    "\n",
    "out_morgan = os.path.join(OUT_DIR, \"AMES_train_Morgan_SHAP_summary.png\")\n",
    "save_shap_summary(shap_morgan, X_morgan_sub, out_morgan, max_display=20)\n",
    "\n",
    "df_top_morgan = get_top_shap_features(shap_morgan, X_morgan_sub, top_n=20)\n",
    "csv_morgan = os.path.join(OUT_DIR, \"AMES_train_Morgan_SHAP_top20.csv\")\n",
    "df_top_morgan.to_csv(csv_morgan, index=False)\n",
    "\n",
    "for feat in df_top_morgan[\"feature\"].iloc[:TOP_DEP]:\n",
    "    dep_morgan = os.path.join(OUT_DIR, f\"AMES_train_Morgan_SHAP_dependence_{feat}.png\")\n",
    "    save_shap_dependence(shap_morgan, X_morgan_sub, feat, dep_morgan)\n",
    "\n",
    "# --- APF (XGB) ---\n",
    "print(\"AMES (train) - APF (XGB) SHAP\")\n",
    "model_apf = load_model(MODEL_APF)\n",
    "type_apf = get_model_type(model_apf)\n",
    "_, shap_apf = compute_shap_values(model_apf, X_apf_sub, type_apf)\n",
    "\n",
    "out_apf = os.path.join(OUT_DIR, \"AMES_train_APF_SHAP_summary.png\")\n",
    "save_shap_summary(shap_apf, X_apf_sub, out_apf, max_display=20)\n",
    "\n",
    "df_top_apf = get_top_shap_features(shap_apf, X_apf_sub, top_n=20)\n",
    "csv_apf = os.path.join(OUT_DIR, \"AMES_train_APF_SHAP_top20.csv\")\n",
    "df_top_apf.to_csv(csv_apf, index=False)\n",
    "\n",
    "for feat in df_top_apf[\"feature\"].iloc[:TOP_DEP]:\n",
    "    dep_apf = os.path.join(OUT_DIR, f\"AMES_train_APF_SHAP_dependence_{feat}.png\")\n",
    "    save_shap_dependence(shap_apf, X_apf_sub, feat, dep_apf)\n",
    "\n",
    "# --- Physicochemical (RF) ---\n",
    "print(\"AMES (train) - Physchem (RF) SHAP\")\n",
    "model_phys = load_model(MODEL_PHYS)\n",
    "type_phys = get_model_type(model_phys)\n",
    "_, shap_phys = compute_shap_values(model_phys, X_phys_sub, type_phys)\n",
    "\n",
    "out_phys = os.path.join(OUT_DIR, \"AMES_train_Physchem_SHAP_summary.png\")\n",
    "save_shap_summary(shap_phys, X_phys_sub, out_phys, max_display=20)\n",
    "\n",
    "df_top_phys = get_top_shap_features(shap_phys, X_phys_sub, top_n=20)\n",
    "csv_phys = os.path.join(OUT_DIR, \"AMES_train_Physchem_SHAP_top20.csv\")\n",
    "df_top_phys.to_csv(csv_phys, index=False)\n",
    "\n",
    "for feat in df_top_phys[\"feature\"].iloc[:TOP_DEP]:\n",
    "    dep_phys = os.path.join(OUT_DIR, f\"AMES_train_Physchem_SHAP_dependence_{feat}.png\")\n",
    "    save_shap_dependence(shap_phys, X_phys_sub, feat, dep_phys)\n",
    "\n",
    "print(\"Done. SHAP train-set (subsample) plots and tables saved to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9416993-77b3-4661-9006-bd4d15094826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "\n",
    "# =========================\n",
    "# Konfigurasi path\n",
    "# =========================\n",
    "\n",
    "TRAIN_PATH = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Development Toxicity\\Dev_Train set_with_fingerprints_sorted_with_RDKit_and_CDK_features.xlsx\"\n",
    "\n",
    "MODEL_MACCS  = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Development Toxicity\\QSAR\\Dev_SVM_MACCS.pkl\"\n",
    "MODEL_MORGAN = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Development Toxicity\\QSAR\\Dev_xgb_morgan.pkl\"\n",
    "MODEL_APF    = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Development Toxicity\\QSAR\\Dev_rf_apf.pkl\"\n",
    "MODEL_PHYS   = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Development Toxicity\\QSAR\\Dev_rf_rdkitcdk.pkl\"\n",
    "\n",
    "BASE_OUT = r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\"\n",
    "ENDPOINT_NAME = \"Development_Toxicity\"\n",
    "OUT_DIR = os.path.join(BASE_OUT, ENDPOINT_NAME)\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Opsional: sembunyikan warning mismatch sklearn version\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
    "\n",
    "# =========================\n",
    "# Helper functions\n",
    "# =========================\n",
    "\n",
    "def load_model(model_path):\n",
    "    \"\"\"Load model dengan joblib.\"\"\"\n",
    "    return joblib.load(model_path)\n",
    "\n",
    "def get_model_type(model):\n",
    "    \"\"\"Identifikasi tipe model (tree vs lainnya).\"\"\"\n",
    "    name = model.__class__.__name__.lower()\n",
    "    if \"xgb\" in name or \"forest\" in name or \"tree\" in name or \"boost\" in name:\n",
    "        return \"tree\"\n",
    "    elif \"svc\" in name or \"svm\" in name:\n",
    "        return \"svm\"\n",
    "    else:\n",
    "        return \"other\"\n",
    "\n",
    "def subsample_X(X, n=1000, random_state=42):\n",
    "    \"\"\"Subsampling supaya SHAP jauh lebih cepat.\"\"\"\n",
    "    if len(X) > n:\n",
    "        return X.sample(n, random_state=random_state)\n",
    "    return X\n",
    "\n",
    "def compute_shap_values(model, X, model_type, n_background=500, class_index=1):\n",
    "    \"\"\"\n",
    "    Hitung SHAP untuk satu kelas (class_index, mis. 1 = toxic).\n",
    "    Return shap_values dengan shape (n_samples, n_features).\n",
    "    \"\"\"\n",
    "    if model_type == \"tree\":\n",
    "        if len(X) > n_background:\n",
    "            background = X.sample(n_background, random_state=42)\n",
    "        else:\n",
    "            background = X\n",
    "\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model,\n",
    "            data=background,\n",
    "            feature_perturbation=\"interventional\",\n",
    "            model_output=\"probability\"\n",
    "        )\n",
    "        sv = explainer.shap_values(X, check_additivity=False)\n",
    "    else:\n",
    "        if len(X) > n_background:\n",
    "            background = X.sample(n_background, random_state=42)\n",
    "        else:\n",
    "            background = X\n",
    "        explainer = shap.KernelExplainer(model.predict_proba, background)\n",
    "        sv = explainer.shap_values(X)\n",
    "\n",
    "    # Normalisasi bentuk: ambil SHAP untuk kelas target\n",
    "    if isinstance(sv, list):\n",
    "        sv = np.array(sv[class_index])  # (n_samples, n_features)\n",
    "    else:\n",
    "        sv = np.array(sv)\n",
    "        # kasus (n_samples, n_features, n_classes)\n",
    "        if sv.ndim == 3:\n",
    "            sv = sv[:, :, class_index]\n",
    "\n",
    "    # kalau masih >2D, flatten fitur\n",
    "    if sv.ndim > 2:\n",
    "        sv = sv.reshape(sv.shape[0], -1)\n",
    "\n",
    "    return explainer, sv  # sv: (n_samples, n_features)\n",
    "\n",
    "def save_shap_summary(shap_values, X, out_path_png, max_display=20):\n",
    "    \"\"\"Summary plot SHAP (global importance) untuk satu kelas.\"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sv = np.array(shap_values)  # (n_samples, n_features)\n",
    "    shap.summary_plot(\n",
    "        sv,\n",
    "        X,\n",
    "        feature_names=X.columns.tolist(),\n",
    "        show=False,\n",
    "        max_display=max_display\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def expand_list_column(df, col_name, prefix):\n",
    "    \"\"\"\n",
    "    df[col_name] berisi string/list seperti '[0, 1, 0, ...]'.\n",
    "    Return: DataFrame dengan kolom prefix_0, prefix_1, ...\n",
    "    \"\"\"\n",
    "    series = df[col_name].apply(lambda x: ast.literal_eval(str(x)))\n",
    "    n_bits = len(series.iloc[0])\n",
    "    arr = np.vstack(series.values)\n",
    "    cols = [f\"{prefix}_{i}\" for i in range(n_bits)]\n",
    "    return pd.DataFrame(arr, columns=cols, index=df.index)\n",
    "\n",
    "def get_top_shap_features(shap_values, X, top_n=20):\n",
    "    \"\"\"\n",
    "    Ambil top-N fitur berdasarkan mean(|SHAP|) untuk satu kelas.\n",
    "    shap_values diharapkan (n_samples, n_features).\n",
    "    \"\"\"\n",
    "    sv = np.array(shap_values)\n",
    "    if sv.ndim > 2:\n",
    "        sv = sv.reshape(sv.shape[0], -1)\n",
    "\n",
    "    mean_abs = np.mean(np.abs(sv), axis=0)   # (n_features,)\n",
    "\n",
    "    # cek konsistensi dengan X\n",
    "    n_feat_X = X.shape[1]\n",
    "    if mean_abs.shape[0] != n_feat_X:\n",
    "        n_common = min(mean_abs.shape[0], n_feat_X)\n",
    "        print(f\"[WARNING] n_features SHAP ({mean_abs.shape[0]}) != n_features X ({n_feat_X}), \"\n",
    "              f\"pakai {n_common} pertama.\")\n",
    "        mean_abs = mean_abs[:n_common]\n",
    "        cols = np.array(X.columns)[:n_common]\n",
    "    else:\n",
    "        cols = np.array(X.columns)\n",
    "\n",
    "    idx = np.argsort(mean_abs)[-top_n:][::-1]\n",
    "    idx = np.asarray(idx).ravel().astype(int)\n",
    "\n",
    "    features = cols[idx]\n",
    "    values = mean_abs[idx]\n",
    "\n",
    "    data = {\n",
    "        \"rank\": np.arange(1, len(idx) + 1),\n",
    "        \"feature\": features,\n",
    "        \"mean_abs_shap\": values,\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def save_shap_dependence(shap_values, X, feature_name, out_path_png):\n",
    "    \"\"\"Dependence plot untuk satu fitur, satu kelas.\"\"\"\n",
    "    sv = np.array(shap_values)  # (n_samples, n_features)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    shap.dependence_plot(\n",
    "        feature_name,\n",
    "        sv,\n",
    "        X,\n",
    "        show=False\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path_png, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "# =========================\n",
    "# 1. Load TRAIN set\n",
    "# =========================\n",
    "\n",
    "df_all = pd.read_excel(TRAIN_PATH)\n",
    "print(\"Dev train shape:\", df_all.shape)\n",
    "print(\"Columns:\", df_all.columns.tolist())\n",
    "\n",
    "# =========================\n",
    "# 2. Expand fingerprint columns (MACCS, Morgan, APF)\n",
    "# =========================\n",
    "\n",
    "FP_COL_MORGAN = \"Morgan_Descriptors\"\n",
    "FP_COL_MACCS  = \"MACCS_Descriptors\"\n",
    "FP_COL_APF    = \"APF_Descriptors\"\n",
    "\n",
    "maccs_bits  = expand_list_column(df_all, FP_COL_MACCS,  \"MACCS\")\n",
    "morgan_bits = expand_list_column(df_all, FP_COL_MORGAN, \"Morgan\")\n",
    "apf_bits    = expand_list_column(df_all, FP_COL_APF,    \"APF\")\n",
    "\n",
    "df_all = pd.concat([df_all, maccs_bits, morgan_bits, apf_bits], axis=1)\n",
    "\n",
    "# Hapus kolom fingerprint mentah\n",
    "df_all = df_all.drop(columns=[FP_COL_MACCS, FP_COL_MORGAN, FP_COL_APF])\n",
    "\n",
    "# =========================\n",
    "# 3. Definisi blok fitur per descriptor\n",
    "# =========================\n",
    "\n",
    "NON_FEATURE_COLS = [\n",
    "    \"SMILES\",\n",
    "    \"Outcome\",\n",
    "    \"RDK_Descriptors\",\n",
    "]\n",
    "\n",
    "maccs_cols  = [c for c in df_all.columns if c.startswith(\"MACCS_\")]\n",
    "morgan_cols = [c for c in df_all.columns if c.startswith(\"Morgan_\")]\n",
    "apf_cols    = [c for c in df_all.columns if c.startswith(\"APF_\")]\n",
    "\n",
    "physchem_cols = [\n",
    "    c for c in df_all.columns\n",
    "    if c not in NON_FEATURE_COLS\n",
    "    and c not in maccs_cols\n",
    "    and c not in morgan_cols\n",
    "    and c not in apf_cols\n",
    "]\n",
    "\n",
    "print(\"Num MACCS features:\", len(maccs_cols))\n",
    "print(\"Num Morgan features:\", len(morgan_cols))\n",
    "print(\"Num APF features:\", len(apf_cols))\n",
    "print(\"Num physchem features:\", len(physchem_cols))\n",
    "\n",
    "# =========================\n",
    "# 4. Siapkan X per descriptor + paksa numeric\n",
    "# =========================\n",
    "\n",
    "X_maccs  = df_all[maccs_cols].copy()\n",
    "X_morgan = df_all[morgan_cols].copy()\n",
    "X_apf    = df_all[apf_cols].copy()\n",
    "X_phys   = df_all[physchem_cols].copy()\n",
    "\n",
    "for name, X in [(\"MACCS\", X_maccs), (\"Morgan\", X_morgan), (\"APF\", X_apf), (\"Physchem\", X_phys)]:\n",
    "    X_num = X.apply(pd.to_numeric, errors=\"raise\")\n",
    "    if name == \"MACCS\":\n",
    "        X_maccs = X_num\n",
    "    elif name == \"Morgan\":\n",
    "        X_morgan = X_num\n",
    "    elif name == \"APF\":\n",
    "        X_apf = X_num\n",
    "    else:\n",
    "        X_phys = X_num\n",
    "\n",
    "# =========================\n",
    "# 4b. Subsample untuk SHAP (percepat)\n",
    "# =========================\n",
    "\n",
    "N_SHAP_SAMPLE = 1000  # bisa turunkan ke 500 kalau masih berat\n",
    "\n",
    "X_maccs_sub  = subsample_X(X_maccs,  N_SHAP_SAMPLE)\n",
    "X_morgan_sub = subsample_X(X_morgan, N_SHAP_SAMPLE)\n",
    "X_apf_sub    = subsample_X(X_apf,    N_SHAP_SAMPLE)\n",
    "X_phys_sub   = subsample_X(X_phys,   N_SHAP_SAMPLE)\n",
    "\n",
    "# =========================\n",
    "# 5. SHAP untuk tiap model/descriptor\n",
    "# =========================\n",
    "\n",
    "TOP_DEP = 5  # berapa banyak fitur top yang dibuat dependence plot\n",
    "\n",
    "# --- MACCS (SVM) ---\n",
    "print(\"Dev (train) - MACCS (SVM) SHAP\")\n",
    "model_maccs = load_model(MODEL_MACCS)\n",
    "type_maccs = get_model_type(model_maccs)\n",
    "_, shap_maccs = compute_shap_values(model_maccs, X_maccs_sub, type_maccs)\n",
    "\n",
    "out_maccs = os.path.join(OUT_DIR, \"Dev_train_MACCS_SHAP_summary.png\")\n",
    "save_shap_summary(shap_maccs, X_maccs_sub, out_maccs, max_display=20)\n",
    "\n",
    "df_top_maccs = get_top_shap_features(shap_maccs, X_maccs_sub, top_n=20)\n",
    "csv_maccs = os.path.join(OUT_DIR, \"Dev_train_MACCS_SHAP_top20.csv\")\n",
    "df_top_maccs.to_csv(csv_maccs, index=False)\n",
    "\n",
    "for feat in df_top_maccs[\"feature\"].iloc[:TOP_DEP]:\n",
    "    dep_maccs = os.path.join(OUT_DIR, f\"Dev_train_MACCS_SHAP_dependence_{feat}.png\")\n",
    "    save_shap_dependence(shap_maccs, X_maccs_sub, feat, dep_maccs)\n",
    "\n",
    "# --- Morgan (XGB) ---\n",
    "print(\"Dev (train) - Morgan (XGB) SHAP\")\n",
    "model_morgan = load_model(MODEL_MORGAN)\n",
    "type_morgan = get_model_type(model_morgan)\n",
    "_, shap_morgan = compute_shap_values(model_morgan, X_morgan_sub, type_morgan)\n",
    "\n",
    "out_morgan = os.path.join(OUT_DIR, \"Dev_train_Morgan_SHAP_summary.png\")\n",
    "save_shap_summary(shap_morgan, X_morgan_sub, out_morgan, max_display=20)\n",
    "\n",
    "df_top_morgan = get_top_shap_features(shap_morgan, X_morgan_sub, top_n=20)\n",
    "csv_morgan = os.path.join(OUT_DIR, \"Dev_train_Morgan_SHAP_top20.csv\")\n",
    "df_top_morgan.to_csv(csv_morgan, index=False)\n",
    "\n",
    "for feat in df_top_morgan[\"feature\"].iloc[:TOP_DEP]:\n",
    "    dep_morgan = os.path.join(OUT_DIR, f\"Dev_train_Morgan_SHAP_dependence_{feat}.png\")\n",
    "    save_shap_dependence(shap_morgan, X_morgan_sub, feat, dep_morgan)\n",
    "\n",
    "# --- APF (RF) ---\n",
    "print(\"Dev (train) - APF (RF) SHAP\")\n",
    "model_apf = load_model(MODEL_APF)\n",
    "type_apf = get_model_type(model_apf)\n",
    "_, shap_apf = compute_shap_values(model_apf, X_apf_sub, type_apf)\n",
    "\n",
    "out_apf = os.path.join(OUT_DIR, \"Dev_train_APF_SHAP_summary.png\")\n",
    "save_shap_summary(shap_apf, X_apf_sub, out_apf, max_display=20)\n",
    "\n",
    "df_top_apf = get_top_shap_features(shap_apf, X_apf_sub, top_n=20)\n",
    "csv_apf = os.path.join(OUT_DIR, \"Dev_train_APF_SHAP_top20.csv\")\n",
    "df_top_apf.to_csv(csv_apf, index=False)\n",
    "\n",
    "for feat in df_top_apf[\"feature\"].iloc[:TOP_DEP]:\n",
    "    dep_apf = os.path.join(OUT_DIR, f\"Dev_train_APF_SHAP_dependence_{feat}.png\")\n",
    "    save_shap_dependence(shap_apf, X_apf_sub, feat, dep_apf)\n",
    "\n",
    "# --- Physicochemical (RF) ---\n",
    "print(\"Dev (train) - Physchem (RF) SHAP\")\n",
    "model_phys = load_model(MODEL_PHYS)\n",
    "type_phys = get_model_type(model_phys)\n",
    "_, shap_phys = compute_shap_values(model_phys, X_phys_sub, type_phys)\n",
    "\n",
    "out_phys = os.path.join(OUT_DIR, \"Dev_train_Physchem_SHAP_summary.png\")\n",
    "save_shap_summary(shap_phys, X_phys_sub, out_phys, max_display=20)\n",
    "\n",
    "df_top_phys = get_top_shap_features(shap_phys, X_phys_sub, top_n=20)\n",
    "csv_phys = os.path.join(OUT_DIR, \"Dev_train_Physchem_SHAP_top20.csv\")\n",
    "df_top_phys.to_csv(csv_phys, index=False)\n",
    "\n",
    "for feat in df_top_phys[\"feature\"].iloc[:TOP_DEP]:\n",
    "    dep_phys = os.path.join(OUT_DIR, f\"Dev_train_Physchem_SHAP_dependence_{feat}.png\")\n",
    "    save_shap_dependence(shap_phys, X_phys_sub, feat, dep_phys)\n",
    "\n",
    "print(\"Done. SHAP train-set (subsample) plots and tables saved to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81537be1-d645-4bdb-8a1e-f6042b66903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "files = [\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Development_Toxicity\\Dev_train_Physchem_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Development_Toxicity\\Dev_train_APF_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Development_Toxicity\\Dev_train_MACCS_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Development_Toxicity\\Dev_train_Morgan_SHAP_top20.csv\",\n",
    "]\n",
    "\n",
    "for path in files:\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # sesuaikan kalau nama kolom berbeda\n",
    "    feat_col = \"feature\"\n",
    "    shap_col = \"mean_abs_shap\"\n",
    "\n",
    "    df_plot = df.sort_values(shap_col, ascending=True)\n",
    "\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    plt.barh(df_plot[feat_col], df_plot[shap_col], color=\"red\")\n",
    "    plt.xlabel(\"Mean |SHAP| value\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "\n",
    "    base = os.path.basename(path)\n",
    "    title = os.path.splitext(base)[0]\n",
    "    plt.title(title)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    folder = os.path.dirname(path)\n",
    "    png_name = os.path.splitext(base)[0] + \"_SHAP_barh.png\"\n",
    "    out_path = os.path.join(folder, png_name)\n",
    "\n",
    "    plt.savefig(out_path, dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f887dc6-8880-4c5e-a23e-43570aa8d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "paths = [\n",
    "    # Baris 1: Morgan (barh, summary)\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Neurotoxicity\\Neurotox_train_Morgan_SHAP_top20_SHAP_barh.png\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Neurotoxicity\\Neurotox_train_Morgan_SHAP_summary.png\",\n",
    "\n",
    "    # Baris 1: MACCS (barh, summary)\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Neurotoxicity\\Neurotox_train_MACCS_SHAP_top20_SHAP_barh.png\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Neurotoxicity\\Neurotox_train_MACCS_SHAP_summary.png\",\n",
    "\n",
    "    # Baris 2: APF (barh, summary)\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Neurotoxicity\\Neurotox_train_APF_SHAP_top20_SHAP_barh.png\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Neurotoxicity\\Neurotox_train_APF_SHAP_summary.png\",\n",
    "\n",
    "    # Baris 2: Physchem (barh, summary)\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Neurotoxicity\\Neurotox_train_Physchem_SHAP_top20_SHAP_barh.png\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Neurotoxicity\\Neurotox_train_Physchem_SHAP_summary.png\",\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(16, 8))\n",
    "\n",
    "for ax, path in zip(axes.ravel(), paths):\n",
    "    img = mpimg.imread(path)\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "fig.subplots_adjust(\n",
    "    left=0.0, right=1.0, top=1.0, bottom=0.0,\n",
    "    wspace=0.0, hspace=0.0\n",
    ")\n",
    "\n",
    "out_path = r\"C:\\Fauzan\\Manuskrip QSAR 3\\SHAP\\Neurotoxicity\\Neurotox_SHAP_Morgan_MACCS_APF_Physchem_2x4.png\"\n",
    "plt.savefig(out_path, dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c45568-7863-41d3-9dd8-d0c834387697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Daftar file SHAP top-20\n",
    "# ------------------------------------------------------------------\n",
    "files = [\n",
    "    # AMES\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\AMES_Mutagenicity\\AMES_train_Physchem_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\AMES_Mutagenicity\\AMES_train_APF_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\AMES_Mutagenicity\\AMES_train_MACCS_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\AMES_Mutagenicity\\AMES_train_Morgan_SHAP_top20.csv\",\n",
    "\n",
    "    # Cardiotoxicity\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Cardiotoxicity_hERG\\Cardiotox_train_Physchem_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Cardiotoxicity_hERG\\Cardiotox_train_APF_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Cardiotoxicity_hERG\\Cardiotox_train_MACCS_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Cardiotoxicity_hERG\\Cardiotox_train_Morgan_SHAP_top20.csv\",\n",
    "\n",
    "    # Developmental toxicity\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Development_Toxicity\\Dev_train_Morgan_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Development_Toxicity\\Dev_train_Physchem_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Development_Toxicity\\Dev_train_APF_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Development_Toxicity\\Dev_train_MACCS_SHAP_top20.csv\",\n",
    "\n",
    "    # DIN\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Drug_Induced_Nephrotoxicity_DIN\\DIN_train_Morgan_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Drug_Induced_Nephrotoxicity_DIN\\DIN_train_Physchem_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Drug_Induced_Nephrotoxicity_DIN\\DIN_train_APF_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Drug_Induced_Nephrotoxicity_DIN\\DIN_train_MACCS_SHAP_top20.csv\",\n",
    "\n",
    "    # Hepatotoxicity\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Hepatotoxicity\\Hepato_train_Morgan_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Hepatotoxicity\\Hepato_train_Physchem_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Hepatotoxicity\\Hepato_train_APF_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Hepatotoxicity\\Hepato_train_MACCS_SHAP_top20.csv\",\n",
    "\n",
    "    # Neurotoxicity\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Neurotoxicity\\Neurotox_train_Morgan_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Neurotoxicity\\Neurotox_train_Physchem_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Neurotoxicity\\Neurotox_train_APF_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Neurotoxicity\\Neurotox_train_MACCS_SHAP_top20.csv\",\n",
    "\n",
    "    # Respiratory irritation\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Respiratory_Irritation\\Resp_train_Morgan_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Respiratory_Irritation\\Resp_train_Physchem_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Respiratory_Irritation\\Resp_train_APF_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Respiratory_Irritation\\Resp_train_MACCS_SHAP_top20.csv\",\n",
    "\n",
    "    # Skin sensitization\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Skin_Sensitization\\SkinSen_train_Morgan_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Skin_Sensitization\\SkinSen_train_Physchem_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Skin_Sensitization\\SkinSen_train_APF_SHAP_top20.csv\",\n",
    "    r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\Skin_Sensitization\\SkinSen_train_MACCS_SHAP_top20.csv\",\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Baca & gabungkan\n",
    "# ------------------------------------------------------------------\n",
    "dfs = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    # Ambil hanya kolom penting\n",
    "    df = df[[\"rank\", \"feature\"]].copy()\n",
    "\n",
    "    # Ambil metadata dari nama file\n",
    "    fname = Path(f).stem\n",
    "    parts = fname.split(\"_\")\n",
    "\n",
    "    df[\"endpoint\"] = parts[0]\n",
    "    df[\"descriptor\"] = parts[2]  # Physchem / APF / MACCS / Morgan\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "shap_summary = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Urutkan rapi\n",
    "shap_summary = shap_summary.sort_values(\n",
    "    by=[\"endpoint\", \"descriptor\", \"rank\"]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Simpan hasil\n",
    "# ------------------------------------------------------------------\n",
    "out_file = r\"C:\\Fauzan\\Manuskrip QSAR 3\\Revisi 1\\SHAP\\SHAP_top20_rank_feature_summary.csv\"\n",
    "shap_summary.to_csv(out_file, index=False)\n",
    "\n",
    "print(\"Ringkasan SHAP berhasil dibuat:\")\n",
    "print(shap_summary.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee32e42-a2dc-4498-92c2-206fb3af0b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8840a6a-f100-4a64-97a5-d320c2092f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my-work-env]",
   "language": "python",
   "name": "conda-env-my-work-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
