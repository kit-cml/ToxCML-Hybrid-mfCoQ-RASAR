{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a2d1b6-05d9-4966-85bc-be219a96821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Standard library\n",
    "# =========================\n",
    "import os\n",
    "import ast\n",
    "\n",
    "# =========================\n",
    "# Third-party libraries\n",
    "# =========================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# RDKit\n",
    "# =========================\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import (\n",
    "    AllChem,\n",
    "    MACCSkeys,\n",
    "    rdMolDescriptors,\n",
    "    RDKFingerprint,\n",
    "    Descriptors\n",
    ")\n",
    "from rdkit.DataStructs import (\n",
    "    TanimotoSimilarity,\n",
    "    ExplicitBitVect\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# CDK\n",
    "# =========================\n",
    "from CDK_pywrapper import CDK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdb090b-4eaa-4831-a51f-cb7ac702ca04",
   "metadata": {},
   "source": [
    "# mfCoQ-RASAR Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57194a8-9c65-43ba-9009-df532d5008ba",
   "metadata": {},
   "source": [
    "# Weight Optimzation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f65cc8-c684-410b-98f2-2bf39ff5d8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit.DataStructs.cDataStructs import ExplicitBitVect\n",
    "from rdkit import DataStructs\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import ast\n",
    "import os\n",
    "\n",
    "# ===============================================================\n",
    "# 0. Paths & config\n",
    "# ===============================================================\n",
    "train_path = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Acute Dermal Toxicity\\Train_set_Dermal_with_fingerprints_RDKit_CDK.xlsx\"\n",
    "qsar_model_dir = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Acute Dermal Toxicity\\QSAR Model\"\n",
    "ra_train_file = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\Old Endpoints\\Acute Dermal Toxicity (manual split)\\Read Across\\Train_set_Dermal_balanced_with_fingerprints.xlsx\"\n",
    "sf_ra_path = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Acute Dermal Toxicity\\Read Across\\Sf_per_fingerprint.csv\"\n",
    "output_dir = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Acute Dermal Toxicity\\Evaluation\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# kandidat w\n",
    "W_GRID = np.arange(0.01, 1.00, 0.01)\n",
    "\n",
    "# ===============================================================\n",
    "# 1. Load TRAIN set (untuk mfCoQ-RASAR CV)\n",
    "# ===============================================================\n",
    "train_df_qsar = pd.read_excel(train_path)\n",
    "y_all = train_df_qsar['Outcome'].astype(int).values\n",
    "smiles_all = train_df_qsar['SMILES'].astype(str).values\n",
    "\n",
    "# helper: parse fingerprints → np.array\n",
    "def convert_to_array(series):\n",
    "    return np.array(series.apply(ast.literal_eval).tolist())\n",
    "\n",
    "X_qsar_by_desc = {\n",
    "    \"Morgan\":   convert_to_array(train_df_qsar[\"Morgan_Descriptors\"]),\n",
    "    \"MACCS\":    convert_to_array(train_df_qsar[\"MACCS_Descriptors\"]),\n",
    "    \"APF\":      convert_to_array(train_df_qsar[\"APF_Descriptors\"]),\n",
    "    \"Physchem\": train_df_qsar.drop(columns=[\n",
    "        'SMILES', 'Morgan_Descriptors', 'MACCS_Descriptors',\n",
    "        'APF_Descriptors', 'RDK_Descriptors', 'Outcome'\n",
    "    ]).values\n",
    "}\n",
    "\n",
    "descriptors = [\"Morgan\", \"MACCS\", \"APF\", \"Physchem\"]\n",
    "alg_best = {\n",
    "    \"Morgan\":   \"XGB\",\n",
    "    \"MACCS\":    \"SVM\",\n",
    "    \"APF\":      \"RF\",\n",
    "    \"Physchem\": \"XGB\",\n",
    "}\n",
    "\n",
    "# QSAR model paths (best per descriptor)\n",
    "qsar_model_paths = {\n",
    "    \"Morgan\":   os.path.join(qsar_model_dir, \"Dermal_xgb_morgan.pkl\"),\n",
    "    \"MACCS\":    os.path.join(qsar_model_dir, \"Dermal_SVM_MACCS.pkl\"),\n",
    "    \"APF\":      os.path.join(qsar_model_dir, \"Dermal_rf_apf.pkl\"),\n",
    "    \"Physchem\": os.path.join(qsar_model_dir, \"Dermal_xgb_rdkitcdk.pkl\"),\n",
    "}\n",
    "qsar_models = {d: joblib.load(p) for d, p in qsar_model_paths.items()}\n",
    "\n",
    "# Load Sm (QSAR) – hasil 10-fold scaffold-CV QSAR\n",
    "Sm_files = {\n",
    "    \"Morgan\":   os.path.join(qsar_model_dir, \"Sm_Morgan_XGB.csv\"),\n",
    "    \"MACCS\":    os.path.join(qsar_model_dir, \"Sm_MACCS_SVM.csv\"),\n",
    "    \"APF\":      os.path.join(qsar_model_dir, \"Sm_APF_RF.csv\"),\n",
    "    \"Physchem\": os.path.join(qsar_model_dir, \"Sm_Physchem_XGB.csv\"),\n",
    "}\n",
    "Sm_qsar = {}\n",
    "for desc, path in Sm_files.items():\n",
    "    sm_df = pd.read_csv(path)\n",
    "    Sm_qsar[desc] = float(sm_df[\"Sm\"].values[0])\n",
    "\n",
    "# ===============================================================\n",
    "# 2. Scaffold-based 10-fold split (Bemis–Murcko)\n",
    "# ===============================================================\n",
    "def get_bemis_murcko_scaffold(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold) if scaffold is not None else None\n",
    "\n",
    "def scaffold_kfold_indices(smiles_list, n_splits=10, random_state=42):\n",
    "    scaffolds = [get_bemis_murcko_scaffold(smi) for smi in smiles_list]\n",
    "    scaffold_to_indices = {}\n",
    "    for idx, scaf in enumerate(scaffolds):\n",
    "        scaffold_to_indices.setdefault(scaf, []).append(idx)\n",
    "    unique_scaffolds = list(scaffold_to_indices.keys())\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(unique_scaffolds)\n",
    "    # manual split scaffolds ≈ 10-fold\n",
    "    fold_scaffolds = np.array_split(unique_scaffolds, n_splits)\n",
    "    folds = []\n",
    "    for i in range(n_splits):\n",
    "        val_scaf = fold_scaffolds[i]\n",
    "        train_scaf = [s for j, fs in enumerate(fold_scaffolds) if j != i for s in fs]\n",
    "        train_idx = []\n",
    "        val_idx = []\n",
    "        for s in train_scaf:\n",
    "            train_idx.extend(scaffold_to_indices[s])\n",
    "        for s in val_scaf:\n",
    "            val_idx.extend(scaffold_to_indices[s])\n",
    "        folds.append((np.array(train_idx, dtype=int), np.array(val_idx, dtype=int)))\n",
    "    return folds\n",
    "\n",
    "folds = scaffold_kfold_indices(smiles_all, n_splits=10, random_state=42)\n",
    "\n",
    "# ===============================================================\n",
    "# 3. Read-Across: train data & Sf\n",
    "# ===============================================================\n",
    "ra_train_df = pd.read_excel(ra_train_file)\n",
    "y_train_ra = ra_train_df['Outcome'].astype(int).values\n",
    "ra_fps = ['Morgan_Descriptors', 'MACCS_Descriptors', 'RDK_Descriptors']\n",
    "\n",
    "def convert_list_to_bitvect(fp_list):\n",
    "    if isinstance(fp_list, str):\n",
    "        fp_list = ast.literal_eval(fp_list)\n",
    "    n_bits = len(fp_list)\n",
    "    bv = ExplicitBitVect(n_bits)\n",
    "    for i, bit in enumerate(fp_list):\n",
    "        if int(bit):\n",
    "            bv.SetBit(i)\n",
    "    return bv\n",
    "\n",
    "def prepare_fingerprints(df, fingerprint_cols):\n",
    "    fps_dict = {}\n",
    "    for col in fingerprint_cols:\n",
    "        fps_dict[col] = [convert_list_to_bitvect(x) for x in df[col]]\n",
    "    return fps_dict\n",
    "\n",
    "train_fps_ra = prepare_fingerprints(ra_train_df, ra_fps)\n",
    "\n",
    "def compute_tanimoto_similarity(target_fp, reference_fps):\n",
    "    return np.array([DataStructs.TanimotoSimilarity(target_fp, ref_fp) for ref_fp in reference_fps])\n",
    "\n",
    "def predict_ra_weighted(test_fps_list, train_fps_list, y_train, k=5, tanimoto_cutoff=0.0):\n",
    "    y_train = np.asarray(y_train, dtype=float)\n",
    "    probs = []\n",
    "    for test_fp in test_fps_list:\n",
    "        sims = compute_tanimoto_similarity(test_fp, train_fps_list)\n",
    "        if tanimoto_cutoff > 0.0:\n",
    "            valid_idx = np.where(sims >= tanimoto_cutoff)[0]\n",
    "        else:\n",
    "            valid_idx = np.arange(len(sims))\n",
    "        if len(valid_idx) == 0:\n",
    "            probs.append(y_train.mean())\n",
    "            continue\n",
    "        sims_valid = sims[valid_idx]\n",
    "        if len(sims_valid) > k:\n",
    "            nn_local = np.argsort(sims_valid)[-k:]\n",
    "            nn_idx = valid_idx[nn_local]\n",
    "        else:\n",
    "            nn_idx = valid_idx\n",
    "        nn_sims = sims[nn_idx]\n",
    "        nn_y = y_train[nn_idx]\n",
    "        w = nn_sims.copy()\n",
    "        if w.sum() == 0:\n",
    "            probs.append(nn_y.mean())\n",
    "        else:\n",
    "            probs.append(np.sum(w * nn_y) / np.sum(w))\n",
    "    return np.array(probs)\n",
    "\n",
    "# Sf untuk RA\n",
    "sf_df = pd.read_csv(sf_ra_path)\n",
    "Sf_ra = {}\n",
    "for col in ra_fps:\n",
    "    row = sf_df.loc[sf_df[\"Fingerprint\"] == col]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"Sf untuk {col} tidak ditemukan di {sf_ra_path}\")\n",
    "    Sf_ra[col] = float(row[\"Sf\"].values[0])\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Helper metrik\n",
    "# ---------------------------------------------------------------\n",
    "def compute_bacc_from_preds(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bacc = 0.5 * (sens + spec)\n",
    "    return bacc, sens, spec\n",
    "\n",
    "# ===============================================================\n",
    "# 4. 10-fold scaffold-CV mfCoQ-RASAR untuk cari w*\n",
    "# ===============================================================\n",
    "S_w_list = []  # simpan S_w per weight\n",
    "\n",
    "for w in W_GRID:\n",
    "    fold_scores = []  # S_{f,w}\n",
    "    for fold_id, (idx_train, idx_val) in enumerate(folds, start=1):\n",
    "        # subset train/val QSAR\n",
    "        y_tr = y_all[idx_train]\n",
    "        y_val = y_all[idx_val]\n",
    "\n",
    "        # QSAR: consensus di fold ini (pakai model final; ini approx CV model di manuskrip)\n",
    "        probs_qsar_val_desc = {}\n",
    "        for desc in descriptors:\n",
    "            X_tr_desc = X_qsar_by_desc[desc][idx_train]\n",
    "            X_val_desc = X_qsar_by_desc[desc][idx_val]\n",
    "            model = qsar_models[desc]\n",
    "            # retrain di fold-train\n",
    "            model.fit(X_tr_desc, y_tr)\n",
    "            probs_qsar_val_desc[desc] = model.predict_proba(X_val_desc)[:, 1]\n",
    "\n",
    "        # bobot QSAR descriptor-level = Sm (per descriptor), normalisasi\n",
    "        Sm_vals_fold = np.array([Sm_qsar[d] for d in descriptors], dtype=float)\n",
    "        w_desc = Sm_vals_fold / Sm_vals_fold.sum()\n",
    "        P_qsar_val = np.average(\n",
    "            np.vstack([probs_qsar_val_desc[d] for d in descriptors]),\n",
    "            axis=0,\n",
    "            weights=w_desc\n",
    "        )\n",
    "\n",
    "        # RA: di manuskrip RA dilatih di training set global; di sini pakai train RA tetap,\n",
    "        # tapi prediksi hanya untuk subset val QSAR (pakai SMILES yang sama urutan train_df_qsar)\n",
    "        # kita butuh RA prob untuk molekul val: gunakan fingerprint di qsar train_df\n",
    "        val_df = train_df_qsar.iloc[idx_val].reset_index(drop=True)\n",
    "\n",
    "        test_fps_val = prepare_fingerprints(val_df, ra_fps)\n",
    "\n",
    "        ra_probs_fp_val = {}\n",
    "        for col in ra_fps:\n",
    "            ra_probs_fp_val[col] = predict_ra_weighted(\n",
    "                test_fps_val[col], train_fps_ra[col], y_train_ra, k=5, tanimoto_cutoff=0.0\n",
    "            )\n",
    "\n",
    "        Sf_vals = np.array([Sf_ra[col] for col in ra_fps], dtype=float)\n",
    "        w_fp = Sf_vals / Sf_vals.sum()\n",
    "        P_ra_val = np.average(\n",
    "            np.vstack([ra_probs_fp_val[col] for col in ra_fps]),\n",
    "            axis=0,\n",
    "            weights=w_fp\n",
    "        )\n",
    "\n",
    "        # mfCoQ-RASAR prob untuk fold ini & w\n",
    "        P_mf_val = w * P_qsar_val + (1 - w) * P_ra_val\n",
    "        y_pred_val = (P_mf_val >= 0.5).astype(int)\n",
    "\n",
    "        # metrik fold\n",
    "        try:\n",
    "            auc = roc_auc_score(y_val, P_mf_val)\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "        bacc, _, _ = compute_bacc_from_preds(y_val, y_pred_val)\n",
    "\n",
    "        if not np.isnan(auc):\n",
    "            S_fw = 0.5 * (auc + bacc)\n",
    "        else:\n",
    "            S_fw = bacc  # fallback jika AUC NaN\n",
    "\n",
    "        fold_scores.append(S_fw)\n",
    "\n",
    "    S_w = float(np.mean(fold_scores))\n",
    "    S_w_list.append({\"w\": w, \"S_w\": S_w})\n",
    "    print(f\"w={w:.2f} -> S_w={S_w:.4f}\")\n",
    "\n",
    "S_w_df = pd.DataFrame(S_w_list).sort_values(by=\"S_w\", ascending=False)\n",
    "best_row = S_w_df.iloc[0]\n",
    "best_w = float(best_row[\"w\"])\n",
    "best_Sw = float(best_row[\"S_w\"])\n",
    "\n",
    "print(\"\\n=== Optimal mfCoQ-RASAR weight (train CV) ===\")\n",
    "print(f\"w* (QSAR weight) = {best_w:.2f}, S_w* = {best_Sw:.4f}\")\n",
    "\n",
    "# simpan ke CSV\n",
    "w_out_path = os.path.join(output_dir, \"mfCoQ-RASAR_best_w_from_10fold_scaffoldCV.csv\")\n",
    "S_w_df.to_csv(w_out_path, index=False)\n",
    "print(\"Grid S_w(w) dan w* disimpan ke:\", w_out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936485ea-b475-4ae7-8e40-662cdd3cf391",
   "metadata": {},
   "source": [
    "# Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d264b8-5375-4d1b-a634-a22831448738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit.DataStructs.cDataStructs import ExplicitBitVect\n",
    "from rdkit import DataStructs\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, roc_auc_score\n",
    ")\n",
    "import joblib\n",
    "import ast\n",
    "import os\n",
    "\n",
    "# ===============================================================\n",
    "# 0. Paths & config\n",
    "# ===============================================================\n",
    "test_path = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Acute Dermal Toxicity\\Test_set_Dermal_with_fingerprints_RDKit_CDK.xlsx\"\n",
    "qsar_model_dir = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Acute Dermal Toxicity\\QSAR Model\"\n",
    "ra_train_file = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\Old Endpoints\\Acute Dermal Toxicity (manual split)\\Read Across\\Train_set_Dermal_balanced_with_fingerprints.xlsx\"\n",
    "sf_ra_path = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Acute Dermal Toxicity\\Read Across\\Sf_per_fingerprint.csv\"\n",
    "best_w_path = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Acute Dermal Toxicity\\Evaluation\\mfCoQ-RASAR_best_w_from_10fold_scaffoldCV.csv\"\n",
    "output_path = r\"C:\\Fauzan\\Manuscripts QSAR-RA 2\\q-RASAR\\Acute Dermal Toxicity\\Evaluation\\mfCoQ-RASAR_Test_Performance.xlsx\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# ===============================================================\n",
    "# 1. Load w* dari training CV\n",
    "# ===============================================================\n",
    "w_df = pd.read_csv(best_w_path)\n",
    "w_star = float(w_df.sort_values(\"S_w\", ascending=False).iloc[0][\"w\"])\n",
    "print(f\"Using optimal w* (QSAR weight) from train CV: {w_star:.2f}\")\n",
    "\n",
    "# ===============================================================\n",
    "# 2. Load TEST set\n",
    "# ===============================================================\n",
    "test_df = pd.read_excel(test_path)\n",
    "y_true = test_df[\"Outcome\"].astype(int).values\n",
    "\n",
    "def convert_to_array(series):\n",
    "    return np.array(series.apply(ast.literal_eval).tolist())\n",
    "\n",
    "X_test_qsar = {\n",
    "    \"Morgan\":   convert_to_array(test_df[\"Morgan_Descriptors\"]),\n",
    "    \"MACCS\":    convert_to_array(test_df[\"MACCS_Descriptors\"]),\n",
    "    \"APF\":      convert_to_array(test_df[\"APF_Descriptors\"]),\n",
    "    \"Physchem\": test_df.drop(columns=[\n",
    "        \"SMILES\", \"Morgan_Descriptors\", \"MACCS_Descriptors\",\n",
    "        \"APF_Descriptors\", \"RDK_Descriptors\", \"Outcome\"\n",
    "    ]).values\n",
    "}\n",
    "descriptors = [\"Morgan\", \"MACCS\", \"APF\", \"Physchem\"]\n",
    "\n",
    "# ===============================================================\n",
    "# 3. Load QSAR models & Sm (best per descriptor)\n",
    "# ===============================================================\n",
    "qsar_models_info = {\n",
    "    \"Morgan\":   os.path.join(qsar_model_dir, \"Dermal_xgb_morgan.pkl\"),\n",
    "    \"MACCS\":    os.path.join(qsar_model_dir, \"Dermal_SVM_MACCS.pkl\"),\n",
    "    \"APF\":      os.path.join(qsar_model_dir, \"Dermal_rf_apf.pkl\"),\n",
    "    \"Physchem\": os.path.join(qsar_model_dir, \"Dermal_xgb_rdkitcdk.pkl\"),\n",
    "}\n",
    "qsar_models = {d: joblib.load(p) for d, p in qsar_models_info.items()}\n",
    "\n",
    "Sm_files = {\n",
    "    \"Morgan\":   os.path.join(qsar_model_dir, \"Sm_Morgan_XGB.csv\"),\n",
    "    \"MACCS\":    os.path.join(qsar_model_dir, \"Sm_MACCS_SVM.csv\"),\n",
    "    \"APF\":      os.path.join(qsar_model_dir, \"Sm_APF_RF.csv\"),\n",
    "    \"Physchem\": os.path.join(qsar_model_dir, \"Sm_Physchem_XGB.csv\"),\n",
    "}\n",
    "Sm_qsar = {}\n",
    "for desc, path in Sm_files.items():\n",
    "    sm_df = pd.read_csv(path)\n",
    "    Sm_qsar[desc] = float(sm_df[\"Sm\"].values[0])\n",
    "\n",
    "Sm_vals = np.array([Sm_qsar[d] for d in descriptors], dtype=float)\n",
    "w_qsar_desc = Sm_vals / Sm_vals.sum()\n",
    "\n",
    "# QSAR probability per descriptor\n",
    "probs_qsar_desc = {}\n",
    "for desc in descriptors:\n",
    "    model = qsar_models[desc]\n",
    "    X = X_test_qsar[desc]\n",
    "    probs_qsar_desc[desc] = model.predict_proba(X)[:, 1]\n",
    "\n",
    "qsar_consensus_probs = np.average(\n",
    "    np.vstack([probs_qsar_desc[d] for d in descriptors]),\n",
    "    axis=0,\n",
    "    weights=w_qsar_desc\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 4. Read-Across consensus on TEST\n",
    "# ===============================================================\n",
    "ra_train_df = pd.read_excel(ra_train_file)\n",
    "y_train_ra = ra_train_df[\"Outcome\"].astype(int).values\n",
    "ra_fps = [\"Morgan_Descriptors\", \"MACCS_Descriptors\", \"RDK_Descriptors\"]\n",
    "\n",
    "def to_bitvect(fp_list):\n",
    "    if isinstance(fp_list, str):\n",
    "        fp_list = ast.literal_eval(fp_list)\n",
    "    n_bits = len(fp_list)\n",
    "    bv = ExplicitBitVect(n_bits)\n",
    "    for i, bit in enumerate(fp_list):\n",
    "        if int(bit):\n",
    "            bv.SetBit(i)\n",
    "    return bv\n",
    "\n",
    "def prepare_fps(df, fp_cols):\n",
    "    res = {}\n",
    "    for col in fp_cols:\n",
    "        res[col] = [to_bitvect(x) for x in df[col]]\n",
    "    return res\n",
    "\n",
    "train_fps_ra = prepare_fps(ra_train_df, ra_fps)\n",
    "test_fps_ra  = prepare_fps(test_df, ra_fps)\n",
    "\n",
    "def tanimoto_sim(target_fp, ref_fps):\n",
    "    return np.array([DataStructs.TanimotoSimilarity(target_fp, r) for r in ref_fps])\n",
    "\n",
    "def predict_ra_weighted(test_fps_list, train_fps_list, y_train, k=5, tanimoto_cutoff=0.0):\n",
    "    y_train = np.asarray(y_train, dtype=float)\n",
    "    probs = []\n",
    "    for t_fp in test_fps_list:\n",
    "        sims = tanimoto_sim(t_fp, train_fps_list)\n",
    "        if tanimoto_cutoff > 0.0:\n",
    "            valid_idx = np.where(sims >= tanimoto_cutoff)[0]\n",
    "        else:\n",
    "            valid_idx = np.arange(len(sims))\n",
    "        if len(valid_idx) == 0:\n",
    "            probs.append(y_train.mean())\n",
    "            continue\n",
    "        sims_valid = sims[valid_idx]\n",
    "        if len(sims_valid) > k:\n",
    "            nn_local = np.argsort(sims_valid)[-k:]\n",
    "            nn_idx = valid_idx[nn_local]\n",
    "        else:\n",
    "            nn_idx = valid_idx\n",
    "        nn_sims = sims[nn_idx]\n",
    "        nn_y = y_train[nn_idx]\n",
    "        w = nn_sims.copy()\n",
    "        if w.sum() == 0:\n",
    "            probs.append(nn_y.mean())\n",
    "        else:\n",
    "            probs.append(np.sum(w * nn_y) / np.sum(w))\n",
    "    return np.array(probs)\n",
    "\n",
    "# RA prob per fingerprint\n",
    "ra_probs_fps = {}\n",
    "for col in ra_fps:\n",
    "    ra_probs_fps[col] = predict_ra_weighted(\n",
    "        test_fps_ra[col], train_fps_ra[col], y_train_ra, k=5, tanimoto_cutoff=0.0\n",
    "    )\n",
    "\n",
    "# Load Sf untuk RA weighting\n",
    "sf_df = pd.read_csv(sf_ra_path)\n",
    "Sf_ra = {}\n",
    "for col in ra_fps:\n",
    "    row = sf_df.loc[sf_df[\"Fingerprint\"] == col]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"Sf untuk {col} tidak ditemukan di {sf_ra_path}\")\n",
    "    Sf_ra[col] = float(row[\"Sf\"].values[0])\n",
    "\n",
    "Sf_vals = np.array([Sf_ra[col] for col in ra_fps], dtype=float)\n",
    "w_ra_fp = Sf_vals / Sf_vals.sum()\n",
    "\n",
    "ra_consensus_probs = np.average(\n",
    "    np.vstack([ra_probs_fps[col] for col in ra_fps]),\n",
    "    axis=0,\n",
    "    weights=w_ra_fp\n",
    ")\n",
    "\n",
    "# ===============================================================\n",
    "# 5. mfCoQ-RASAR with fixed w*\n",
    "# ===============================================================\n",
    "mfcoq_probs = w_star * qsar_consensus_probs + (1 - w_star) * ra_consensus_probs\n",
    "preds = (mfcoq_probs >= 0.5).astype(int)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()\n",
    "acc = accuracy_score(y_true, preds)\n",
    "bacc = 0.5 * (\n",
    "    (tp / (tp + fn) if (tp + fn) > 0 else 0.0) +\n",
    "    (tn / (tn + fp) if (tn + fp) > 0 else 0.0)\n",
    ")\n",
    "sen = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "spe = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "auc = roc_auc_score(y_true, mfcoq_probs)\n",
    "\n",
    "# ===============================================================\n",
    "# 6. Bootstrap CI 95% (sesuai manuskrip)\n",
    "# ===============================================================\n",
    "def bootstrap_ci(probs, y_true, n_bootstrap=1000, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    probs = np.asarray(probs)\n",
    "    y_true = np.asarray(y_true)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    n = len(y_true)\n",
    "\n",
    "    accs, baccs, sens, spes, aucs = [], [], [], [], []\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.choice(n, size=n, replace=True)\n",
    "        y_b = y_true[idx]\n",
    "        p_b = probs[idx]\n",
    "        pred_b = preds[idx]\n",
    "        tn_b, fp_b, fn_b, tp_b = confusion_matrix(y_b, pred_b).ravel()\n",
    "        accs.append(accuracy_score(y_b, pred_b))\n",
    "        se = tp_b / (tp_b + fn_b) if (tp_b + fn_b) > 0 else 0.0\n",
    "        sp = tn_b / (tn_b + fp_b) if (tn_b + fp_b) > 0 else 0.0\n",
    "        sens.append(se)\n",
    "        spes.append(sp)\n",
    "        baccs.append(0.5 * (se + sp))\n",
    "        try:\n",
    "            aucs.append(roc_auc_score(y_b, p_b))\n",
    "        except ValueError:\n",
    "            aucs.append(np.nan)\n",
    "\n",
    "    def summarize(vals):\n",
    "        vals = np.array(vals, dtype=float)\n",
    "        med = np.nanmedian(vals)\n",
    "        lo = np.nanpercentile(vals, 2.5)\n",
    "        hi = np.nanpercentile(vals, 97.5)\n",
    "        half = (hi - lo) / 2.0\n",
    "        return f\"{med:.2f} ± {half:.2f}\"\n",
    "\n",
    "    return {\n",
    "        \"ACC_CI\": summarize(accs),\n",
    "        \"BACC_CI\": summarize(baccs),\n",
    "        \"SEN_CI\": summarize(sens),\n",
    "        \"SPE_CI\": summarize(spes),\n",
    "        \"AUC_CI\": summarize(aucs),\n",
    "    }\n",
    "\n",
    "ci = bootstrap_ci(mfcoq_probs, y_true)\n",
    "\n",
    "# ===============================================================\n",
    "# 7. Save results\n",
    "# ===============================================================\n",
    "results = {\n",
    "    \"w_star\": [w_star],\n",
    "    \"AUC\": [auc],\n",
    "    \"BACC\": [bacc],\n",
    "    \"ACC\": [acc],\n",
    "    \"SEN\": [sen],\n",
    "    \"SPE\": [spe],\n",
    "    \"TN\": [tn],\n",
    "    \"FP\": [fp],\n",
    "    \"FN\": [fn],\n",
    "    \"TP\": [tp],\n",
    "    \"AUC_CI\": [ci[\"AUC_CI\"]],\n",
    "    \"BACC_CI\": [ci[\"BACC_CI\"]],\n",
    "    \"ACC_CI\": [ci[\"ACC_CI\"]],\n",
    "    \"SEN_CI\": [ci[\"SEN_CI\"]],\n",
    "    \"SPE_CI\": [ci[\"SPE_CI\"]],\n",
    "}\n",
    "\n",
    "df_out = pd.DataFrame(results)\n",
    "df_out.to_excel(output_path, index=False)\n",
    "\n",
    "print(\"mfCoQ-RASAR test performance saved to:\", output_path)\n",
    "print(df_out.T)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my-work-env]",
   "language": "python",
   "name": "conda-env-my-work-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
