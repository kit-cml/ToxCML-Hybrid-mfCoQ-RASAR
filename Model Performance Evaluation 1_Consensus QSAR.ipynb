{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2367f-98dd-4439-9dda-9c43b10f4b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import warnings\n",
    "import pickle\n",
    "import bz2\n",
    "from glob import glob\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import MACCSkeys, Descriptors, PandasTools, Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.DataStructs import ExplicitBitVect\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, accuracy_score, f1_score,\n",
    "    roc_auc_score, cohen_kappa_score\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, RepeatedStratifiedKFold,\n",
    "    ShuffleSplit, StratifiedShuffleSplit\n",
    ")\n",
    "\n",
    "from standardiser import break_bonds, neutralise, rules, unsalt\n",
    "from standardiser.utils import StandardiseException, sanity_check\n",
    "\n",
    "# Optional: untuk autoreload jika di Jupyter\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn = lambda *args, **kwargs: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e055c0-d9d1-4024-8201-8e82be0f8b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fungsi untuk ubah string ke list of int\n",
    "def string_to_list(bit_string):\n",
    "    if isinstance(bit_string, str):\n",
    "        return list(map(int, bit_string.strip('[]').split(', ')))\n",
    "    return bit_string\n",
    "\n",
    "# Load test set dari Excel\n",
    "test_file = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Test_set_Dermal_balanced_with_fingerprints_sorted_with_RDKit_and_CDK_features.xlsx\"\n",
    "test_df = pd.read_excel(test_file)\n",
    "\n",
    "# Konversi kolom deskriptor jika masih berupa string\n",
    "for col in ['Morgan_Descriptors', 'MACCS_Descriptors', 'APF_Descriptors']:\n",
    "    if col in test_df.columns:\n",
    "        if isinstance(test_df[col].iloc[0], str):\n",
    "            test_df[col] = test_df[col].apply(string_to_list)\n",
    "\n",
    "# Tampilkan hasil\n",
    "print(\"Test DataFrame:\")\n",
    "print(test_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdccbfbc-f934-4369-8cf0-c40a1d25b2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat nama-nama kolom yang ada di DataFrame\n",
    "print(\"Daftar kolom dalam test_df:\")\n",
    "print(test_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18098b-ae30-4283-a1e2-3c377d39b9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek jumlah NaN sebelum dihapus\n",
    "nan_before = test_df.isnull().sum().sum()\n",
    "\n",
    "# Hapus baris yang mengandung NaN\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "# Tampilkan informasi jumlah NaN\n",
    "if nan_before > 0:\n",
    "    print(f\"Total nilai NaN yang dihapus dari test_df: {nan_before}\")\n",
    "else:\n",
    "    print(\"Tidak ada nilai NaN yang ditemukan dalam test_df.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3426d6-5739-4b66-8748-3c7770111382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Buat salinan kolom Outcome\n",
    "S = test_df['Outcome'].copy()\n",
    "\n",
    "# Plot distribusi kelas\n",
    "fig, ax = plt.subplots()\n",
    "ax = S.hist(bins=np.arange(-0.5, 5), edgecolor='black')\n",
    "ax.set_xticks(range(0, 5))\n",
    "ax.set_xlabel(\"Outcome Class\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Distribusi Outcome (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "# Encoding label\n",
    "le = LabelEncoder()\n",
    "outcomes = np.unique(test_df['Outcome'])\n",
    "le.fit(outcomes)\n",
    "y = le.transform(test_df['Outcome'])\n",
    "\n",
    "# Info distribusi\n",
    "print(\"Classes                          :\", outcomes)\n",
    "print(\"Number of cpds in each class     :\", np.bincount(y))\n",
    "print(\"Total number of cpds             :\", len(y))\n",
    "\n",
    "# Ganti label Outcome menjadi angka (mapping)\n",
    "S = test_df['Outcome']\n",
    "info = {}\n",
    "for i, cls in enumerate(S.unique()):\n",
    "    info[cls] = i\n",
    "    S = S.replace(cls, i)\n",
    "\n",
    "# Optional: simpan mapping info kalau mau pakai nanti\n",
    "print(\"Label mapping (kelas ‚Üí angka):\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6537dc8b-2f34-4700-839b-c604c9a2963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Ambil label Outcome dari test_df\n",
    "S = test_df['Outcome'].copy()\n",
    "\n",
    "# Encode label ke angka\n",
    "info = {}\n",
    "for i, cls in enumerate(S.unique()):\n",
    "    info[cls] = i\n",
    "    S = S.replace(cls, i)\n",
    "\n",
    "# Konversi label ke numpy array bertipe int32\n",
    "y_test = np.int32(S)\n",
    "\n",
    "# Konversi MACCS, Morgan, dan APF Descriptors ke array numpy\n",
    "def convert_to_array(desc_list):\n",
    "    return np.array([eval(desc) if isinstance(desc, str) else desc for desc in desc_list])\n",
    "\n",
    "x_test_macckeys = convert_to_array(test_df['MACCS_Descriptors'])\n",
    "x_test_morgan = convert_to_array(test_df['Morgan_Descriptors'])\n",
    "x_test_apf = convert_to_array(test_df['APF_Descriptors'])  # <-- tambahan APF\n",
    "\n",
    "# Cek isi\n",
    "print(\"Label classes (encoded)       :\", info)\n",
    "print(\"Jumlah senyawa per kelas      :\", np.bincount(y_test))\n",
    "print(\"Total jumlah senyawa (test)   :\", len(y_test))\n",
    "print(\"x_test_macckeys shape         :\", x_test_macckeys.shape)\n",
    "print(\"x_test_morgan shape           :\", x_test_morgan.shape)\n",
    "print(\"x_test_apf shape              :\", x_test_apf.shape)  # <-- cek APF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010e642-68ad-43b9-a871-acda79a9214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rdkitcdk = test_df.drop(columns=['SMILES',\n",
    "    'Outcome',\n",
    "    'Morgan_Descriptors',\n",
    "    'MACCS_Descriptors',\n",
    "    'APF_Descriptors'])\n",
    "x_rdkitcdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bd9c6-4fe5-497f-9619-a3e6d9b28038",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_rdkitcdk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26511f4a-8281-4210-99ae-563b479e7893",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rdkitcdk  = x_rdkitcdk.apply(lambda row: row.values, axis=1).tolist()\n",
    "\n",
    "# Add the new column 'rdkit_cdk' to test_df\n",
    "test_df['rdkit_cdk'] = x_rdkitcdk \n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e516f7f-4687-4e01-99a7-3d96af57e521",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.int32(S)\n",
    "x_test_morgan = np.array(list(test_df['Morgan_Descriptors']))\n",
    "x_test_macckeys = np.array(list(test_df['MACCS_Descriptors']))\n",
    "x_test_rdkit_cdk = np.array(list(test_df['rdkit_cdk']))\n",
    "x_test_apf = np.array(list(test_df['APF_Descriptors']))  # <-- tambahan APF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2662b5c5-c093-4a3e-9780-4d7d9f587b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= np.int32((S))\n",
    "x_test_rdkit_cdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76820ca9-55b5-4ad0-917a-870bb5d0a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, classification_report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f917f-31a5-4d17-a6e9-9fc155aa49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = test_df['Outcome'].astype(int)  # Ensure it's of integer type, suitable for metrics calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8458555d-55ba-4ed4-9389-2e0acc367524",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714fece-e727-4ce4-a255-b3bf20caafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_str_to_float(lst):\n",
    "    return [float(x) for x in lst if x != '' and x is not None]\n",
    "\n",
    "test_df['rdkit_cdk'] = test_df['rdkit_cdk'].apply(convert_list_str_to_float)\n",
    "\n",
    "X_rdkitcdk = np.array(test_df['rdkit_cdk'].tolist(), dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93040a13-70e0-4441-a35a-4c77cc186d83",
   "metadata": {},
   "source": [
    "# EVALUASI DESCRIPTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54fafb-02a2-417a-8685-b4676445f22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Test_set_Dermal_balanced_with_fingerprints_sorted_with_RDKit_and_CDK_features.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b918d231-171c-4a51-afa9-716c26c2c1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "import joblib\n",
    "import itertools\n",
    "import os\n",
    "import ast  # untuk konversi string ke list\n",
    "\n",
    "# ==========================\n",
    "# Load test set\n",
    "# ==========================\n",
    "test_file = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Test_set_Dermal_balanced_with_fingerprints_sorted_with_RDKit_and_CDK_features.xlsx\"\n",
    "test_df = pd.read_excel(test_file)\n",
    "\n",
    "drop_cols = ['SMILES', 'Morgan_Descriptors', 'MACCS_Descriptors', 'APF_Descriptors', 'Outcome']\n",
    "x_rdkitcdk_test = test_df.drop(columns=drop_cols)\n",
    "y_true = test_df['Outcome'].astype(int).values\n",
    "\n",
    "# ==========================\n",
    "# Helper: konversi string fingerprint ke array\n",
    "# ==========================\n",
    "def convert_to_array(series):\n",
    "    return np.array(series.apply(ast.literal_eval).tolist())\n",
    "\n",
    "X_test_by_desc = {\n",
    "    \"Morgan\": convert_to_array(test_df[\"Morgan_Descriptors\"]),\n",
    "    \"MACCS\":  convert_to_array(test_df[\"MACCS_Descriptors\"]),\n",
    "    \"APF\":    convert_to_array(test_df[\"APF_Descriptors\"]),\n",
    "    \"Physchem\": x_rdkitcdk_test.values  # RDKit+CDK\n",
    "}\n",
    "\n",
    "descriptors = [\"Morgan\", \"MACCS\", \"APF\", \"Physchem\"]\n",
    "algorithms = [\"RF\", \"XGB\", \"SVM\"]\n",
    "\n",
    "# ==========================\n",
    "# Load base models\n",
    "# ==========================\n",
    "model_base_dir = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model\"\n",
    "\n",
    "model_paths = {\n",
    "    (\"Morgan\",  \"RF\"):  os.path.join(model_base_dir, \"Dermal_rf_morgan.pkl\"),\n",
    "    (\"MACCS\",   \"RF\"):  os.path.join(model_base_dir, \"Dermal_rf_macckeys.pkl\"),\n",
    "    (\"APF\",     \"RF\"):  os.path.join(model_base_dir, \"Dermal_rf_apf.pkl\"),\n",
    "    (\"Physchem\",\"RF\"):  os.path.join(model_base_dir, \"Dermal_rf_rdkitcdk.pkl\"),\n",
    "\n",
    "    (\"Morgan\",  \"XGB\"): os.path.join(model_base_dir, \"Dermal_xgb_morgan.pkl\"),\n",
    "    (\"MACCS\",   \"XGB\"): os.path.join(model_base_dir, \"Dermal_xgb_maccs.pkl\"),\n",
    "    (\"APF\",     \"XGB\"): os.path.join(model_base_dir, \"Dermal_xgb_apf.pkl\"),\n",
    "    (\"Physchem\",\"XGB\"): os.path.join(model_base_dir, \"Dermal_xgb_rdkitcdk.pkl\"),\n",
    "\n",
    "    (\"Morgan\",  \"SVM\"): os.path.join(model_base_dir, \"Dermal_SVM_Morgan.pkl\"),\n",
    "    (\"MACCS\",   \"SVM\"): os.path.join(model_base_dir, \"Dermal_SVM_MACCS.pkl\"),\n",
    "    (\"APF\",     \"SVM\"): os.path.join(model_base_dir, \"Dermal_SVM_APF.pkl\"),\n",
    "    (\"Physchem\",\"SVM\"): os.path.join(model_base_dir, \"Dermal_svm_rdkitcdk.pkl\"),\n",
    "}\n",
    "\n",
    "models = {}\n",
    "for (desc, algo), path in model_paths.items():\n",
    "    models[(desc, algo)] = joblib.load(path)\n",
    "\n",
    "print(\"Semua base QSAR models berhasil dimuat.\\n\")\n",
    "\n",
    "# ==========================\n",
    "# Load Sm (10-fold scaffold-CV) untuk weighting\n",
    "# ==========================\n",
    "\n",
    "sm_files = {\n",
    "    (\"Morgan\",  \"RF\"):  os.path.join(model_base_dir, \"Sm_Morgan_RF.csv\"),\n",
    "    (\"MACCS\",   \"RF\"):  os.path.join(model_base_dir, \"Sm_MACCS_RF.csv\"),\n",
    "    (\"APF\",     \"RF\"):  os.path.join(model_base_dir, \"Sm_APF_RF.csv\"),\n",
    "    (\"Physchem\",\"RF\"):  os.path.join(model_base_dir, \"Sm_Physchem_RF.csv\"),\n",
    "\n",
    "    (\"Morgan\",  \"XGB\"): os.path.join(model_base_dir, \"Sm_Morgan_XGB.csv\"),\n",
    "    (\"MACCS\",   \"XGB\"): os.path.join(model_base_dir, \"Sm_MACCS_XGB.csv\"),\n",
    "    (\"APF\",     \"XGB\"): os.path.join(model_base_dir, \"Sm_APF_XGB.csv\"),\n",
    "    (\"Physchem\",\"XGB\"): os.path.join(model_base_dir, \"Sm_Physchem_XGB.csv\"),\n",
    "\n",
    "    (\"Morgan\",  \"SVM\"): os.path.join(model_base_dir, \"Sm_Morgan_SVM.csv\"),\n",
    "    (\"MACCS\",   \"SVM\"): os.path.join(model_base_dir, \"Sm_MACCS_SVM.csv\"),\n",
    "    (\"APF\",     \"SVM\"): os.path.join(model_base_dir, \"Sm_APF_SVM.csv\"),\n",
    "    (\"Physchem\",\"SVM\"): os.path.join(model_base_dir, \"Sm_Physchem_SVM.csv\"),\n",
    "}\n",
    "\n",
    "Sm = {}\n",
    "for key, path in sm_files.items():\n",
    "    df_sm = pd.read_csv(path)\n",
    "    Sm[key] = float(df_sm[\"Sm\"].values[0])\n",
    "\n",
    "# ==========================\n",
    "# Bootstrap metrics (median ¬± half-range)\n",
    "# ==========================\n",
    "def bootstrap_metrics_simple(probs, y_true, n_bootstrap=1000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y_true = np.asarray(y_true)\n",
    "    probs = np.asarray(probs)\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "    n = len(y_true)\n",
    "\n",
    "    accs, sens_list, spec_list, aucs = [], [], [], []\n",
    "    tn_list, fp_list, fn_list, tp_list = [], [], [], []\n",
    "\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.choice(n, size=n, replace=True)\n",
    "        y_b = y_true[idx]\n",
    "        p_b = probs[idx]\n",
    "        pred_b = preds[idx]\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_b, pred_b).ravel()\n",
    "        tn_list.append(tn)\n",
    "        fp_list.append(fp)\n",
    "        fn_list.append(fn)\n",
    "        tp_list.append(tp)\n",
    "\n",
    "        accs.append(accuracy_score(y_b, pred_b))\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "        sens_list.append(sens)\n",
    "        spec_list.append(spec)\n",
    "        try:\n",
    "            aucs.append(roc_auc_score(y_b, p_b))\n",
    "        except ValueError:\n",
    "            aucs.append(np.nan)\n",
    "\n",
    "    def summarize(values):\n",
    "        values = np.array(values, dtype=float)\n",
    "        med = np.nanmedian(values)\n",
    "        lo = np.nanmin(values)\n",
    "        hi = np.nanmax(values)\n",
    "        err = (hi - lo) / 2.0\n",
    "        return f\"{med:.3f} ¬± {err:.3f}\"\n",
    "\n",
    "    metrics = {\n",
    "        \"Accuracy\":   summarize(accs),\n",
    "        \"Sensitivity\":summarize(sens_list),\n",
    "        \"Specificity\":summarize(spec_list),\n",
    "        \"AUC\":        summarize(aucs),\n",
    "        \"TN\": int(np.mean(tn_list)),\n",
    "        \"FP\": int(np.mean(fp_list)),\n",
    "        \"FN\": int(np.mean(fn_list)),\n",
    "        \"TP\": int(np.mean(tp_list)),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# ==========================\n",
    "# 1) Performansi individual base-model\n",
    "# ==========================\n",
    "results = []\n",
    "\n",
    "for desc in descriptors:\n",
    "    X_desc = X_test_by_desc[desc]\n",
    "    for algo in algorithms:\n",
    "        model = models[(desc, algo)]\n",
    "        probs = model.predict_proba(X_desc)[:, 1]\n",
    "        m = bootstrap_metrics_simple(probs, y_true)\n",
    "        m[\"Type\"] = \"Individual\"\n",
    "        m[\"Descriptor\"] = desc\n",
    "        m[\"Algorithm\"] = algo\n",
    "        m[\"Name\"] = f\"{desc}-{algo}\"\n",
    "        results.append(m)\n",
    "\n",
    "# ==========================\n",
    "# 2) Consensus per descriptor (across algorithms) dengan bobot Sm\n",
    "# ==========================\n",
    "descriptor_consensus_probs = {}\n",
    "\n",
    "for desc in descriptors:\n",
    "    X_desc = X_test_by_desc[desc]\n",
    "\n",
    "    # kumpulkan prob per algoritme + Sm\n",
    "    probs_alg = {}\n",
    "    sm_alg = []\n",
    "    for algo in algorithms:\n",
    "        key = (desc, algo)\n",
    "        model = models[key]\n",
    "        probs_alg[algo] = model.predict_proba(X_desc)[:, 1]\n",
    "        sm_alg.append(Sm[key])\n",
    "\n",
    "    sm_alg = np.array(sm_alg, dtype=float)\n",
    "    w_alg = sm_alg / sm_alg.sum()  # normalisasi bobot\n",
    "\n",
    "    # urutkan sesuai algorithms agar konsisten\n",
    "    prob_matrix = np.vstack([probs_alg[algo] for algo in algorithms])  # shape: (3, n_samples)\n",
    "    cons_probs = np.average(prob_matrix, axis=0, weights=w_alg)\n",
    "    descriptor_consensus_probs[desc] = cons_probs\n",
    "\n",
    "    m = bootstrap_metrics_simple(cons_probs, y_true)\n",
    "    m[\"Type\"] = \"DescriptorConsensus\"\n",
    "    m[\"Descriptor\"] = desc\n",
    "    m[\"Algorithm\"] = \"RF+XGB+SVM\"\n",
    "    m[\"Name\"] = f\"{desc}_QSAR_consensus\"\n",
    "    results.append(m)\n",
    "\n",
    "# ==========================\n",
    "# 3) Full consensus QSAR across descriptors (weighted by Sm descriptor-level)\n",
    "# ==========================\n",
    "# contoh: weight descriptor-level = rata-rata Sm semua algoritme untuk descriptor tsb\n",
    "Sm_desc = {}\n",
    "for desc in descriptors:\n",
    "    sm_list = [Sm[(desc, algo)] for algo in algorithms]\n",
    "    Sm_desc[desc] = float(np.mean(sm_list))\n",
    "\n",
    "Sm_desc_arr = np.array([Sm_desc[d] for d in descriptors], dtype=float)\n",
    "w_desc = Sm_desc_arr / Sm_desc_arr.sum()\n",
    "\n",
    "prob_matrix_desc = np.vstack([descriptor_consensus_probs[d] for d in descriptors])  # (4, n_samples)\n",
    "full_qsar_probs = np.average(prob_matrix_desc, axis=0, weights=w_desc)\n",
    "\n",
    "m_full = bootstrap_metrics_simple(full_qsar_probs, y_true)\n",
    "m_full[\"Type\"] = \"FullQSARConsensus\"\n",
    "m_full[\"Descriptor\"] = \"All\"\n",
    "m_full[\"Algorithm\"] = \"QSAR_consensus\"\n",
    "m_full[\"Name\"] = \"Full_QSAR_mfCoQ_component\"\n",
    "results.append(m_full)\n",
    "\n",
    "# ==========================\n",
    "# Simpan hasil ke Excel\n",
    "# ==========================\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# ekstrak nilai AUC median untuk sorting\n",
    "results_df[\"AUC_val\"] = results_df[\"AUC\"].str.extract(r\"([0-9.]+)\").astype(float)\n",
    "results_df = results_df.sort_values(by=\"AUC_val\", ascending=False).drop(columns=[\"AUC_val\"])\n",
    "\n",
    "cols_order = [\"Type\", \"Name\", \"Descriptor\", \"Algorithm\", \"AUC\", \"Accuracy\",\n",
    "              \"Sensitivity\", \"Specificity\", \"TN\", \"FP\", \"FN\", \"TP\"]\n",
    "results_df = results_df[cols_order]\n",
    "\n",
    "save_path = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Evaluation\\Evaluation_QSAR_with_Sm_weighting.xlsx\"\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "results_df.to_excel(save_path, index=False)\n",
    "\n",
    "print(f\"Hasil evaluasi QSAR (individual + descriptor consensus + full QSAR) disimpan ke:\\n{save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca60c43-d6ca-44b3-a96e-f4765e3ddbac",
   "metadata": {},
   "source": [
    "# Without CI 95%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2218b1-0928-4476-9da3-e757655cbb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏\n",
    "# ================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "import joblib\n",
    "import os\n",
    "import ast  # Î¨∏ÏûêÏó¥ÏùÑ Î¶¨Ïä§Ìä∏/Î∞∞Ïó¥Î°ú Î≥ÄÌôòÌï† Îïå ÏÇ¨Ïö©\n",
    "\n",
    "# ================================\n",
    "# ÌÖåÏä§Ìä∏ÏÖã Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "# ================================\n",
    "test_files = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Test_set_Dermal_balanced_with_fingerprints_sorted_with_RDKit_and_CDK_features.xlsx\"\n",
    "if isinstance(test_files, str):\n",
    "    test_files = [test_files]\n",
    "\n",
    "# ================================\n",
    "# Î™®Îç∏ Î∂àÎü¨Ïò§Í∏∞ (base models)\n",
    "# ================================\n",
    "model_base_dir = r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model\"\n",
    "\n",
    "descriptors = [\"Morgan\", \"MACCS\", \"APF\", \"Physchem\"]\n",
    "algorithms = [\"RF\", \"XGB\", \"SVM\"]\n",
    "\n",
    "model_paths = {\n",
    "    (\"Morgan\",  \"SVM\"): os.path.join(model_base_dir, \"Dermal_SVM_Morgan.pkl\"),\n",
    "    (\"MACCS\",   \"SVM\"): os.path.join(model_base_dir, \"Dermal_SVM_MACCS.pkl\"),\n",
    "    (\"APF\",     \"SVM\"): os.path.join(model_base_dir, \"Dermal_SVM_APF.pkl\"),\n",
    "    (\"Physchem\",\"SVM\"): os.path.join(model_base_dir, \"Dermal_svm_rdkitcdk.pkl\"),\n",
    "\n",
    "    (\"Morgan\",  \"RF\"):  os.path.join(model_base_dir, \"Dermal_rf_morgan.pkl\"),\n",
    "    (\"MACCS\",   \"RF\"):  os.path.join(model_base_dir, \"Dermal_rf_macckeys.pkl\"),\n",
    "    (\"APF\",     \"RF\"):  os.path.join(model_base_dir, \"Dermal_rf_apf.pkl\"),\n",
    "    (\"Physchem\",\"RF\"):  os.path.join(model_base_dir, \"Dermal_rf_rdkitcdk.pkl\"),\n",
    "\n",
    "    (\"Morgan\",  \"XGB\"): os.path.join(model_base_dir, \"Dermal_xgb_morgan.pkl\"),\n",
    "    (\"MACCS\",   \"XGB\"): os.path.join(model_base_dir, \"Dermal_xgb_maccs.pkl\"),\n",
    "    (\"APF\",     \"XGB\"): os.path.join(model_base_dir, \"Dermal_xgb_apf.pkl\"),\n",
    "    (\"Physchem\",\"XGB\"): os.path.join(model_base_dir, \"Dermal_xgb_rdkitcdk.pkl\"),\n",
    "}\n",
    "\n",
    "loaded_models = {}\n",
    "for key, path in model_paths.items():\n",
    "    loaded_models[key] = joblib.load(path)\n",
    "print(\"‚úÖ Î™®Îì† base QSAR Î™®Îç∏Ïù¥ ÏÑ±Í≥µÏ†ÅÏúºÎ°ú Î°úÎìúÎêòÏóàÏäµÎãàÎã§.\\n\")\n",
    "\n",
    "# ================================\n",
    "# Sm (10-fold scaffold-CV score) Î°úÎî©\n",
    "# ================================\n",
    "sm_files = {\n",
    "    (\"Morgan\",  \"RF\"):  os.path.join(model_base_dir, \"Sm_Morgan_RF.csv\"),\n",
    "    (\"MACCS\",   \"RF\"):  os.path.join(model_base_dir, \"Sm_MACCS_RF.csv\"),\n",
    "    (\"APF\",     \"RF\"):  os.path.join(model_base_dir, \"Sm_APF_RF.csv\"),\n",
    "    (\"Physchem\",\"RF\"):  os.path.join(model_base_dir, \"Sm_Physchem_RF.csv\"),\n",
    "\n",
    "    (\"Morgan\",  \"XGB\"): os.path.join(model_base_dir, \"Sm_Morgan_XGB.csv\"),\n",
    "    (\"MACCS\",   \"XGB\"): os.path.join(model_base_dir, \"Sm_MACCS_XGB.csv\"),\n",
    "    (\"APF\",     \"XGB\"): os.path.join(model_base_dir, \"Sm_APF_XGB.csv\"),\n",
    "    (\"Physchem\",\"XGB\"): os.path.join(model_base_dir, \"Sm_Physchem_XGB.csv\"),\n",
    "\n",
    "    (\"Morgan\",  \"SVM\"): os.path.join(model_base_dir, \"Sm_Morgan_SVM.csv\"),\n",
    "    (\"MACCS\",   \"SVM\"): os.path.join(model_base_dir, \"Sm_MACCS_SVM.csv\"),\n",
    "    (\"APF\",     \"SVM\"): os.path.join(model_base_dir, \"Sm_APF_SVM.csv\"),\n",
    "    (\"Physchem\",\"SVM\"): os.path.join(model_base_dir, \"Sm_Physchem_SVM.csv\"),\n",
    "}\n",
    "\n",
    "Sm = {}\n",
    "for key, path in sm_files.items():\n",
    "    sm_df = pd.read_csv(path)\n",
    "    Sm[key] = float(sm_df[\"Sm\"].values[0])\n",
    "\n",
    "# ================================\n",
    "# ÌèâÍ∞Ä ÏßÄÌëú Í≥ÑÏÇ∞ Ìï®Ïàò\n",
    "# ================================\n",
    "def compute_metrics(probs, y_true):\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, preds).ravel()\n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    sen = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    spe = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, probs)\n",
    "    except:\n",
    "        auc = np.nan\n",
    "    return {\n",
    "        'Accuracy': round(acc, 3),\n",
    "        'Sensitivity': round(sen, 3),\n",
    "        'Specificity': round(spe, 3),\n",
    "        'AUC': round(auc, 3),\n",
    "        'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp\n",
    "    }\n",
    "\n",
    "def convert_to_array(series):\n",
    "    return np.array(series.apply(ast.literal_eval).tolist())\n",
    "\n",
    "# ================================\n",
    "# ÌÖåÏä§Ìä∏ÏÖã Ï≤òÎ¶¨ Î£®ÌîÑ\n",
    "# ================================\n",
    "for test_file in test_files:\n",
    "    print(f\"üîç ÌååÏùº Ï≤òÎ¶¨ Ï§ë: {test_file}\")\n",
    "    test_df = pd.read_excel(test_file)\n",
    "\n",
    "    drop_cols = ['SMILES', 'Morgan_Descriptors', 'MACCS_Descriptors', 'APF_Descriptors', 'Outcome']\n",
    "    x_rdkitcdk_test = test_df.drop(columns=drop_cols)\n",
    "    y_true = test_df['Outcome'].astype(int).values\n",
    "\n",
    "    X_test_by_desc = {\n",
    "        \"Morgan\":   convert_to_array(test_df[\"Morgan_Descriptors\"]),\n",
    "        \"MACCS\":    convert_to_array(test_df[\"MACCS_Descriptors\"]),\n",
    "        \"APF\":      convert_to_array(test_df[\"APF_Descriptors\"]),\n",
    "        \"Physchem\": x_rdkitcdk_test.values\n",
    "    }\n",
    "\n",
    "    results_list = []\n",
    "\n",
    "    # 1) Individual base models\n",
    "    for desc in descriptors:\n",
    "        X_desc = X_test_by_desc[desc]\n",
    "        for algo in algorithms:\n",
    "            model = loaded_models[(desc, algo)]\n",
    "            probs = model.predict_proba(X_desc)[:, 1]\n",
    "            metrics = compute_metrics(probs, y_true)\n",
    "            metrics['Type'] = \"Individual\"\n",
    "            metrics['Combination'] = f\"{desc}-{algo}\"\n",
    "            results_list.append(metrics)\n",
    "\n",
    "    # 2) Consensus per descriptor (RF+XGB+SVM, dibobot Sm)\n",
    "    desc_cons_probs = {}\n",
    "    for desc in descriptors:\n",
    "        X_desc = X_test_by_desc[desc]\n",
    "\n",
    "        probs_alg = []\n",
    "        sm_alg = []\n",
    "        for algo in algorithms:\n",
    "            model = loaded_models[(desc, algo)]\n",
    "            probs_alg.append(model.predict_proba(X_desc)[:, 1])\n",
    "            sm_alg.append(Sm[(desc, algo)])\n",
    "\n",
    "        probs_alg = np.vstack(probs_alg)   # shape (3, n_samples)\n",
    "        sm_alg = np.array(sm_alg, dtype=float)\n",
    "        w_alg = sm_alg / sm_alg.sum()\n",
    "\n",
    "        cons_probs_desc = np.average(probs_alg, axis=0, weights=w_alg)\n",
    "        desc_cons_probs[desc] = cons_probs_desc\n",
    "\n",
    "        metrics = compute_metrics(cons_probs_desc, y_true)\n",
    "        metrics['Type'] = \"DescriptorConsensus\"\n",
    "        metrics['Combination'] = f\"{desc}_QSAR_consensus(RF+XGB+SVM)\"\n",
    "        results_list.append(metrics)\n",
    "\n",
    "    # 3) Full QSAR consensus across descriptors (dibobot Sm descriptor-level)\n",
    "    Sm_desc = {}\n",
    "    for desc in descriptors:\n",
    "        Sm_desc[desc] = float(np.mean([Sm[(desc, algo)] for algo in algorithms]))\n",
    "    w_desc = np.array([Sm_desc[d] for d in descriptors], dtype=float)\n",
    "    w_desc = w_desc / w_desc.sum()\n",
    "\n",
    "    probs_desc_mat = np.vstack([desc_cons_probs[d] for d in descriptors])  # (4, n_samples)\n",
    "    full_qsar_probs = np.average(probs_desc_mat, axis=0, weights=w_desc)\n",
    "\n",
    "    metrics = compute_metrics(full_qsar_probs, y_true)\n",
    "    metrics['Type'] = \"FullQSARConsensus\"\n",
    "    metrics['Combination'] = \"QSAR_full_mfCoQ_component\"\n",
    "    results_list.append(metrics)\n",
    "\n",
    "    # ================================\n",
    "    # Í≤∞Í≥º Ï†ÄÏû•\n",
    "    # ================================\n",
    "    metrics_df = pd.DataFrame(results_list).sort_values(by=\"AUC\", ascending=False)\n",
    "    metrics_df = metrics_df[['Type', 'Combination', 'AUC', 'Accuracy', 'Sensitivity', 'Specificity', 'TN', 'FP', 'FN', 'TP']]\n",
    "\n",
    "    set_name = os.path.splitext(os.path.basename(test_file))[0]\n",
    "    save_path = fr\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Evaluation\\NoCI95_Evaluation_{set_name}_QSAR_with_Sm_weighting.xlsx\"\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    metrics_df.to_excel(save_path, index=False)\n",
    "\n",
    "    print(f\"‚úÖ {set_name}Ïùò QSAR Í∞úÎ≥Ñ/Ïª®ÏÑºÏÑúÏä§ Í≤∞Í≥º({len(metrics_df)})Í∞Ä Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§:\\n   {save_path}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
