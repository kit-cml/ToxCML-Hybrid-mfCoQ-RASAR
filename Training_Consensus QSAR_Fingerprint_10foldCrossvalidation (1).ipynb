{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem.Descriptors import MolLogP\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,f1_score\n",
    "from rdkit.Chem.AllChem import GetMorganFingerprintAsBitVect\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.DataStructs import ExplicitBitVect\n",
    "import sys\n",
    "import multiprocessing\n",
    "from standardiser import break_bonds, neutralise, rules, unsalt\n",
    "from standardiser.utils import StandardiseException, sanity_check\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "def warn(*args, **kwargs):\n",
    "    pass \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn = warn\n",
    "from rdkit.Chem import AllChem as Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "import sys\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import csv\n",
    "from rdkit.Chem import MACCSkeys\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import _pickle as cPickle\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit    \n",
    "import bz2\n",
    "from glob import glob\n",
    "import _pickle as cPickle\n",
    "import pickle\n",
    "# Draw.DrawingOptions.atomLabelFontFace = \"DejaVu Sans\"\n",
    "# Draw.DrawingOptions.atomLabelFontSize = 18\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATASET TRAINING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "\n",
    "# Function to read Excel file into DataFrame\n",
    "def load_excel_to_df(filename):\n",
    "    df = pd.read_excel(filename)\n",
    "    return df\n",
    "\n",
    "# Convert strings back to lists of integers\n",
    "def string_to_list(bit_string):\n",
    "    if isinstance(bit_string, str):\n",
    "        return list(map(int, bit_string.strip('[]').split(', ')))\n",
    "    else:\n",
    "        return bit_string\n",
    "\n",
    "# Load Excel file\n",
    "train_df = load_excel_to_df(r\"C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Train_set_Dermal_balanced_with_fingerprints_sorted_with_RDKit_and_CDK_features.xlsx\")\n",
    "\n",
    "# Apply the string-to-list conversion for the fingerprint columns\n",
    "fingerprint_columns = ['Morgan_Descriptors', 'MACCS_Descriptors', 'APF_Descriptors']\n",
    "for col in fingerprint_columns:\n",
    "    if col in train_df.columns:\n",
    "        train_df[col] = train_df[col].apply(string_to_list)\n",
    "    else:\n",
    "        print(f\"Warning: Column {col} not found in Excel file!\")\n",
    "\n",
    "# Define combined_df as train_df\n",
    "combined_df = train_df\n",
    "\n",
    "print(\"Combined DataFrame:\")\n",
    "print(combined_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combine_df= combined_df.sort_values(['Outcome'], ascending=True)\n",
    "combined_df['RowID'] = combined_df.index\n",
    "combined_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label encoding\n",
    "le = LabelEncoder()\n",
    "outcomes = np.unique(combined_df['Outcome'])\n",
    "le.fit(outcomes)\n",
    "y = le.transform(combined_df['Outcome'])\n",
    "\n",
    "# Hasil\n",
    "classes, counts = np.unique(y, return_counts=True)\n",
    "\n",
    "print(\"Classes                          : \", classes)\n",
    "print(\"Number of cpds in each class     : \", counts)\n",
    "print(\"Total number of cpds             : \", len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pd.Series(le.transform(combined_df['Outcome']))  # Sama dengan y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = S.hist(bins=np.arange(-0.5,5))\n",
    "ax.set_xticks(range(0,5))\n",
    "#info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "y = np.int32(S)  # pastikan S sudah didefinisikan\n",
    "\n",
    "# Fingerprint arrays\n",
    "x_maccs = np.array(list(combined_df['MACCS_Descriptors']))\n",
    "x_morgan = np.array(list(combined_df['Morgan_Descriptors']))\n",
    "x_apf = np.array(list(combined_df['APF_Descriptors']))  # ditambahkan APF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORGAN_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, accuracy_score,\n",
    "    roc_auc_score, confusion_matrix, precision_score,\n",
    "    recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# =========================================================\n",
    "# Fungsi: Bemis–Murcko scaffold-based KFold (10-fold)\n",
    "# =========================================================\n",
    "def get_bemis_murcko_scaffold(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold) if scaffold is not None else None\n",
    "\n",
    "def scaffold_kfold_indices(smiles_list, n_splits=10, random_state=42):\n",
    "    scaffolds = [get_bemis_murcko_scaffold(smi) for smi in smiles_list]\n",
    "    scaffold_to_indices = {}\n",
    "    for idx, scaf in enumerate(scaffolds):\n",
    "        scaffold_to_indices.setdefault(scaf, []).append(idx)\n",
    "\n",
    "    unique_scaffolds = list(scaffold_to_indices.keys())\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(unique_scaffolds)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    folds = []\n",
    "    scaffold_array = np.array(unique_scaffolds)\n",
    "    for scaf_train_idx, scaf_val_idx in kf.split(scaffold_array):\n",
    "        train_idx = []\n",
    "        val_idx = []\n",
    "        for i in scaf_train_idx:\n",
    "            train_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        for i in scaf_val_idx:\n",
    "            val_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        folds.append((np.array(train_idx, dtype=int),\n",
    "                      np.array(val_idx, dtype=int)))\n",
    "    return folds\n",
    "\n",
    "def compute_bacc_from_preds(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    return 0.5 * (sens + spec), sens, spec\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ Data, fitur, target (x_morgan, y, SMILES)\n",
    "# =========================================================\n",
    "# Asumsi:\n",
    "#  - x_morgan: np.ndarray (n_samples, n_features)\n",
    "#  - y: np.ndarray (n_samples,)\n",
    "#  - smiles_all: list/array SMILES dari train set untuk scaffold-CV\n",
    "# Pastikan sudah didefinisikan sebelumnya, mis. dari train_df['SMILES'].\n",
    "\n",
    "# contoh:\n",
    "# smiles_all = train_df['SMILES'].astype(str).values\n",
    "\n",
    "# =========================================================\n",
    "# 2️⃣ Hyperparameter grid\n",
    "# =========================================================\n",
    "paramgrid = {\n",
    "    \"max_features\": [\n",
    "        x_morgan.shape[1],\n",
    "        x_morgan.shape[1] // 2,\n",
    "        x_morgan.shape[1] // 4,\n",
    "        x_morgan.shape[1] // 12,\n",
    "        x_morgan.shape[1] // 10,\n",
    "        x_morgan.shape[1] // 7,\n",
    "        x_morgan.shape[1] // 5,\n",
    "        x_morgan.shape[1] // 3\n",
    "    ],\n",
    "    \"n_estimators\": [10, 100, 300, 500],\n",
    "}\n",
    "\n",
    "# scoring utama untuk GridSearch (seperti sebelumnya)\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# =========================================================\n",
    "# 3️⃣ 10-fold scaffold-CV untuk dapatkan Sm\n",
    "# =========================================================\n",
    "folds = scaffold_kfold_indices(smiles_all, n_splits=10, random_state=42)\n",
    "\n",
    "accuracies = []\n",
    "auc_scores = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "specificities = []\n",
    "sensitivity_scores = []\n",
    "ppvs = []\n",
    "npvs = []\n",
    "bacc_scores = []\n",
    "ccrs = []\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2.0\n",
    "\n",
    "for fold_id, (train_idx, val_idx) in enumerate(folds, start=1):\n",
    "    X_train, X_val = x_morgan[train_idx], x_morgan[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(class_weight='balanced'),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,              # inner CV (bisa stratified biasa)\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    y_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_val, y_proba)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    prec = precision_score(y_val, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bacc = 0.5 * (sens + spec)\n",
    "\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    ccr = calculate_ccr(sens, spec)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(sens)\n",
    "    f1_scores.append(f1)\n",
    "    sensitivity_scores.append(sens)\n",
    "    specificities.append(spec)\n",
    "    ppvs.append(ppv)\n",
    "    npvs.append(npv)\n",
    "    bacc_scores.append(bacc)\n",
    "    ccrs.append(ccr)\n",
    "\n",
    "    print(f\"Fold {fold_id}: AUC={auc:.4f}, BACC={bacc:.4f}, ACC={acc:.4f}\")\n",
    "\n",
    "# rata-rata CV metrics\n",
    "mean_acc = np.nanmean(accuracies)\n",
    "mean_auc = np.nanmean(auc_scores)\n",
    "mean_bacc = np.nanmean(bacc_scores)\n",
    "\n",
    "Sm = 0.5 * (mean_auc + mean_bacc)    # definisi Sm di manuskrip\n",
    "\n",
    "cv_metrics = {\n",
    "    \"Accuracy\": mean_acc,\n",
    "    \"AUC\": mean_auc,\n",
    "    \"BACC\": mean_bacc,\n",
    "    \"Precision\": np.nanmean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.nanmean(sensitivity_scores),\n",
    "    \"F1\": np.nanmean(f1_scores),\n",
    "    \"Specificity\": np.nanmean(specificities),\n",
    "    \"PPV\": np.nanmean(ppvs),\n",
    "    \"NPV\": np.nanmean(npvs),\n",
    "    \"CCR\": np.nanmean(ccrs),\n",
    "    \"Sm\": Sm\n",
    "}\n",
    "\n",
    "print(\"\\n=== QSAR 10-fold scaffold-CV (Morgan RF) ===\")\n",
    "for k, v in cv_metrics.items():\n",
    "    print(f'CV {k}: {v:.4f}')\n",
    "\n",
    "# =========================================================\n",
    "# 4️⃣ Train final model on full dataset (seperti sebelumnya)\n",
    "# =========================================================\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced'),\n",
    "    param_grid=paramgrid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_final.fit(x_morgan, y)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# =========================================================\n",
    "# 5️⃣ Save final model + CV metrics (termasuk Sm)\n",
    "# =========================================================\n",
    "model_folder = r'C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_folder, 'Dermal_rf_morgan.pkl')\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(\"\\nFinal Random Forest model saved successfully at:\")\n",
    "print(model_path)\n",
    "\n",
    "metrics_df = pd.DataFrame([cv_metrics])\n",
    "metrics_path = os.path.join(model_folder, 'Dermal_rf_morgan_CV_metrics_with_Sm.xlsx')\n",
    "metrics_df.to_excel(metrics_path, index=False)\n",
    "print(\"CV metrics (incl. Sm) saved successfully at:\")\n",
    "print(metrics_path)\n",
    "\n",
    "# Opsional: simpan Sm saja ke CSV untuk keperluan consensus QSAR\n",
    "sm_csv_path = os.path.join(model_folder, 'Sm_Morgan_RF.csv')\n",
    "pd.DataFrame([{\"Descriptor\": \"Morgan\", \"Algorithm\": \"RF\", \"Sm\": Sm}]).to_csv(sm_csv_path, index=False)\n",
    "print(\"Sm saved for consensus QSAR weighting at:\")\n",
    "print(sm_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACCS_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, accuracy_score,\n",
    "    roc_auc_score, confusion_matrix, precision_score,\n",
    "    recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# =========================================================\n",
    "# Fungsi: Bemis–Murcko scaffold-based KFold (10-fold)\n",
    "# =========================================================\n",
    "def get_bemis_murcko_scaffold(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold) if scaffold is not None else None\n",
    "\n",
    "def scaffold_kfold_indices(smiles_list, n_splits=10, random_state=42):\n",
    "    scaffolds = [get_bemis_murcko_scaffold(smi) for smi in smiles_list]\n",
    "    scaffold_to_indices = {}\n",
    "    for idx, scaf in enumerate(scaffolds):\n",
    "        scaffold_to_indices.setdefault(scaf, []).append(idx)\n",
    "\n",
    "    unique_scaffolds = list(scaffold_to_indices.keys())\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(unique_scaffolds)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    folds = []\n",
    "    scaffold_array = np.array(unique_scaffolds)\n",
    "    for scaf_train_idx, scaf_val_idx in kf.split(scaffold_array):\n",
    "        train_idx = []\n",
    "        val_idx = []\n",
    "        for i in scaf_train_idx:\n",
    "            train_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        for i in scaf_val_idx:\n",
    "            val_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        folds.append((np.array(train_idx, dtype=int),\n",
    "                      np.array(val_idx, dtype=int)))\n",
    "    return folds\n",
    "\n",
    "def compute_bacc_from_preds(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bacc = 0.5 * (sens + spec)\n",
    "    return bacc, sens, spec\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ Data, fitur, target (x_maccs, y, SMILES)\n",
    "# =========================================================\n",
    "# Asumsi:\n",
    "#   - x_maccs: np.ndarray (n_samples, n_features)\n",
    "#   - y: np.ndarray (n_samples,)\n",
    "#   - smiles_all: array/list SMILES train set (sama urutannya dengan x_maccs, y)\n",
    "\n",
    "# contoh:\n",
    "# smiles_all = train_df['SMILES'].astype(str).values\n",
    "\n",
    "# =========================================================\n",
    "# 2️⃣ Hyperparameter grid\n",
    "# =========================================================\n",
    "paramgrid = {\n",
    "    \"max_features\": [\n",
    "        x_maccs.shape[1],\n",
    "        x_maccs.shape[1] // 2,\n",
    "        x_maccs.shape[1] // 4,\n",
    "        x_maccs.shape[1] // 12,\n",
    "        x_maccs.shape[1] // 10,\n",
    "        x_maccs.shape[1] // 7,\n",
    "        x_maccs.shape[1] // 5,\n",
    "        x_maccs.shape[1] // 3\n",
    "    ],\n",
    "    \"n_estimators\": [10, 100, 300, 500],\n",
    "}\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# =========================================================\n",
    "# 3️⃣ 10-fold scaffold-CV untuk dapatkan Sm (MACCS + RF)\n",
    "# =========================================================\n",
    "folds = scaffold_kfold_indices(smiles_all, n_splits=10, random_state=42)\n",
    "\n",
    "accuracies, auc_scores, precisions, recalls = [], [], [], []\n",
    "f1_scores, specificities, sensitivity_scores = [], [], []\n",
    "ppvs, npvs, ccrs, bacc_scores = [], [], [], []\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2.0\n",
    "\n",
    "for fold_id, (train_idx, val_idx) in enumerate(\n",
    "    tqdm(folds, total=len(folds), desc=\"Outer scaffold-CV folds (MACCS RF)\")\n",
    "):\n",
    "    X_train, X_val = x_maccs[train_idx], x_maccs[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(class_weight='balanced'),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,          # inner CV biasa\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    y_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_val, y_proba)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    prec = precision_score(y_val, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    bacc, sens, spec = compute_bacc_from_preds(y_val, y_pred)\n",
    "\n",
    "    ppv = (confusion_matrix(y_val, y_pred)[1, 1] /\n",
    "           (confusion_matrix(y_val, y_pred)[1, 1] + confusion_matrix(y_val, y_pred)[0, 1])\n",
    "           ) if (confusion_matrix(y_val, y_pred)[1, 1] + confusion_matrix(y_val, y_pred)[0, 1]) > 0 else 0.0\n",
    "    npv = (confusion_matrix(y_val, y_pred)[0, 0] /\n",
    "           (confusion_matrix(y_val, y_pred)[0, 0] + confusion_matrix(y_val, y_pred)[1, 0])\n",
    "           ) if (confusion_matrix(y_val, y_pred)[0, 0] + confusion_matrix(y_val, y_pred)[1, 0]) > 0 else 0.0\n",
    "    ccr = calculate_ccr(sens, spec)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(sens)\n",
    "    f1_scores.append(f1)\n",
    "    specificities.append(spec)\n",
    "    sensitivity_scores.append(sens)\n",
    "    ppvs.append(ppv)\n",
    "    npvs.append(npv)\n",
    "    ccrs.append(ccr)\n",
    "    bacc_scores.append(bacc)\n",
    "\n",
    "    print(f\"Fold {fold_id+1}: AUC={auc:.4f}, BACC={bacc:.4f}, ACC={acc:.4f}\")\n",
    "\n",
    "mean_acc = np.nanmean(accuracies)\n",
    "mean_auc = np.nanmean(auc_scores)\n",
    "mean_bacc = np.nanmean(bacc_scores)\n",
    "\n",
    "Sm = 0.5 * (mean_auc + mean_bacc)\n",
    "\n",
    "cv_metrics = {\n",
    "    \"Accuracy\": mean_acc,\n",
    "    \"AUC\": mean_auc,\n",
    "    \"BACC\": mean_bacc,\n",
    "    \"Precision\": np.nanmean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.nanmean(sensitivity_scores),\n",
    "    \"F1\": np.nanmean(f1_scores),\n",
    "    \"Specificity\": np.nanmean(specificities),\n",
    "    \"PPV\": np.nanmean(ppvs),\n",
    "    \"NPV\": np.nanmean(npvs),\n",
    "    \"CCR\": np.nanmean(ccrs),\n",
    "    \"Sm\": Sm\n",
    "}\n",
    "\n",
    "print(\"\\n=== QSAR 10-fold scaffold-CV (MACCS RF) ===\")\n",
    "for k, v in cv_metrics.items():\n",
    "    print(f'CV {k}: {v:.4f}')\n",
    "\n",
    "# =========================================================\n",
    "# 4️⃣ Train final model di seluruh dataset (seperti sebelumnya)\n",
    "# =========================================================\n",
    "print(\"\\nTraining final MACCS RF model on full dataset...\")\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced'),\n",
    "    param_grid=paramgrid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_final.fit(x_maccs, y)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# =========================================================\n",
    "# 5️⃣ Save final model + CV metrics + Sm\n",
    "# =========================================================\n",
    "model_folder = r'C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_folder, 'Dermal_rf_macckeys.pkl')\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(\"Final Random Forest model saved successfully at:\")\n",
    "print(model_path)\n",
    "\n",
    "metrics_df = pd.DataFrame([cv_metrics])\n",
    "metrics_path = os.path.join(model_folder, 'Dermal_rf_macckeys_CV_metrics_with_Sm.xlsx')\n",
    "metrics_df.to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics (incl. Sm) saved successfully at: {metrics_path}\")\n",
    "\n",
    "# Sm untuk keperluan consensus QSAR\n",
    "sm_csv_path = os.path.join(model_folder, 'Sm_MACCS_RF.csv')\n",
    "pd.DataFrame([{\"Descriptor\": \"MACCS\", \"Algorithm\": \"RF\", \"Sm\": Sm}]).to_csv(sm_csv_path, index=False)\n",
    "print(\"Sm saved for consensus QSAR weighting at:\")\n",
    "print(sm_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APF_RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, accuracy_score,\n",
    "    roc_auc_score, confusion_matrix, precision_score,\n",
    "    recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# =========================================================\n",
    "# Fungsi: Bemis–Murcko scaffold-based KFold (10-fold)\n",
    "# =========================================================\n",
    "def get_bemis_murcko_scaffold(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold) if scaffold is not None else None\n",
    "\n",
    "def scaffold_kfold_indices(smiles_list, n_splits=10, random_state=42):\n",
    "    scaffolds = [get_bemis_murcko_scaffold(smi) for smi in smiles_list]\n",
    "    scaffold_to_indices = {}\n",
    "    for idx, scaf in enumerate(scaffolds):\n",
    "        scaffold_to_indices.setdefault(scaf, []).append(idx)\n",
    "\n",
    "    unique_scaffolds = list(scaffold_to_indices.keys())\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(unique_scaffolds)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    folds = []\n",
    "    scaffold_array = np.array(unique_scaffolds)\n",
    "    for scaf_train_idx, scaf_val_idx in kf.split(scaffold_array):\n",
    "        train_idx = []\n",
    "        val_idx = []\n",
    "        for i in scaf_train_idx:\n",
    "            train_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        for i in scaf_val_idx:\n",
    "            val_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        folds.append((np.array(train_idx, dtype=int),\n",
    "                      np.array(val_idx, dtype=int)))\n",
    "    return folds\n",
    "\n",
    "def compute_bacc_from_preds(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bacc = 0.5 * (sens + spec)\n",
    "    return bacc, sens, spec\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ Data, fitur, target (x_apf, y, SMILES)\n",
    "# =========================================================\n",
    "# Asumsi:\n",
    "#   - x_apf: np.ndarray (n_samples, n_features)\n",
    "#   - y: np.ndarray (n_samples,)\n",
    "#   - smiles_all: array/list SMILES train set, urutan sama dengan x_apf & y\n",
    "# contoh:\n",
    "# smiles_all = train_df['SMILES'].astype(str).values\n",
    "\n",
    "# =========================================================\n",
    "# 2️⃣ Hyperparameter grid\n",
    "# =========================================================\n",
    "paramgrid = {\n",
    "    \"max_features\": [\n",
    "        x_apf.shape[1],\n",
    "        x_apf.shape[1] // 2,\n",
    "        x_apf.shape[1] // 4,\n",
    "        x_apf.shape[1] // 12,\n",
    "        x_apf.shape[1] // 10,\n",
    "        x_apf.shape[1] // 7,\n",
    "        x_apf.shape[1] // 5,\n",
    "        x_apf.shape[1] // 3\n",
    "    ],\n",
    "    \"n_estimators\": [10, 100, 300, 500],\n",
    "}\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# =========================================================\n",
    "# 3️⃣ 10-fold scaffold-CV untuk dapatkan Sm (APF + RF)\n",
    "# =========================================================\n",
    "folds = scaffold_kfold_indices(smiles_all, n_splits=10, random_state=42)\n",
    "\n",
    "accuracies, auc_scores, precisions, recalls = [], [], [], []\n",
    "f1_scores, specificities, sensitivity_scores = [], [], []\n",
    "ppvs, npvs, ccrs, bacc_scores = [], [], [], []\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2.0\n",
    "\n",
    "for fold_id, (train_idx, val_idx) in enumerate(\n",
    "    tqdm(folds, total=len(folds), desc=\"Outer scaffold-CV folds (APF RF)\")\n",
    "):\n",
    "    X_train, X_val = x_apf[train_idx], x_apf[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=RandomForestClassifier(class_weight='balanced'),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,          # inner CV biasa\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    y_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_val, y_proba)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    prec = precision_score(y_val, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    bacc, sens, spec = compute_bacc_from_preds(y_val, y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    tn = cm[0, 0]\n",
    "    fn = cm[1, 0]\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    ccr = calculate_ccr(sens, spec)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(sens)\n",
    "    f1_scores.append(f1)\n",
    "    specificities.append(spec)\n",
    "    sensitivity_scores.append(sens)\n",
    "    ppvs.append(ppv)\n",
    "    npvs.append(npv)\n",
    "    ccrs.append(ccr)\n",
    "    bacc_scores.append(bacc)\n",
    "\n",
    "    print(f\"Fold {fold_id+1}: AUC={auc:.4f}, BACC={bacc:.4f}, ACC={acc:.4f}\")\n",
    "\n",
    "mean_acc = np.nanmean(accuracies)\n",
    "mean_auc = np.nanmean(auc_scores)\n",
    "mean_bacc = np.nanmean(bacc_scores)\n",
    "\n",
    "Sm = 0.5 * (mean_auc + mean_bacc)\n",
    "\n",
    "cv_metrics = {\n",
    "    \"Accuracy\": mean_acc,\n",
    "    \"AUC\": mean_auc,\n",
    "    \"BACC\": mean_bacc,\n",
    "    \"Precision\": np.nanmean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.nanmean(sensitivity_scores),\n",
    "    \"F1\": np.nanmean(f1_scores),\n",
    "    \"Specificity\": np.nanmean(specificities),\n",
    "    \"PPV\": np.nanmean(ppvs),\n",
    "    \"NPV\": np.nanmean(npvs),\n",
    "    \"CCR\": np.nanmean(ccrs),\n",
    "    \"Sm\": Sm\n",
    "}\n",
    "\n",
    "print(\"\\n=== QSAR 10-fold scaffold-CV (APF RF) ===\")\n",
    "for k, v in cv_metrics.items():\n",
    "    print(f'CV {k}: {v:.4f}')\n",
    "\n",
    "# =========================================================\n",
    "# 4️⃣ Train final model di seluruh dataset\n",
    "# =========================================================\n",
    "print(\"\\nTraining final APF RF model on full dataset...\")\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight='balanced'),\n",
    "    param_grid=paramgrid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "grid_final.fit(x_apf, y)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# =========================================================\n",
    "# 5️⃣ Save final model + CV metrics + Sm\n",
    "# =========================================================\n",
    "model_folder = r'C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_folder, 'Dermal_rf_apf.pkl')\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(\"Final Random Forest model saved successfully at:\")\n",
    "print(model_path)\n",
    "\n",
    "metrics_df = pd.DataFrame([cv_metrics])\n",
    "metrics_path = os.path.join(model_folder, 'Dermal_rf_apf_CV_metrics_with_Sm.xlsx')\n",
    "metrics_df.to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics (incl. Sm) saved successfully at: {metrics_path}\")\n",
    "\n",
    "# Sm untuk keperluan consensus QSAR\n",
    "sm_csv_path = os.path.join(model_folder, 'Sm_APF_RF.csv')\n",
    "pd.DataFrame([{\"Descriptor\": \"APF\", \"Algorithm\": \"RF\", \"Sm\": Sm}]).to_csv(sm_csv_path, index=False)\n",
    "print(\"Sm saved for consensus QSAR weighting at:\")\n",
    "print(sm_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORGAN_XBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score, accuracy_score, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ==========================\n",
    "# Features and target\n",
    "# ==========================\n",
    "# x_morgan, y sudah didefinisikan sebelumnya\n",
    "# Pastikan y = np.int32(S) jika perlu\n",
    "\n",
    "# ==========================\n",
    "# Hyperparameter grid untuk XGBoost\n",
    "# ==========================\n",
    "paramgrid = {\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# ==========================\n",
    "# Scoring function\n",
    "# ==========================\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# ==========================\n",
    "# 10-fold CV\n",
    "# ==========================\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies, auc_scores, precisions, recalls, f1_scores = [], [], [], [], []\n",
    "specificities, sensitivity_scores, ppvs, npvs, ccrs = [], [], [], [], []\n",
    "confusion_matrices = []\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2\n",
    "\n",
    "# ==========================\n",
    "# Outer CV loop dengan progress\n",
    "# ==========================\n",
    "print(\"Starting 10-fold cross-validation...\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tqdm(cv.split(x_morgan, y), total=10, desc=\"CV Folds\")):\n",
    "    X_train, X_test = x_morgan[train_idx], x_morgan[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        estimator=XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss'),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_proba))\n",
    "    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    \n",
    "    ppvs.append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "    npvs.append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "    ccrs.append(calculate_ccr(sensitivity, specificity))\n",
    "    \n",
    "    print(f\"Fold {fold+1} done. Best params: {grid.best_params_}\")\n",
    "\n",
    "# ==========================\n",
    "# Report metrics\n",
    "# ==========================\n",
    "cv_metrics = {\n",
    "    \"Accuracy\": np.mean(accuracies),\n",
    "    \"AUC\": np.mean(auc_scores),\n",
    "    \"Precision\": np.mean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.mean(sensitivity_scores),\n",
    "    \"F1\": np.mean(f1_scores),\n",
    "    \"Specificity\": np.mean(specificities),\n",
    "    \"PPV\": np.mean(ppvs),\n",
    "    \"NPV\": np.mean(npvs),\n",
    "    \"CCR\": np.mean(ccrs)\n",
    "}\n",
    "\n",
    "print(\"\\n=== 10-Fold CV Metrics ===\")\n",
    "for k, v in cv_metrics.items():\n",
    "    print(f'{k}: {v:.4f}')\n",
    "\n",
    "# ==========================\n",
    "# Fit final model di seluruh dataset\n",
    "# ==========================\n",
    "print(\"\\nTraining final XGBoost model on full dataset with GridSearchCV...\")\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss'),\n",
    "    param_grid=paramgrid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_final.fit(x_morgan, y)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# ==========================\n",
    "# Save final model & metrics\n",
    "# ==========================\n",
    "model_folder = r'C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(model_folder, 'Dermal_xgb_morgan.pkl')\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(\"Final XGBoost model saved successfully!\")\n",
    "\n",
    "# Save CV metrics\n",
    "metrics_df = pd.DataFrame([cv_metrics])\n",
    "metrics_path = os.path.join(model_folder, 'Dermal_xgb_morgan_CV_metrics.xlsx')\n",
    "metrics_df.to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics report saved successfully at: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACCS_XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, accuracy_score,\n",
    "    roc_auc_score, confusion_matrix, precision_score,\n",
    "    recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# =========================================================\n",
    "# Fungsi: Bemis–Murcko scaffold-based KFold (10-fold)\n",
    "# =========================================================\n",
    "def get_bemis_murcko_scaffold(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold) if scaffold is not None else None\n",
    "\n",
    "def scaffold_kfold_indices(smiles_list, n_splits=10, random_state=42):\n",
    "    scaffolds = [get_bemis_murcko_scaffold(smi) for smi in smiles_list]\n",
    "    scaffold_to_indices = {}\n",
    "    for idx, scaf in enumerate(scaffolds):\n",
    "        scaffold_to_indices.setdefault(scaf, []).append(idx)\n",
    "\n",
    "    unique_scaffolds = list(scaffold_to_indices.keys())\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(unique_scaffolds)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    folds = []\n",
    "    scaffold_array = np.array(unique_scaffolds)\n",
    "    for scaf_train_idx, scaf_val_idx in kf.split(scaffold_array):\n",
    "        train_idx = []\n",
    "        val_idx = []\n",
    "        for i in scaf_train_idx:\n",
    "            train_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        for i in scaf_val_idx:\n",
    "            val_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        folds.append((np.array(train_idx, dtype=int),\n",
    "                      np.array(val_idx, dtype=int)))\n",
    "    return folds\n",
    "\n",
    "def compute_bacc_from_preds(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bacc = 0.5 * (sens + spec)\n",
    "    return bacc, sens, spec\n",
    "\n",
    "# =========================================================\n",
    "# 1️⃣ Data, fitur, target (x_maccs, y, SMILES)\n",
    "# =========================================================\n",
    "# Asumsi:\n",
    "#   - x_maccs: np.ndarray (n_samples, n_features)\n",
    "#   - y: np.ndarray (n_samples,)\n",
    "#   - smiles_all: array/list SMILES train set (urutannya sama)\n",
    "# contoh:\n",
    "# smiles_all = train_df['SMILES'].astype(str).values\n",
    "\n",
    "# =========================================================\n",
    "# 2️⃣ Hyperparameter grid XGBoost\n",
    "# =========================================================\n",
    "paramgrid = {\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# =========================================================\n",
    "# 3️⃣ 10-fold scaffold-CV untuk dapatkan Sm (MACCS + XGB)\n",
    "# =========================================================\n",
    "folds = scaffold_kfold_indices(smiles_all, n_splits=10, random_state=42)\n",
    "\n",
    "accuracies, auc_scores, precisions, recalls = [], [], [], []\n",
    "f1_scores, specificities, sensitivity_scores = [], [], []\n",
    "ppvs, npvs, ccrs, bacc_scores = [], [], [], []\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2.0\n",
    "\n",
    "print(\"Starting 10-fold scaffold-based CV for MACCS + XGBoost...\")\n",
    "\n",
    "for fold_id, (train_idx, val_idx) in enumerate(\n",
    "    tqdm(folds, total=len(folds), desc=\"Outer scaffold-CV folds (MACCS XGB)\")\n",
    "):\n",
    "    X_train, X_val = x_maccs[train_idx], x_maccs[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=XGBClassifier(\n",
    "            objective='binary:logistic',\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        ),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    y_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_val, y_proba)\n",
    "    except ValueError:\n",
    "        auc = np.nan\n",
    "\n",
    "    prec = precision_score(y_val, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    bacc, sens, spec = compute_bacc_from_preds(y_val, y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_val, y_pred)\n",
    "    tp = cm[1, 1]\n",
    "    fp = cm[0, 1]\n",
    "    tn = cm[0, 0]\n",
    "    fn = cm[1, 0]\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "    ccr = calculate_ccr(sens, spec)\n",
    "\n",
    "    accuracies.append(acc)\n",
    "    auc_scores.append(auc)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(sens)\n",
    "    f1_scores.append(f1)\n",
    "    specificities.append(spec)\n",
    "    sensitivity_scores.append(sens)\n",
    "    ppvs.append(ppv)\n",
    "    npvs.append(npv)\n",
    "    ccrs.append(ccr)\n",
    "    bacc_scores.append(bacc)\n",
    "\n",
    "    print(f\"Fold {fold_id+1}: AUC={auc:.4f}, BACC={bacc:.4f}, ACC={acc:.4f}, Best params: {grid.best_params_}\")\n",
    "\n",
    "mean_acc = np.nanmean(accuracies)\n",
    "mean_auc = np.nanmean(auc_scores)\n",
    "mean_bacc = np.nanmean(bacc_scores)\n",
    "\n",
    "Sm = 0.5 * (mean_auc + mean_bacc)\n",
    "\n",
    "cv_metrics = {\n",
    "    \"Accuracy\": mean_acc,\n",
    "    \"AUC\": mean_auc,\n",
    "    \"BACC\": mean_bacc,\n",
    "    \"Precision\": np.nanmean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.nanmean(sensitivity_scores),\n",
    "    \"F1\": np.nanmean(f1_scores),\n",
    "    \"Specificity\": np.nanmean(specificities),\n",
    "    \"PPV\": np.nanmean(ppvs),\n",
    "    \"NPV\": np.nanmean(npvs),\n",
    "    \"CCR\": np.nanmean(ccrs),\n",
    "    \"Sm\": Sm\n",
    "}\n",
    "\n",
    "print(\"\\n=== 10-Fold scaffold-CV Metrics for MACCS + XGBoost ===\")\n",
    "for k, v in cv_metrics.items():\n",
    "    print(f'{k}: {v:.4f}')\n",
    "\n",
    "# =========================================================\n",
    "# 4️⃣ Fit final model di seluruh dataset\n",
    "# =========================================================\n",
    "print(\"\\nTraining final XGBoost model on full dataset (MACCS features)...\")\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=XGBClassifier(\n",
    "        objective='binary:logistic',\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    ),\n",
    "    param_grid=paramgrid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_final.fit(x_maccs, y)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# =========================================================\n",
    "# 5️⃣ Save final model & metrics (incl. Sm)\n",
    "# =========================================================\n",
    "model_folder = r'C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "model_path = os.path.join(model_folder, 'Dermal_xgb_maccs.pkl')\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(\"Final XGBoost model saved successfully at:\")\n",
    "print(model_path)\n",
    "\n",
    "metrics_df = pd.DataFrame([cv_metrics])\n",
    "metrics_path = os.path.join(model_folder, 'Dermal_xgb_maccs_CV_metrics_with_Sm.xlsx')\n",
    "metrics_df.to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics (incl. Sm) saved successfully at: {metrics_path}\")\n",
    "\n",
    "# Sm untuk consensus QSAR\n",
    "sm_csv_path = os.path.join(model_folder, 'Sm_MACCS_XGB.csv')\n",
    "pd.DataFrame([{\"Descriptor\": \"MACCS\", \"Algorithm\": \"XGB\", \"Sm\": Sm}]).to_csv(sm_csv_path, index=False)\n",
    "print(\"Sm saved for consensus QSAR weighting at:\")\n",
    "print(sm_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APF_XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score, accuracy_score, roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# ==========================\n",
    "# Features and target\n",
    "# ==========================\n",
    "# x_apf, y sudah didefinisikan sebelumnya\n",
    "\n",
    "# ==========================\n",
    "# Hyperparameter grid untuk XGBoost\n",
    "# ==========================\n",
    "paramgrid = {\n",
    "    \"max_depth\": [3, 5, 7, 10],\n",
    "    \"n_estimators\": [100, 200, 300],\n",
    "    \"learning_rate\": [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "# ==========================\n",
    "# Scoring function\n",
    "# ==========================\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# ==========================\n",
    "# 10-fold CV\n",
    "# ==========================\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "accuracies, auc_scores, precisions, recalls, f1_scores = [], [], [], [], []\n",
    "specificities, sensitivity_scores, ppvs, npvs, ccrs = [], [], [], [], []\n",
    "confusion_matrices = []\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2\n",
    "\n",
    "# ==========================\n",
    "# Outer CV loop dengan progress\n",
    "# ==========================\n",
    "print(\"Starting 10-fold cross-validation for APF features...\")\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tqdm(cv.split(x_apf, y), total=10, desc=\"CV Folds\")):\n",
    "    X_train, X_test = x_apf[train_idx], x_apf[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    grid = GridSearchCV(\n",
    "        estimator=XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss'),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_proba = best_model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    # Metrics\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "    auc_scores.append(roc_auc_score(y_test, y_proba))\n",
    "    precisions.append(precision_score(y_test, y_pred, zero_division=0))\n",
    "    recalls.append(recall_score(y_test, y_pred))\n",
    "    f1_scores.append(f1_score(y_test, y_pred))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    confusion_matrices.append(cm)\n",
    "    \n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    sensitivity_scores.append(sensitivity)\n",
    "    specificities.append(specificity)\n",
    "    \n",
    "    ppvs.append(tp / (tp + fp) if (tp + fp) > 0 else 0)\n",
    "    npvs.append(tn / (tn + fn) if (tn + fn) > 0 else 0)\n",
    "    ccrs.append(calculate_ccr(sensitivity, specificity))\n",
    "    \n",
    "    print(f\"Fold {fold+1} done. Best params: {grid.best_params_}\")\n",
    "\n",
    "# ==========================\n",
    "# Report metrics\n",
    "# ==========================\n",
    "cv_metrics = {\n",
    "    \"Accuracy\": np.mean(accuracies),\n",
    "    \"AUC\": np.mean(auc_scores),\n",
    "    \"Precision\": np.mean(precisions),\n",
    "    \"Recall (Sensitivity)\": np.mean(sensitivity_scores),\n",
    "    \"F1\": np.mean(f1_scores),\n",
    "    \"Specificity\": np.mean(specificities),\n",
    "    \"PPV\": np.mean(ppvs),\n",
    "    \"NPV\": np.mean(npvs),\n",
    "    \"CCR\": np.mean(ccrs)\n",
    "}\n",
    "\n",
    "print(\"\\n=== 10-Fold CV Metrics for APF ===\")\n",
    "for k, v in cv_metrics.items():\n",
    "    print(f'{k}: {v:.4f}')\n",
    "\n",
    "# ==========================\n",
    "# Fit final model di seluruh dataset\n",
    "# ==========================\n",
    "print(\"\\nTraining final XGBoost model on full dataset (APF features)...\")\n",
    "grid_final = GridSearchCV(\n",
    "    estimator=XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss'),\n",
    "    param_grid=paramgrid,\n",
    "    scoring=kappa_scorer,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_final.fit(x_apf, y)\n",
    "final_model = grid_final.best_estimator_\n",
    "\n",
    "# ==========================\n",
    "# Save final model & metrics\n",
    "# ==========================\n",
    "model_folder = r'C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_path = os.path.join(model_folder, 'Dermal_xgb_apf.pkl')\n",
    "joblib.dump(final_model, model_path, compress=9)\n",
    "print(\"Final XGBoost model saved successfully!\")\n",
    "\n",
    "# Save CV metrics\n",
    "metrics_df = pd.DataFrame([cv_metrics])\n",
    "metrics_path = os.path.join(model_folder, 'Dermal_xgb_apf_CV_metrics.xlsx')\n",
    "metrics_df.to_excel(metrics_path, index=False)\n",
    "print(f\"CV metrics report saved successfully at: {metrics_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORGAN_MACCS_APF_SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from sklearn.metrics import (\n",
    "    make_scorer, cohen_kappa_score, accuracy_score,\n",
    "    roc_auc_score, confusion_matrix, precision_score,\n",
    "    recall_score, f1_score\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# =========================================================\n",
    "# Fungsi: Bemis–Murcko scaffold-based KFold (10-fold)\n",
    "# =========================================================\n",
    "def get_bemis_murcko_scaffold(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    scaffold = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    return Chem.MolToSmiles(scaffold) if scaffold is not None else None\n",
    "\n",
    "def scaffold_kfold_indices(smiles_list, n_splits=10, random_state=42):\n",
    "    scaffolds = [get_bemis_murcko_scaffold(smi) for smi in smiles_list]\n",
    "    scaffold_to_indices = {}\n",
    "    for idx, scaf in enumerate(scaffolds):\n",
    "        scaffold_to_indices.setdefault(scaf, []).append(idx)\n",
    "\n",
    "    unique_scaffolds = list(scaffold_to_indices.keys())\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    rng.shuffle(unique_scaffolds)\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "    folds = []\n",
    "    scaffold_array = np.array(unique_scaffolds)\n",
    "    for scaf_train_idx, scaf_val_idx in kf.split(scaffold_array):\n",
    "        train_idx = []\n",
    "        val_idx = []\n",
    "        for i in scaf_train_idx:\n",
    "            train_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        for i in scaf_val_idx:\n",
    "            val_idx.extend(scaffold_to_indices[scaffold_array[i]])\n",
    "        folds.append((np.array(train_idx, dtype=int),\n",
    "                      np.array(val_idx, dtype=int)))\n",
    "    return folds\n",
    "\n",
    "def compute_bacc_from_preds(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_pred = np.asarray(y_pred, dtype=int)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    bacc = 0.5 * (sens + spec)\n",
    "    return bacc, sens, spec\n",
    "\n",
    "def calculate_ccr(sensitivity, specificity):\n",
    "    return (sensitivity + specificity) / 2.0\n",
    "\n",
    "# =========================================================\n",
    "# Features, target, SMILES\n",
    "# =========================================================\n",
    "# Pastikan x_morgan, x_maccs, x_apf, y, smiles_all sudah didefinisikan\n",
    "feature_sets = {\n",
    "    \"Morgan\": x_morgan,\n",
    "    \"MACCS\": x_maccs,\n",
    "    \"APF\": x_apf\n",
    "}\n",
    "# contoh:\n",
    "# smiles_all = train_df['SMILES'].astype(str).values\n",
    "\n",
    "# =========================================================\n",
    "# Hyperparameter grid SVM\n",
    "# =========================================================\n",
    "paramgrid = {\n",
    "    \"C\": [0.1, 1, 10, 100],\n",
    "    \"kernel\": ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    \"gamma\": ['scale', 'auto']\n",
    "}\n",
    "\n",
    "kappa_scorer = make_scorer(cohen_kappa_score, weights='quadratic')\n",
    "\n",
    "# =========================================================\n",
    "# Output folder\n",
    "# =========================================================\n",
    "model_folder = r'C:\\Fauzan\\Manuskrip QSAR 1\\Major Revision\\Acute Dermal Toxicity (manual split)\\Model'\n",
    "os.makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "# =========================================================\n",
    "# Loop tiap feature set\n",
    "# =========================================================\n",
    "for name, X in feature_sets.items():\n",
    "    print(f\"\\n===== Processing {name} features (SVM) =====\")\n",
    "\n",
    "    folds = scaffold_kfold_indices(smiles_all, n_splits=10, random_state=42)\n",
    "\n",
    "    accuracies, auc_scores, precisions, recalls = [], [], [], []\n",
    "    f1_scores, specificities, sensitivity_scores = [], [], []\n",
    "    ppvs, npvs, ccrs, bacc_scores = [], [], [], []\n",
    "\n",
    "    for fold_id, (train_idx, val_idx) in enumerate(\n",
    "        tqdm(folds, total=len(folds), desc=f\"{name} scaffold-CV folds (SVM)\")\n",
    "    ):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "        grid = GridSearchCV(\n",
    "            estimator=SVC(probability=True),\n",
    "            param_grid=paramgrid,\n",
    "            scoring=kappa_scorer,\n",
    "            cv=5,\n",
    "            verbose=1,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        grid.fit(X_train, y_train)\n",
    "        best_model = grid.best_estimator_\n",
    "\n",
    "        y_pred = best_model.predict(X_val)\n",
    "        y_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        try:\n",
    "            auc = roc_auc_score(y_val, y_proba)\n",
    "        except ValueError:\n",
    "            auc = np.nan\n",
    "\n",
    "        prec = precision_score(y_val, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "        bacc, sens, spec = compute_bacc_from_preds(y_val, y_pred)\n",
    "\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        tp = cm[1, 1]\n",
    "        fp = cm[0, 1]\n",
    "        tn = cm[0, 0]\n",
    "        fn = cm[1, 0]\n",
    "        ppv = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0\n",
    "        ccr = calculate_ccr(sens, spec)\n",
    "\n",
    "        accuracies.append(acc)\n",
    "        auc_scores.append(auc)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(sens)\n",
    "        f1_scores.append(f1)\n",
    "        specificities.append(spec)\n",
    "        sensitivity_scores.append(sens)\n",
    "        ppvs.append(ppv)\n",
    "        npvs.append(npv)\n",
    "        ccrs.append(ccr)\n",
    "        bacc_scores.append(bacc)\n",
    "\n",
    "        print(f\"Fold {fold_id+1} ({name}): AUC={auc:.4f}, BACC={bacc:.4f}, ACC={acc:.4f}, Best params: {grid.best_params_}\")\n",
    "\n",
    "    mean_acc = np.nanmean(accuracies)\n",
    "    mean_auc = np.nanmean(auc_scores)\n",
    "    mean_bacc = np.nanmean(bacc_scores)\n",
    "    Sm = 0.5 * (mean_auc + mean_bacc)\n",
    "\n",
    "    cv_metrics = {\n",
    "        \"Accuracy\": mean_acc,\n",
    "        \"AUC\": mean_auc,\n",
    "        \"BACC\": mean_bacc,\n",
    "        \"Precision\": np.nanmean(precisions),\n",
    "        \"Recall (Sensitivity)\": np.nanmean(sensitivity_scores),\n",
    "        \"F1\": np.nanmean(f1_scores),\n",
    "        \"Specificity\": np.nanmean(specificities),\n",
    "        \"PPV\": np.nanmean(ppvs),\n",
    "        \"NPV\": np.nanmean(npvs),\n",
    "        \"CCR\": np.nanmean(ccrs),\n",
    "        \"Sm\": Sm\n",
    "    }\n",
    "\n",
    "    print(f\"\\n=== 10-Fold scaffold-CV Metrics for {name} (SVM) ===\")\n",
    "    for k, v in cv_metrics.items():\n",
    "        print(f'{k}: {v:.4f}')\n",
    "\n",
    "    # ==========================\n",
    "    # Train final model full dataset\n",
    "    # ==========================\n",
    "    print(f\"\\nTraining final SVM model on full dataset ({name} features)...\")\n",
    "    grid_final = GridSearchCV(\n",
    "        estimator=SVC(probability=True),\n",
    "        param_grid=paramgrid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        verbose=1,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid_final.fit(X, y)\n",
    "    final_model = grid_final.best_estimator_\n",
    "\n",
    "    # ==========================\n",
    "    # Save model & metrics & Sm\n",
    "    # ==========================\n",
    "    model_path = os.path.join(model_folder, f'Dermal_SVM_{name}.pkl')\n",
    "    joblib.dump(final_model, model_path, compress=9)\n",
    "\n",
    "    metrics_df = pd.DataFrame([cv_metrics])\n",
    "    metrics_path = os.path.join(model_folder, f'Dermal_SVM_{name}_CV_metrics_with_Sm.xlsx')\n",
    "    metrics_df.to_excel(metrics_path, index=False)\n",
    "\n",
    "    sm_csv_path = os.path.join(model_folder, f'Sm_{name}_SVM.csv')\n",
    "    pd.DataFrame([{\"Descriptor\": name, \"Algorithm\": \"SVM\", \"Sm\": Sm}]).to_csv(sm_csv_path, index=False)\n",
    "\n",
    "    print(f\"Final SVM model for {name} saved at:\\n{model_path}\")\n",
    "    print(f\"CV metrics (incl. Sm) saved at:\\n{metrics_path}\")\n",
    "    print(f\"Sm for {name} SVM saved at:\\n{sm_csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
